{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#samesame","title":"samesame","text":"<p>Same, same but different ...</p> <p><code>samesame</code> implements classifier two-sample tests (CTSTs) and as a bonus  extension, a noninferiority test (NIT). </p> <p>These were either missing or implemented with some tradeoffs  (looking at you, sample-splitting) in existing libraries. And so,  <code>samesame</code> fills in the gaps :)</p>"},{"location":"#motivation","title":"Motivation","text":"<p>What is <code>samesame</code> good for? It is for data (model) validation, performance monitoring, drift detection (dataset shift), statistical process control, and so on and so forth. </p> <p>Want more?  Here you go. This motivating example comes from the related R package  <code>dsos</code>.</p>"},{"location":"#installation","title":"Installation","text":"<p>To install, run the following command:</p> <pre><code>python -m pip install samesame\n</code></pre>"},{"location":"#usage","title":"Usage","text":""},{"location":"#functionality","title":"Functionality","text":"<p>Below, you will find an overview of common modules in <code>samesame</code>. </p> Function Module Bayesian inference <code>samesame.bayes</code> Classifier two-sample tests (CTSTs) <code>samesame.ctst</code> Noninferiority tests (NITs) <code>samesame.nit</code>"},{"location":"#attributes","title":"Attributes","text":"<p>When the method is a statistical test, <code>samesame</code> saves (stores) the results of some potentially computationally intensive results in attributes. These attributes, when available, can be accessed as follows. </p> Attribute Description <code>.statistic</code> The test statistic for the hypothesis. <code>.null</code> The null distribution for the hypothesis. <code>.pvalue</code> The p-value for the hypothesis. <code>.posterior</code> The posterior distribution for the hypothesis. <code>.bayes_factor</code> The bayes factor for the hypothesis."},{"location":"#examples","title":"Examples","text":"<p>To get started, please see the examples in the docs.</p>"},{"location":"#dependencies","title":"Dependencies","text":"<p><code>samesame</code> has few dependencies beyond the standard library. It will  probably work with some older Python versions. It is, in short, a lightweight dependency for most machine learning projects.<code>samesame</code> is built on top of, and is compatible with, scikit-learn and numpy.</p>"},{"location":"api/bayes/","title":"bayes","text":"<p>Functions for working with p-values and Bayes factors.</p> <p>This module provides functions for converting between p-values and Bayes factors, as well as calculating Bayes factors from posterior distributions. These are useful for hypothesis testing and interpreting statistical evidence.</p>"},{"location":"api/bayes/#samesame.bayes.as_bf","title":"<code>as_bf(pvalue)</code>","text":"<p>Convert a one-sided p-value to a Bayes factor.</p> <p>This Bayes factor quantifies evidence in favour of a directional effect over the null hypothesis of no such effect.</p> <p>Parameters:</p> Name Type Description Default <code>pvalue</code> <code>NDArray | float</code> <p>The p-value(s) to be converted to Bayes factor(s). Can be a single value or an array of values.</p> required <p>Returns:</p> Type Description <code>NDArray | float</code> <p>The corresponding Bayes factor(s). The return type matches the input type.</p> See Also <p>as_pvalue : Convert a Bayes factor to a p-value.</p> Notes <p>The Bayes factor is derived from the one-sided p-value using a Bayesian interpretation, which quantifies evidence in favor of a directional effect over the null hypothesis of no such effect.</p> <p>See [1]_ for theoretical details and implications.</p> References <p>.. [1] Marsman, Maarten, and Eric-Jan Wagenmakers. \"Three Insights from    a Bayesian Interpretation of the One-Sided P Value.\" Educational and    Psychological Measurement, vol. 77, no. 3, 2017, pp. 529-539.    doi:10.1177/0013164416669201.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from samesame.bayes import as_bf\n&gt;&gt;&gt; as_bf(0.5)\nnp.float64(1.0)\n&gt;&gt;&gt; as_bf(np.array([0.05, 0.1, 0.5]))\narray([19., 9., 1.])\n</code></pre> Source code in <code>src/samesame/bayes.py</code> <pre><code>def as_bf(\n    pvalue: NDArray | float,\n) -&gt; NDArray | float:\n    \"\"\"\n    Convert a one-sided p-value to a Bayes factor.\n\n    This Bayes factor quantifies evidence in favour of a directional effect\n    over the null hypothesis of no such effect.\n\n    Parameters\n    ----------\n    pvalue : NDArray | float\n        The p-value(s) to be converted to Bayes factor(s). Can be a single\n        value or an array of values.\n\n    Returns\n    -------\n    NDArray | float\n        The corresponding Bayes factor(s). The return type matches the\n        input type.\n\n    See Also\n    --------\n    as_pvalue : Convert a Bayes factor to a p-value.\n\n    Notes\n    -----\n    The Bayes factor is derived from the one-sided p-value using a\n    Bayesian interpretation, which quantifies evidence in favor of a\n    directional effect over the null hypothesis of no such effect.\n\n    See [1]_ for theoretical details and implications.\n\n    References\n    ----------\n    .. [1] Marsman, Maarten, and Eric-Jan Wagenmakers. \"Three Insights from\n       a Bayesian Interpretation of the One-Sided P Value.\" *Educational and\n       Psychological Measurement*, vol. 77, no. 3, 2017, pp. 529-539.\n       doi:10.1177/0013164416669201.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from samesame.bayes import as_bf\n    &gt;&gt;&gt; as_bf(0.5)\n    np.float64(1.0)\n    &gt;&gt;&gt; as_bf(np.array([0.05, 0.1, 0.5])) # doctest: +NORMALIZE_WHITESPACE\n    array([19., 9., 1.])\n    \"\"\"\n    if np.any(np.logical_or(pvalue &gt;= 1, pvalue &lt;= 0)):\n        raise ValueError(\"pvalue must be within the open interval (0, 1).\")\n    pvalue = np.clip(pvalue, 1e-10, 1 - 1e-10)  # Avoid numerical issues at extremes\n    return 1.0 / np.exp(logit(pvalue))  # evidence in favor of an effect\n</code></pre>"},{"location":"api/bayes/#samesame.bayes.as_pvalue","title":"<code>as_pvalue(bayes_factor)</code>","text":"<p>Convert a Bayes factor of a directional effect to a one-sided p-value.</p> <p>The Bayes factor quantifies evidence in favour of a directional effect over the null hypothesis of no such effect.</p> <p>Parameters:</p> Name Type Description Default <code>bayes_factor</code> <code>float | NDArray</code> <p>The Bayes factor(s) to be converted to p-value(s). Can be a single value or an array of values.</p> required <p>Returns:</p> Type Description <code>float | NDArray</code> <p>The corresponding p-value(s). The return type matches the input type.</p> See Also <p>as_pvalue : Convert a Bayes factor to a p-value.</p> Notes <p>The equivalence between the Bayes factor in favor of a directional effect and the one-sided p-value for the null of no such effect is discussed in [1]_.</p> References <p>.. [1] Marsman, Maarten, and Eric-Jan Wagenmakers. \"Three Insights from a    Bayesian Interpretation of the One-Sided P Value.\" Educational and    Psychological Measurement, vol. 77, no. 3, 2017, pp. 529\u2013539,    https://doi.org/10.1177/0013164416669201.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from samesame.bayes import as_pvalue\n&gt;&gt;&gt; as_pvalue(1)\nnp.float64(0.5)\n&gt;&gt;&gt; as_pvalue(np.array([19.0, 9.0, 1.0]))\narray([0.05, 0.1 , 0.5 ])\n</code></pre> Source code in <code>src/samesame/bayes.py</code> <pre><code>def as_pvalue(\n    bayes_factor: float | NDArray,\n) -&gt; float | NDArray:\n    \"\"\"\n    Convert a Bayes factor of a directional effect to a one-sided p-value.\n\n    The Bayes factor quantifies evidence in favour of a directional effect\n    over the null hypothesis of no such effect.\n\n    Parameters\n    ----------\n    bayes_factor : float | NDArray\n        The Bayes factor(s) to be converted to p-value(s). Can be a single\n        value or an array of values.\n\n    Returns\n    -------\n    float | NDArray\n        The corresponding p-value(s). The return type matches the input type.\n\n    See Also\n    --------\n    as_pvalue : Convert a Bayes factor to a p-value.\n\n    Notes\n    -----\n    The equivalence between the Bayes factor in favor of a directional effect\n    and the one-sided p-value for the null of no such effect is discussed in\n    [1]_.\n\n    References\n    ----------\n    .. [1] Marsman, Maarten, and Eric-Jan Wagenmakers. \"Three Insights from a\n       Bayesian Interpretation of the One-Sided P Value.\" *Educational and\n       Psychological Measurement*, vol. 77, no. 3, 2017, pp. 529\u2013539,\n       https://doi.org/10.1177/0013164416669201.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from samesame.bayes import as_pvalue\n    &gt;&gt;&gt; as_pvalue(1)\n    np.float64(0.5)\n    &gt;&gt;&gt; as_pvalue(np.array([19.0, 9.0, 1.0])) # doctest: +NORMALIZE_WHITESPACE\n    array([0.05, 0.1 , 0.5 ])\n    \"\"\"\n    if np.any(bayes_factor &lt;= 0):\n        raise ValueError(\"bayes_factor must be strictly positive.\")\n    bf_ = np.clip(bayes_factor, 1e-10, 1e10)  # Ensure numerical stability\n    pvalue = expit(-np.log(bf_))\n    return pvalue\n</code></pre>"},{"location":"api/bayes/#samesame.bayes.bayes_factor","title":"<code>bayes_factor(posterior, threshold=0.0, adjustment=0)</code>","text":"<p>Compute the Bayes factor for a test of direction given a threshold.</p> <p>The Bayes factor quantifies the evidence in favor of the hypothesis that the posterior distribution exceeds the given threshold compared to the hypothesis that it does not.</p> <p>Parameters:</p> Name Type Description Default <code>posterior</code> <code>NDArray</code> <p>An array of posterior samples.</p> required <code>threshold</code> <code>float</code> <p>The threshold value to test against. Default is 0.0.</p> <code>0.0</code> <code>adjustment</code> <code>(0, 1)</code> <p>Adjustment to apply to the Bayes factor calculation. Default is 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>float</code> <p>The computed Bayes factor.</p> See Also <p>as_pvalue : Convert a Bayes factor to a p-value.</p> <p>as_bf : Convert a p-value to a Bayes factor.</p> Notes <p>The Bayes factor is a measure of evidence which compares the likelihood of the data under two competing hypotheses. In this function, the numerator represents the proportion of posterior samples exceeding the threshold, while the denominator represents the proportion of samples not exceeding the threshold. If all samples exceed the threshold, the Bayes factor is set to infinity, indicating overwhelming evidence in favor of the hypothesis.</p> <p>The adjustment parameter allows for slight modifications to the Bayes factor calculation, which can be useful in specific contexts such as sensitivity analyses.</p> <p>See [1]_ for further theoretical details and practical implications of this approach.</p> References <p>.. [1] Marsman, Maarten, and Eric-Jan Wagenmakers. \"Three Insights from a    Bayesian Interpretation of the One-Sided P Value.\" Educational and    Psychological Measurement, vol. 77, no. 3, 2017, pp. 529-539.    doi:10.1177/0013164416669201.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from samesame.bayes import bayes_factor\n&gt;&gt;&gt; posterior_samples = np.array([0.2, 0.5, 0.8, 0.9])\n&gt;&gt;&gt; bayes_factor(posterior_samples, threshold=0.5)\nnp.float64(1.0)\n</code></pre> Source code in <code>src/samesame/bayes.py</code> <pre><code>def bayes_factor(\n    posterior: NDArray,\n    threshold: float = 0.0,\n    adjustment: Literal[0, 1] = 0,\n) -&gt; float:\n    \"\"\"\n    Compute the Bayes factor for a test of direction given a threshold.\n\n    The Bayes factor quantifies the evidence in favor of the hypothesis that\n    the posterior distribution exceeds the given threshold compared to the\n    hypothesis that it does not.\n\n    Parameters\n    ----------\n    posterior : NDArray\n        An array of posterior samples.\n    threshold : float, optional\n        The threshold value to test against. Default is 0.0.\n    adjustment : {0, 1}, optional\n        Adjustment to apply to the Bayes factor calculation. Default is 0.\n\n    Returns\n    -------\n    float\n        The computed Bayes factor.\n\n    See Also\n    --------\n    as_pvalue : Convert a Bayes factor to a p-value.\n\n    as_bf : Convert a p-value to a Bayes factor.\n\n    Notes\n    -----\n    The Bayes factor is a measure of evidence which compares the likelihood of\n    the data under two competing hypotheses. In this function, the numerator\n    represents the proportion of posterior samples exceeding the threshold,\n    while the denominator represents the proportion of samples not exceeding\n    the threshold. If all samples exceed the threshold, the Bayes factor is\n    set to infinity, indicating overwhelming evidence in favor of the\n    hypothesis.\n\n    The adjustment parameter allows for slight modifications to the Bayes\n    factor calculation, which can be useful in specific contexts such as\n    sensitivity analyses.\n\n    See [1]_ for further theoretical details and practical implications of\n    this approach.\n\n    References\n    ----------\n    .. [1] Marsman, Maarten, and Eric-Jan Wagenmakers. \"Three Insights from a\n       Bayesian Interpretation of the One-Sided P Value.\" *Educational and\n       Psychological Measurement*, vol. 77, no. 3, 2017, pp. 529-539.\n       doi:10.1177/0013164416669201.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from samesame.bayes import bayes_factor\n    &gt;&gt;&gt; posterior_samples = np.array([0.2, 0.5, 0.8, 0.9])\n    &gt;&gt;&gt; bayes_factor(posterior_samples, threshold=0.5)\n    np.float64(1.0)\n    \"\"\"\n    return _bayes_factor(posterior, threshold, adjustment)\n</code></pre>"},{"location":"api/ctst/","title":"ctst","text":"<p>Classifier two-sample tests (CTST) from binary classification metrics.</p> <p>The classifier two-sample test broadly consists of three steps: (1) training a classifier, (2) scoring the two samples and (3) turning a test statistic into a p-value from these scores. This test statistic can be the performance metric of a binary classifier such as the (weighted) area under the receiver operating characteristic curve, the Matthews correlation coefficient, and the (balanced) accuracy. This module tackles step (3).</p> References <p>.. [1] Lopez-Paz, David, and Maxime Oquab. \"Revisiting Classifier Two-Sample    Tests.\" International Conference on Learning Representations. 2017.</p> <p>.. [2] Friedman, Jerome. \"On multivariate goodness-of-fit and two-sample    testing.\" No. SLAC-PUB-10325. SLAC National Accelerator Laboratory (SLAC),    Menlo Park, CA (United States), 2004.</p> <p>.. [3] K\u00fcbler, Jonas M., et al. \"Automl two-sample test.\" Advances in Neural    Information Processing Systems 35 (2022): 15929-15941.</p> <p>.. [4] Ci\u00e9men\u00e7on, St\u00e9phan, Marine Depecker, and Nicolas Vayatis. \"AUC    optimization and the two-sample problem.\" Proceedings of the 23rd    International Conference on Neural Information Processing Systems. 2009.</p> <p>.. [5] Hediger, Simon, Loris Michel, and Jeffrey N\u00e4f. \"On the use of random    forest for two-sample testing.\" Computational Statistics &amp; Data Analysis    170 (2022): 107435.</p> <p>.. [6] Kim, Ilmun, et al. \"Classification accuracy as a proxy for two-sample    testing.\" Annals of Statistics 49.1 (2021): 411-434.</p>"},{"location":"api/ctst/#samesame.ctst.CTST","title":"<code>CTST</code>  <code>dataclass</code>","text":"<p>Classifier two-sample test (CTST) using a binary classification metric.</p> <p>This test compares scores (predictions) from two independent samples. Rejecting the null implies that scoring is not random and that the classifier is able to distinguish between the two samples.</p> <p>Attributes:</p> Name Type Description <code>actual</code> <code>NDArray</code> <p>Binary indicator for sample membership.</p> <code>predicted</code> <code>NDArray</code> <p>Estimated (predicted) scores for corresponding samples in <code>actual</code>.</p> <code>metric</code> <code>Callable</code> <p>A callable that conforms to scikit-learn metric API. This function must take two positional arguments e.g. <code>y_true</code> and <code>y_pred</code>.</p> <code>n_resamples</code> <code>(int, optional)</code> <p>Number of resampling iterations, by default 9999.</p> <code>rng</code> <code>(Generator, optional)</code> <p>Random number generator, by default np.random.default_rng().</p> <code>n_jobs</code> <code>(int, optional)</code> <p>Number of parallel jobs, by default 1.</p> <code>batch</code> <code>(int or None, optional)</code> <p>Batch size for parallel processing, by default None.</p> <code>alternative</code> <code>({'less', 'greater', 'two-sided'}, optional)</code> <p>Defines the alternative hypothesis. Default is 'two-sided'.</p> Notes <p>The null distribution is based on permutations. See <code>scipy.stats.permutation_test</code> for more details.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from sklearn.metrics import matthews_corrcoef, roc_auc_score\n&gt;&gt;&gt; from samesame.ctst import CTST\n&gt;&gt;&gt; actual = np.array([0, 1, 1, 0])\n&gt;&gt;&gt; scores = np.array([0.2, 0.8, 0.6, 0.4])\n&gt;&gt;&gt; ctst_mcc = CTST(actual, scores, metric=matthews_corrcoef)\n&gt;&gt;&gt; ctst_auc = CTST(actual, scores, metric=roc_auc_score)\n&gt;&gt;&gt; print(ctst_mcc.pvalue)\n&gt;&gt;&gt; print(ctst_auc.pvalue)\n&gt;&gt;&gt; ctst_ = CTST.from_samples(scores, scores, metric=roc_auc_score)\n&gt;&gt;&gt; isinstance(ctst_, CTST)\nTrue\n</code></pre> Source code in <code>src/samesame/ctst.py</code> <pre><code>@dataclass\nclass CTST:\n    \"\"\"\n    Classifier two-sample test (CTST) using a binary classification metric.\n\n    This test compares scores (predictions) from two independent samples.\n    Rejecting the null implies that scoring is not random and that the\n    classifier is able to distinguish between the two samples.\n\n    Attributes\n    ----------\n    actual : NDArray\n        Binary indicator for sample membership.\n    predicted : NDArray\n        Estimated (predicted) scores for corresponding samples in `actual`.\n    metric : Callable\n        A callable that conforms to scikit-learn metric API. This function\n        must take two positional arguments e.g. `y_true` and `y_pred`.\n    n_resamples : int, optional\n        Number of resampling iterations, by default 9999.\n    rng : np.random.Generator, optional\n        Random number generator, by default np.random.default_rng().\n    n_jobs : int, optional\n        Number of parallel jobs, by default 1.\n    batch : int or None, optional\n        Batch size for parallel processing, by default None.\n    alternative : {'less', 'greater', 'two-sided'}, optional\n        Defines the alternative hypothesis. Default is 'two-sided'.\n\n    Notes\n    -----\n    The null distribution is based on permutations.\n    See `scipy.stats.permutation_test` for more details.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from sklearn.metrics import matthews_corrcoef, roc_auc_score\n    &gt;&gt;&gt; from samesame.ctst import CTST\n    &gt;&gt;&gt; actual = np.array([0, 1, 1, 0])\n    &gt;&gt;&gt; scores = np.array([0.2, 0.8, 0.6, 0.4])\n    &gt;&gt;&gt; ctst_mcc = CTST(actual, scores, metric=matthews_corrcoef)\n    &gt;&gt;&gt; ctst_auc = CTST(actual, scores, metric=roc_auc_score)\n    &gt;&gt;&gt; print(ctst_mcc.pvalue) # doctest: +SKIP\n    &gt;&gt;&gt; print(ctst_auc.pvalue) # doctest: +SKIP\n    &gt;&gt;&gt; ctst_ = CTST.from_samples(scores, scores, metric=roc_auc_score)\n    &gt;&gt;&gt; isinstance(ctst_, CTST)\n    True\n    \"\"\"\n\n    actual: NDArray = field(repr=False)\n    predicted: NDArray = field(repr=False)\n    metric: Callable\n    n_resamples: int = 9999\n    rng: np.random.Generator = np.random.default_rng()\n    n_jobs: int = 1\n    batch: int | None = None\n    alternative: Literal[\"less\", \"greater\", \"two-sided\"] = \"two-sided\"\n\n    def __post_init__(self):\n        \"\"\"Validate inputs.\"\"\"\n        self.actual = column_or_1d(self.actual)\n        self.predicted = column_or_1d(self.predicted)\n        check_consistent_length(self.actual, self.predicted)\n        assert type_of_target(self.actual, \"actual\") == \"binary\"\n        type_predicted = type_of_target(self.predicted, \"predicted\")\n        assert type_predicted in (\n            \"binary\",\n            \"continuous\",\n            \"multiclass\",\n        ), f\"Expected 'predicted' to be binary or continuous, got {type_predicted}.\"\n        assert check_metric_function(self.metric), (\n            f\"'metric' expects a callable that conforms to scikit-learn metric. \"\n            f\"{signature(self.metric)=} does not.\"\n        )\n\n    @cached_property\n    def _result(self):\n        def statistic(*args):\n            return self.metric(args[0], args[1])\n\n        return permutation_test(\n            data=(self.actual, self.predicted),\n            statistic=statistic,\n            permutation_type=\"pairings\",\n            n_resamples=self.n_resamples,\n            alternative=self.alternative,\n            random_state=self.rng,\n        )\n\n    @cached_property\n    def statistic(self) -&gt; float:\n        \"\"\"\n        Compute the observed test statistic.\n\n        Returns\n        -------\n        float\n            The test statistic.\n\n        Notes\n        -----\n        The result is cached to avoid (expensive) recomputation.\n        \"\"\"\n        return self._result.statistic\n\n    @cached_property\n    def null(self) -&gt; NDArray:\n        \"\"\"\n        Compute the null distribution of the test statistic.\n\n        Notes\n        -----\n        The result is cached to avoid (expensive) recomputation since the\n        null distribution requires permutations.\n        \"\"\"\n        return self._result.null_distribution\n\n    @cached_property\n    def pvalue(self):\n        \"\"\"\n        Compute the p-value using permutations.\n\n        Notes\n        -----\n        The result is cached to avoid (expensive) recomputation.\n        \"\"\"\n        return self._result.pvalue\n\n    @classmethod\n    def from_samples(\n        cls,\n        first_sample: NDArray,\n        second_sample: NDArray,\n        metric: Callable,\n        n_resamples: int = 9999,\n        rng: np.random.Generator = np.random.default_rng(),\n        n_jobs: int = 1,\n        batch: int | None = None,\n        alternative: Literal[\"less\", \"greater\", \"two_sided\"] = \"two_sided\",\n    ):\n        \"\"\"\n        Create a CTST instance from two samples.\n\n        Parameters\n        ----------\n        first_sample : NDArray\n            First sample of scores. These can be binary or continuous.\n        second_sample : NDArray\n            Second sample of scores. These can be binary or continuous.\n\n        Returns\n        -------\n        CTST\n            An instance of the CTST class.\n        \"\"\"\n        assert type_of_target(first_sample) == type_of_target(second_sample)\n        samples = (first_sample, second_sample)\n        actual = assign_labels(samples)\n        predicted = concat_samples(samples)\n        return cls(\n            actual,\n            predicted,\n            metric,\n            n_resamples,\n            rng,\n            n_jobs,\n            batch,\n            alternative,\n        )\n</code></pre>"},{"location":"api/ctst/#samesame.ctst.CTST.null","title":"<code>null</code>  <code>cached</code> <code>property</code>","text":"<p>Compute the null distribution of the test statistic.</p> Notes <p>The result is cached to avoid (expensive) recomputation since the null distribution requires permutations.</p>"},{"location":"api/ctst/#samesame.ctst.CTST.pvalue","title":"<code>pvalue</code>  <code>cached</code> <code>property</code>","text":"<p>Compute the p-value using permutations.</p> Notes <p>The result is cached to avoid (expensive) recomputation.</p>"},{"location":"api/ctst/#samesame.ctst.CTST.statistic","title":"<code>statistic</code>  <code>cached</code> <code>property</code>","text":"<p>Compute the observed test statistic.</p> <p>Returns:</p> Type Description <code>float</code> <p>The test statistic.</p> Notes <p>The result is cached to avoid (expensive) recomputation.</p>"},{"location":"api/ctst/#samesame.ctst.CTST.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate inputs.</p> Source code in <code>src/samesame/ctst.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Validate inputs.\"\"\"\n    self.actual = column_or_1d(self.actual)\n    self.predicted = column_or_1d(self.predicted)\n    check_consistent_length(self.actual, self.predicted)\n    assert type_of_target(self.actual, \"actual\") == \"binary\"\n    type_predicted = type_of_target(self.predicted, \"predicted\")\n    assert type_predicted in (\n        \"binary\",\n        \"continuous\",\n        \"multiclass\",\n    ), f\"Expected 'predicted' to be binary or continuous, got {type_predicted}.\"\n    assert check_metric_function(self.metric), (\n        f\"'metric' expects a callable that conforms to scikit-learn metric. \"\n        f\"{signature(self.metric)=} does not.\"\n    )\n</code></pre>"},{"location":"api/ctst/#samesame.ctst.CTST.from_samples","title":"<code>from_samples(first_sample, second_sample, metric, n_resamples=9999, rng=np.random.default_rng(), n_jobs=1, batch=None, alternative='two_sided')</code>  <code>classmethod</code>","text":"<p>Create a CTST instance from two samples.</p> <p>Parameters:</p> Name Type Description Default <code>first_sample</code> <code>NDArray</code> <p>First sample of scores. These can be binary or continuous.</p> required <code>second_sample</code> <code>NDArray</code> <p>Second sample of scores. These can be binary or continuous.</p> required <p>Returns:</p> Type Description <code>CTST</code> <p>An instance of the CTST class.</p> Source code in <code>src/samesame/ctst.py</code> <pre><code>@classmethod\ndef from_samples(\n    cls,\n    first_sample: NDArray,\n    second_sample: NDArray,\n    metric: Callable,\n    n_resamples: int = 9999,\n    rng: np.random.Generator = np.random.default_rng(),\n    n_jobs: int = 1,\n    batch: int | None = None,\n    alternative: Literal[\"less\", \"greater\", \"two_sided\"] = \"two_sided\",\n):\n    \"\"\"\n    Create a CTST instance from two samples.\n\n    Parameters\n    ----------\n    first_sample : NDArray\n        First sample of scores. These can be binary or continuous.\n    second_sample : NDArray\n        Second sample of scores. These can be binary or continuous.\n\n    Returns\n    -------\n    CTST\n        An instance of the CTST class.\n    \"\"\"\n    assert type_of_target(first_sample) == type_of_target(second_sample)\n    samples = (first_sample, second_sample)\n    actual = assign_labels(samples)\n    predicted = concat_samples(samples)\n    return cls(\n        actual,\n        predicted,\n        metric,\n        n_resamples,\n        rng,\n        n_jobs,\n        batch,\n        alternative,\n    )\n</code></pre>"},{"location":"api/nit/","title":"nit","text":""},{"location":"api/nit/#samesame.nit.WeightedAUC","title":"<code>WeightedAUC</code>  <code>dataclass</code>","text":"<p>               Bases: <code>CTST</code></p> <p>Two-sample test for no adverse shift using the weighted AUC (WAUC).</p> <p>This test compares scores from two independent samples. We reject the null hypothesis of no adverse shift for unusually high values of the WAUC i.e. when the second sample is relatively worse than the first one. This is a robust nonparametric noninferiority test (NIT) with no pre-specified margin. It can be used, amongst other things, to detect dataset shift with outlier scores, hence the DSOS acronym.</p> <p>Attributes:</p> Name Type Description <code>actual</code> <code>NDArray</code> <p>Binary indicator for sample membership.</p> <code>predicted</code> <code>NDArray</code> <p>Estimated (predicted) scores for corresponding samples in <code>actual</code>.</p> <code>n_resamples</code> <code>(int, optional)</code> <p>Number of resampling iterations, by default 9999.</p> <code>rng</code> <code>(Generator, optional)</code> <p>Random number generator, by default np.random.default_rng().</p> <code>n_jobs</code> <code>(int, optional)</code> <p>Number of parallel jobs, by default 1.</p> <code>batch</code> <code>(int or None, optional)</code> <p>Batch size for parallel processing, by default None.</p> See Also <p>bayes.as_bf : Convert a one-sided p-value to a Bayes factor.</p> <p>bayes.as_pvalue : Convert a Bayes factor to a one-sided p-value.</p> Notes <p>The frequentist null distribution of the WAUC is based on permutations [1]. The Bayesian posterior distribution of the WAUC is based on the Bayesian bootstrap [2]. Because this is a one-tailed test of direction (it asks the question, 'are we worse off?'), we can convert a one-sided p-value into a Bayes factor and vice versa. We can also use these p-values for sequential testing [3].</p> <p>The test assumes that <code>predicted</code> are outlier scores and/or encode some notions of outlyingness; higher value of <code>predicted</code> indicates worse outcomes.</p> References <p>.. [1] Kamulete, Vathy M. \"Test for non-negligible adverse shifts.\"    Uncertainty in Artificial Intelligence. PMLR, 2022.</p> <p>.. [2] Gu, Jiezhun, Subhashis Ghosal, and Anindya Roy. \"Bayesian bootstrap    estimation of ROC curve.\" Statistics in medicine 27.26 (2008): 5407-5420.</p> <p>.. [3] Kamulete, Vathy M. \"Are you OK? A Bayesian Sequential Test for    Adverse Shift.\" 2025.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from samesame.nit import WeightedAUC\n&gt;&gt;&gt; # alternatively: from samesame.nit import DSOS\n&gt;&gt;&gt; actual = np.array([0, 1, 1, 0])\n&gt;&gt;&gt; scores = np.array([0.2, 0.8, 0.6, 0.4])\n&gt;&gt;&gt; wauc = WeightedAUC(actual, scores)\n&gt;&gt;&gt; print(wauc.pvalue)\n&gt;&gt;&gt; print(wauc.bayes_factor)\n&gt;&gt;&gt; wauc_ = WeightedAUC.from_samples(scores, scores)\n&gt;&gt;&gt; isinstance(wauc_, WeightedAUC)\nTrue\n</code></pre> Source code in <code>src/samesame/nit.py</code> <pre><code>@dataclass\nclass WeightedAUC(CTST):\n    \"\"\"\n    Two-sample test for no adverse shift using the weighted AUC (WAUC).\n\n    This test compares scores from two independent samples. We reject the\n    null hypothesis of no adverse shift for unusually high values of the WAUC\n    i.e. when the second sample is relatively worse than the first one. This\n    is a robust nonparametric noninferiority test (NIT) with no pre-specified\n    margin. It can be used, amongst other things, to detect dataset shift with\n    outlier scores, hence the DSOS acronym.\n\n    Attributes\n    ----------\n    actual : NDArray\n        Binary indicator for sample membership.\n    predicted : NDArray\n        Estimated (predicted) scores for corresponding samples in `actual`.\n    n_resamples : int, optional\n        Number of resampling iterations, by default 9999.\n    rng : np.random.Generator, optional\n        Random number generator, by default np.random.default_rng().\n    n_jobs : int, optional\n        Number of parallel jobs, by default 1.\n    batch : int or None, optional\n        Batch size for parallel processing, by default None.\n\n    See Also\n    --------\n    bayes.as_bf : Convert a one-sided p-value to a Bayes factor.\n\n    bayes.as_pvalue : Convert a Bayes factor to a one-sided p-value.\n\n    Notes\n    -----\n    The frequentist null distribution of the WAUC is based on permutations\n    [1]. The Bayesian posterior distribution of the WAUC is based on the\n    Bayesian bootstrap [2]. Because this is a one-tailed test of direction\n    (it asks the question, 'are we worse off?'), we can convert a one-sided\n    p-value into a Bayes factor and vice versa. We can also use these p-values\n    for sequential testing [3].\n\n    The test assumes that `predicted` are outlier scores and/or encode some\n    notions of outlyingness; higher value of `predicted` indicates worse\n    outcomes.\n\n    References\n    ----------\n    .. [1] Kamulete, Vathy M. \"Test for non-negligible adverse shifts.\"\n       Uncertainty in Artificial Intelligence. PMLR, 2022.\n\n    .. [2] Gu, Jiezhun, Subhashis Ghosal, and Anindya Roy. \"Bayesian bootstrap\n       estimation of ROC curve.\" Statistics in medicine 27.26 (2008): 5407-5420.\n\n    .. [3] Kamulete, Vathy M. \"Are you OK? A Bayesian Sequential Test for\n       Adverse Shift.\" 2025.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from samesame.nit import WeightedAUC\n    &gt;&gt;&gt; # alternatively: from samesame.nit import DSOS\n    &gt;&gt;&gt; actual = np.array([0, 1, 1, 0])\n    &gt;&gt;&gt; scores = np.array([0.2, 0.8, 0.6, 0.4])\n    &gt;&gt;&gt; wauc = WeightedAUC(actual, scores)\n    &gt;&gt;&gt; print(wauc.pvalue) # doctest: +SKIP\n    &gt;&gt;&gt; print(wauc.bayes_factor) # doctest: +SKIP\n    &gt;&gt;&gt; wauc_ = WeightedAUC.from_samples(scores, scores)\n    &gt;&gt;&gt; isinstance(wauc_, WeightedAUC)\n    True\n    \"\"\"\n\n    def __init__(\n        self,\n        actual: NDArray,\n        predicted: NDArray,\n        n_resamples: int = 9999,\n        rng: np.random.Generator = np.random.default_rng(),\n        n_jobs: int = 1,\n        batch: int | None = None,\n    ):\n        \"\"\"Initialize WeightedAUC.\"\"\"\n        super().__init__(\n            actual=actual,\n            predicted=predicted,\n            metric=wauc,\n            n_resamples=n_resamples,\n            rng=rng,\n            n_jobs=n_jobs,\n            batch=batch,\n            alternative=\"greater\",\n        )\n\n    @cached_property\n    def posterior(self) -&gt; NDArray:\n        \"\"\"\n        Compute the posterior distribution of the WAUC.\n\n        Returns\n        -------\n        NDArray\n            The posterior distribution of the WAUC.\n\n        Notes\n        -----\n        The result is cached to avoid (expensive) recomputation since the\n        posterior distribution uses the Bayesian bootstrap.\n        \"\"\"\n        return bayesian_posterior(\n            self.actual,\n            self.predicted,\n            self.metric,\n            self.n_resamples,\n            self.rng,\n        )\n\n    @cached_property\n    def bayes_factor(self):\n        \"\"\"\n        Compute the Bayes factor using the Bayesian bootstrap.\n\n        Notes\n        -----\n        The result is cached to avoid (expensive) recomputation.\n        \"\"\"\n        bayes_threshold = float(np.mean(self.null))\n        bf_ = _bayes_factor(self.posterior, bayes_threshold)\n        return bf_\n\n    @classmethod\n    def from_samples(\n        cls,\n        first_sample: NDArray,\n        second_sample: NDArray,\n        n_resamples: int = 9999,\n        rng: np.random.Generator = np.random.default_rng(),\n        n_jobs: int = 1,\n        batch: int | None = None,\n    ):\n        \"\"\"\n        Create a WeightedAUC instance from two samples.\n\n        Parameters\n        ----------\n        first_sample : NDArray\n            First sample of scores. These can be binary or continuous.\n        second_sample : NDArray\n            Second sample of scores. These can be binary or continuous.\n\n        Returns\n        -------\n        WeightedAUC\n            An instance of the WeightedAUC class.\n        \"\"\"\n        assert type_of_target(first_sample) == type_of_target(second_sample)\n        samples = (first_sample, second_sample)\n        actual = assign_labels(samples)\n        predicted = concat_samples(samples)\n        return cls(\n            actual,\n            predicted,\n            n_resamples,\n            rng,\n            n_jobs,\n            batch,\n        )\n</code></pre>"},{"location":"api/nit/#samesame.nit.WeightedAUC.bayes_factor","title":"<code>bayes_factor</code>  <code>cached</code> <code>property</code>","text":"<p>Compute the Bayes factor using the Bayesian bootstrap.</p> Notes <p>The result is cached to avoid (expensive) recomputation.</p>"},{"location":"api/nit/#samesame.nit.WeightedAUC.posterior","title":"<code>posterior</code>  <code>cached</code> <code>property</code>","text":"<p>Compute the posterior distribution of the WAUC.</p> <p>Returns:</p> Type Description <code>NDArray</code> <p>The posterior distribution of the WAUC.</p> Notes <p>The result is cached to avoid (expensive) recomputation since the posterior distribution uses the Bayesian bootstrap.</p>"},{"location":"api/nit/#samesame.nit.WeightedAUC.__init__","title":"<code>__init__(actual, predicted, n_resamples=9999, rng=np.random.default_rng(), n_jobs=1, batch=None)</code>","text":"<p>Initialize WeightedAUC.</p> Source code in <code>src/samesame/nit.py</code> <pre><code>def __init__(\n    self,\n    actual: NDArray,\n    predicted: NDArray,\n    n_resamples: int = 9999,\n    rng: np.random.Generator = np.random.default_rng(),\n    n_jobs: int = 1,\n    batch: int | None = None,\n):\n    \"\"\"Initialize WeightedAUC.\"\"\"\n    super().__init__(\n        actual=actual,\n        predicted=predicted,\n        metric=wauc,\n        n_resamples=n_resamples,\n        rng=rng,\n        n_jobs=n_jobs,\n        batch=batch,\n        alternative=\"greater\",\n    )\n</code></pre>"},{"location":"api/nit/#samesame.nit.WeightedAUC.from_samples","title":"<code>from_samples(first_sample, second_sample, n_resamples=9999, rng=np.random.default_rng(), n_jobs=1, batch=None)</code>  <code>classmethod</code>","text":"<p>Create a WeightedAUC instance from two samples.</p> <p>Parameters:</p> Name Type Description Default <code>first_sample</code> <code>NDArray</code> <p>First sample of scores. These can be binary or continuous.</p> required <code>second_sample</code> <code>NDArray</code> <p>Second sample of scores. These can be binary or continuous.</p> required <p>Returns:</p> Type Description <code>WeightedAUC</code> <p>An instance of the WeightedAUC class.</p> Source code in <code>src/samesame/nit.py</code> <pre><code>@classmethod\ndef from_samples(\n    cls,\n    first_sample: NDArray,\n    second_sample: NDArray,\n    n_resamples: int = 9999,\n    rng: np.random.Generator = np.random.default_rng(),\n    n_jobs: int = 1,\n    batch: int | None = None,\n):\n    \"\"\"\n    Create a WeightedAUC instance from two samples.\n\n    Parameters\n    ----------\n    first_sample : NDArray\n        First sample of scores. These can be binary or continuous.\n    second_sample : NDArray\n        Second sample of scores. These can be binary or continuous.\n\n    Returns\n    -------\n    WeightedAUC\n        An instance of the WeightedAUC class.\n    \"\"\"\n    assert type_of_target(first_sample) == type_of_target(second_sample)\n    samples = (first_sample, second_sample)\n    actual = assign_labels(samples)\n    predicted = concat_samples(samples)\n    return cls(\n        actual,\n        predicted,\n        n_resamples,\n        rng,\n        n_jobs,\n        batch,\n    )\n</code></pre>"},{"location":"examples/distribution-shifts/","title":"Test for distribution shifts","text":"<p>Given two datasets <code>sample_P</code> and <code>sample_Q</code> from distributions \\(P\\) and  \\(Q\\), the goal is to estimate a \\(p\\)-value for the null hypothesis of equal distribution \\(P=Q\\). <code>samesame</code> implements classifier two-sample tests (CTSTs) for this use case. But why, you ask? I wax lyrical about CTSTs here.  The tl;dr is they are powerful, modular (flexible) and easy to explain, the  elusive trifecta.</p> <p>CTSTs assign your samples to different classes (you give them labels). <code>sample_P</code>  is the positive class, and <code>sample_Q</code>, the negative one or vice versa. Then, you  fit your favorite classifier to see if it can reliably predict the labels. If it  can, it means the two samples are probably different. If it cannot, the two  are more likely similar enough.</p>"},{"location":"examples/distribution-shifts/#data","title":"Data","text":"<p>Let's see it in action. First, generate some data.</p> <p><pre><code>from sklearn.datasets import make_classification\n# Create a small dataset\nX, y = make_classification(\n    n_samples=100,\n    n_features=4,\n    n_classes=2,\n    random_state=123_456,\n)\n</code></pre> The two samples, <code>sample_P</code> and <code>sample_Q</code>, are concatenated into a single dataset, <code>X</code>. The binary labels, <code>y</code>, indicate sample membership, which examples belong to which sample.</p>"},{"location":"examples/distribution-shifts/#cross-fitted-predictions","title":"Cross-fitted predictions","text":"<p>Let's run the tests using cross-fitting. Cross-fitting estimates the performance of a model on unseen data by using out-of-sample predictions.</p> <pre><code>from sklearn.ensemble import HistGradientBoostingClassifier\nfrom sklearn.model_selection import cross_val_predict\n# Get cross-fitted predictions\ny_gb = cross_val_predict(\n    estimator=HistGradientBoostingClassifier(random_state=123_456),\n    X=X,\n    y=y,\n    cv=10,\n    method='predict_proba',\n)[:, 1]  # Get probabilities for the positive class\n# Binarize predictions to be compatible with some metrics\nfrom sklearn.preprocessing import binarize\ny_gb_binary = binarize(y_gb.reshape(-1, 1), threshold=0.5).ravel()\n</code></pre>"},{"location":"examples/distribution-shifts/#classifier-two-sample-tests-ctsts","title":"Classifier two-sample tests (CTSTs)","text":"<p>We can turn binary performance metrics into statistical tests like so:</p> <pre><code>from samesame.ctst import CTST\nfrom sklearn.metrics import balanced_accuracy_score, matthews_corrcoef, roc_auc_score\n\nmetrics = [balanced_accuracy_score, matthews_corrcoef, roc_auc_score]\nfor metric in metrics:\n    predicted = y_gb_binary if metric != roc_auc_score else y_gb\n    ctst = CTST(actual=y, predicted=predicted, metric=metric)\n    print(f\"{metric.__name__}\")\n    print(f\"\\t statistic: {ctst.statistic:.2f}\")\n    print(f\"\\t p-value: {ctst.pvalue:.2f}\")\n</code></pre> <pre><code>balanced_accuracy_score\n     statistic: 0.86\n     p-value: 0.00\nmatthews_corrcoef\n     statistic: 0.72\n     p-value: 0.00\nroc_auc_score\n     statistic: 0.93\n     p-value: 0.00\n</code></pre> <p>In all 3 cases, we reject the null hypothesis of equal distribution \\(P=Q\\). The classifier, <code>HistGradientBoostingClassifier</code>, was able to distinguish between the two samples.</p>"},{"location":"examples/distribution-shifts/#out-of-bag-predictions","title":"Out-of-bag predictions","text":"<p>Instead of cross-fitting, we can also use out-of-bag predictions. Both cross-fitted and out-of-bag predictions don't lose (too many) samples to estimation whereas sample splitting typically uses half the data for  estimation and half for inference, which incurs a loss of statistical power.</p> <pre><code>from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(\n    n_estimators=500,\n    random_state=123_456,\n    oob_score=True,\n    min_samples_leaf=10,\n)\n# Get out-of-bag predictions\ny_rf = rf.fit(X, y).oob_decision_function_[:, 1]\ny_rf_binary = binarize(y_rf.reshape(-1, 1), threshold=0.5).ravel()\nfor metric in metrics:\n    predicted = y_rf_binary if metric != roc_auc_score else y_rf\n    ctst = CTST(actual=y, predicted=predicted, metric=metric)\n    print(f\"{metric.__name__}\")\n    print(f\"\\t statistic: {ctst.statistic:.2f}\")\n    print(f\"\\t p-value: {ctst.pvalue:.2f}\")\n</code></pre> <p>As before, we reject the null hypothesis of equal distribution \\(P=Q\\). The classifier, <code>RandomForestClassifier</code>, was able to tell apart the two samples.</p> <pre><code>balanced_accuracy_score\n     statistic: 0.88\n     p-value: 0.00\nmatthews_corrcoef\n     statistic: 0.76\n     p-value: 0.00\nroc_auc_score\n     statistic: 0.94\n     p-value: 0.00\n</code></pre>"},{"location":"examples/distribution-shifts/#explanations","title":"Explanations","text":"<p>To explain these results, we can use explainable methods on the fitted classifiers above e.g. <code>rf</code>. These help answer questions such as which features contribute the most to distribution shift (feature importance).</p>"},{"location":"examples/distribution-shifts/#conclusion","title":"Conclusion","text":"<p>And voila\u0300! You have successfully run classifier two-sample tests (CTSTs). Note the flexibility (modularity) of the approach. You can use both  cross-fitted and out-of-bag predictions, instead of sample splitting, for inference. You can use any classifier (e.g., <code>HistGradientBoostingClassifier</code>,  <code>RandomForestClassifier</code>, etc.), and any binary classification metric (e.g., <code>AUC</code>, <code>balanced accuracy</code>, <code>Matthews correlation coefficient</code>, etc.).</p>"},{"location":"examples/noninferiority/","title":"Noninferiority Test","text":"<p>Sometimes the pertinent question is, are we worse off? And when that is so, statistical tests of equal distribution and of mean difference are not the best tools for the job (see here).</p> <p>Noninferiority tests can help but these tests come with their own challenges. Typically, we need to define a pre-specified margin, the minimum meaningful difference needed to sound the alarm. This can be difficult, and controversial, to set in advance even for domain experts.</p>"},{"location":"examples/noninferiority/#d-sos","title":"D-SOS","text":"<p>Enter D-SOS,  short for dataset shift with outlier scores. D-SOS is a robust nonparametric noninferiority test that does not require a pre-specified margin. It tests the null of no adverse shift based on outlier scores i.e. it checks whether the new sample is not substantively worse than the old sample, and not if the two are equal as tests of equal distributions do. This two-sample comparison assumes that we have both a training set, the reference  distribution of outlier scores, and a test set.</p>"},{"location":"examples/noninferiority/#prologue","title":"Prologue","text":"<p>An example best illustrates how to use the method. The  case study, reproduced below, is from a clinical trial.</p> <p>You are a consulting statistician at a pharmaceutical company, charged with designing a study of your company\u2019s new arthritis drug, SASGoBowlFor\u2019Em (abbreviated as \u201cBowl\u201d). Your boss realizes that Bowl is unlikely to demonstrate better efficacy than the gold standard, Armanaleg, but its lower cost will make it an attractive alternative for consumers as long as you can show that the efficacy is about the same.</p> <p>Your boss communicates the following study plans to you:</p> <ul> <li>The outcome to be measured is a \u201crelief score,\u201d which ranges from 0 to 20 and is assumed to be approximately normally distributed.</li> <li>Subjects are to be allocated to Armanaleg and Bowl at a ratio of 2 to 3, respectively.</li> <li>The relief score is to be assessed after four weeks on the treatment.</li> <li>Bowl is expected to be slightly less effective than Armanaleg, with a mean relief score of 9.5 compared to 10 for Armanaleg.</li> <li>The minimally acceptable decrease in relief score is considered to be 2 units, corresponding to a 20% decrease, assuming a mean relief score of 10 for Armanaleg.</li> <li>The standard deviation of the relief score is expected to be approximately 2.25 for each treatment. Common standard deviation will be assumed in the data analysis.</li> <li>The sample size should be sufficient to produce an 85% chance of a significant result\u2014that is, a power of 0.85\u2014at a 0.05 significance level.</li> </ul> <p>While quite a bit of this context is helpful and needed to run Schuirmann\u2019s classic method of two one-sided tests, this is not required for D-SOS. The latter assumes no parametric form for the data (normality), and does not require a pre-specified margin (2 units decrease in relief score).</p>"},{"location":"examples/noninferiority/#data","title":"Data","text":"<p>D-SOS works with outlier scores so we turn these \"relief scores\" into \"discomfort scores\" so that the higher the score, the worse the outcome.</p> <pre><code>import numpy as np\n\ndatalines = \"9 14 13 8 10 5 11 9 12 10 9 11 8 11 \\\n4 8 11 16 12 10 9 10 13 12 11 13 9 4 \\\n7 14 8 4 10 11 7 7 13 8 8 13 10 9 \\\n12 9 11 10 12 7 8 5 10 7 13 12 13 11 \\\n7 12 10 11 10 8 6 9 11 8 5 11 10 8\".split()\nrelief = [float(s) for s in datalines]\ndiscomfort = [max(relief) - s for s in relief]\narmanaleg = np.array(discomfort[:28])\nbowl = np.array(discomfort[28:])\n</code></pre>"},{"location":"examples/noninferiority/#analysis","title":"Analysis","text":"<p>To run the test, we specify <code>armanaleg</code> as the control (reference/first sample) and <code>bowl</code> as the new treatment (second sample).</p> <p><pre><code>from samesame.bayes import as_pvalue\nfrom samesame.nit import DSOS\n# alternatively: from samesame.nit import WeightedAUC as DSOS\n\ndsos = DSOS.from_samples(armanaleg, bowl)\nfrequentist = dsos.pvalue\nbayesian = as_pvalue(dsos.bayes_factor)\n</code></pre> ... And the results? Drumroll, please. We fail to reject the null of no adverse shift. That is, we are not worse off with the new treatment.</p> <pre><code>print(f\"Frequentist p-value: {frequentist:.4f}\")\nprint(f\"Bayesian p-value: {bayesian:.4f}\")\n</code></pre> <pre><code>Frequentist p-value: 0.1215\nBayesian p-value: 0.1159\n</code></pre> <p>This is consistent with the original analysis which rejects the null  (a different null than the D-SOS null!) with a p-value of \\(p = 0.0192\\) and concludes:</p> <p>This suggests, as you\u2019d hoped, that the efficacy of Bowl is not appreciably worse than that of Armanaleg</p>"},{"location":"examples/noninferiority/#epilogue","title":"Epilogue","text":"<p>... did you catch it? Under the hood, the D-SOS test is a classifier two-sample test (CTST). It uses outlier scores as predicted values, the weighted AUC as the performance metric and tests against a one-sided alternative (p-value) instead of a two-sided one.</p>"}]}