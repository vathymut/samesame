{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#samesame","title":"samesame","text":"<p>Same, same but different ...</p> <p><code>samesame</code> implements classifier two-sample tests (CTSTs) and as a bonus extension, a noninferiority test (NIT). These tests are either missing or implemented with significant tradeoffs (looking at you, sample-splitting) in existing libraries.</p> <p><code>samesame</code> is versatile, extensible, lightweight, powerful, and agnostic to your inference strategy so long as it is valid (e.g. cross-fitting, sample splitting, etc.).</p>"},{"location":"#motivation","title":"Motivation","text":"<p><code>samesame</code> is for those who need statistical tests for:</p> <ul> <li>Data validation - Verify that data distributions meet expectations</li> <li>Model performance monitoring - Detect performance degradation over time</li> <li>Drift detection - Identify dataset shifts between training and production</li> <li>Statistical process control - Monitor system behavior and quality</li> <li>Covariate balance - Assess balance in observational studies</li> </ul> <p>A motivating example is available from the related R package <code>dsos</code>, which provides some of the same functionality.</p>"},{"location":"#installation","title":"Installation","text":"<p>To install, run the following command:</p> <pre><code>python -m pip install samesame\n</code></pre>"},{"location":"#quick-start","title":"Quick Start","text":"<p>This example demonstrates the key distinction between tests of equal distribution and noninferiority tests\u2014a critical difference for avoiding false alarms in production systems.</p> <p>Simulate outlier scores to test for no adverse shift:</p> <pre><code>from samesame.ctst import CTST\nfrom samesame.nit import DSOS\nfrom sklearn.metrics import roc_auc_score\nimport numpy as np\n\nn_size = 600\nrng = np.random.default_rng(123_456)\nos_train = rng.normal(size=n_size)\nos_test = rng.normal(size=n_size)\nnull_ctst = CTST.from_samples(os_train, os_test, metric=roc_auc_score)\nnull_dsos = DSOS.from_samples(os_train, os_test)\n</code></pre> <p>Test of equal distribution (CTST): Rejects the null of equal distributions</p> <pre><code>print(f\"{null_ctst.pvalue=:.4f}\")\n# null_ctst.pvalue=0.0358\n</code></pre> <p>Noninferiority test (DSOS): Fails to reject the null of no adverse shift</p> <pre><code>print(f\"{null_dsos.pvalue=:.4f}\")\n# null_dsos.pvalue=0.9500\n</code></pre> <p>Key insight: While the test sample (<code>os_test</code>) has a statistically different distribution from the training sample (<code>os_train</code>), it does not contain disproportionally more outliers. This distinction is exactly what <code>samesame</code> highlights\u2014many practitioners conflate \"different distribution\" with \"problematic shift,\" but <code>samesame</code> helps you distinguish between the two.</p>"},{"location":"#usage","title":"Usage","text":""},{"location":"#functionality","title":"Functionality","text":"<p>Below, you will find an overview of common modules in <code>samesame</code>.</p> Function Module Bayesian inference <code>samesame.bayes</code> Classifier two-sample tests (CTSTs) <code>samesame.ctst</code> Noninferiority tests (NITs) <code>samesame.nit</code>"},{"location":"#attributes","title":"Attributes","text":"<p>When the method is a statistical test, <code>samesame</code> saves (stores) the results of some potentially computationally intensive results in attributes. These attributes, when available, can be accessed as follows.</p> Attribute Description <code>.statistic</code> The test statistic for the hypothesis. <code>.null</code> The null distribution for the hypothesis. <code>.pvalue</code> The p-value for the hypothesis. <code>.posterior</code> The posterior distribution for the hypothesis. <code>.bayes_factor</code> The bayes factor for the hypothesis."},{"location":"#examples","title":"Examples","text":"<p>To get started, please see the examples in the docs.</p>"},{"location":"#dependencies","title":"Dependencies","text":"<p><code>samesame</code> has minimal dependencies beyond the Python standard library, making it a lightweight addition to most machine learning projects. It is built on top of, and fully compatible with, scikit-learn and numpy.</p>"},{"location":"api/bayes/","title":"bayes","text":"<p>Functions for working with p-values and Bayes factors.</p> <p>This module provides functions for converting between p-values and Bayes factors, as well as calculating Bayes factors from posterior distributions. These are useful for hypothesis testing and interpreting statistical evidence.</p>"},{"location":"api/bayes/#samesame.bayes.as_bf","title":"<code>as_bf(pvalue)</code>","text":"<p>Convert a one-sided p-value to a Bayes factor.</p> <p>This Bayes factor quantifies evidence in favour of a directional effect over the null hypothesis of no such effect.</p> <p>Parameters:</p> Name Type Description Default <code>pvalue</code> <code>NDArray | float</code> <p>The p-value(s) to be converted to Bayes factor(s). Can be a single value or an array of values.</p> required <p>Returns:</p> Type Description <code>NDArray | float</code> <p>The corresponding Bayes factor(s). The return type matches the input type.</p> See Also <p>as_pvalue : Convert a Bayes factor to a p-value.</p> Notes <p>The Bayes factor is derived from the one-sided p-value using a Bayesian interpretation, which quantifies evidence in favor of a directional effect over the null hypothesis of no such effect.</p> <p>See [1]_ for theoretical details and implications.</p> References <p>.. [1] Marsman, Maarten, and Eric-Jan Wagenmakers. \"Three Insights from    a Bayesian Interpretation of the One-Sided P Value.\" Educational and    Psychological Measurement, vol. 77, no. 3, 2017, pp. 529-539.    doi:10.1177/0013164416669201.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from samesame.bayes import as_bf\n&gt;&gt;&gt; as_bf(0.5)\nnp.float64(1.0)\n&gt;&gt;&gt; as_bf(np.array([0.05, 0.1, 0.5]))\narray([19., 9., 1.])\n</code></pre> Source code in <code>src/samesame/bayes.py</code> <pre><code>def as_bf(\n    pvalue: NDArray | float,\n) -&gt; NDArray | float:\n    \"\"\"\n    Convert a one-sided p-value to a Bayes factor.\n\n    This Bayes factor quantifies evidence in favour of a directional effect\n    over the null hypothesis of no such effect.\n\n    Parameters\n    ----------\n    pvalue : NDArray | float\n        The p-value(s) to be converted to Bayes factor(s). Can be a single\n        value or an array of values.\n\n    Returns\n    -------\n    NDArray | float\n        The corresponding Bayes factor(s). The return type matches the\n        input type.\n\n    See Also\n    --------\n    as_pvalue : Convert a Bayes factor to a p-value.\n\n    Notes\n    -----\n    The Bayes factor is derived from the one-sided p-value using a\n    Bayesian interpretation, which quantifies evidence in favor of a\n    directional effect over the null hypothesis of no such effect.\n\n    See [1]_ for theoretical details and implications.\n\n    References\n    ----------\n    .. [1] Marsman, Maarten, and Eric-Jan Wagenmakers. \"Three Insights from\n       a Bayesian Interpretation of the One-Sided P Value.\" *Educational and\n       Psychological Measurement*, vol. 77, no. 3, 2017, pp. 529-539.\n       doi:10.1177/0013164416669201.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from samesame.bayes import as_bf\n    &gt;&gt;&gt; as_bf(0.5)\n    np.float64(1.0)\n    &gt;&gt;&gt; as_bf(np.array([0.05, 0.1, 0.5])) # doctest: +NORMALIZE_WHITESPACE\n    array([19., 9., 1.])\n    \"\"\"\n    if np.any(np.logical_or(pvalue &gt;= 1, pvalue &lt;= 0)):\n        raise ValueError(\"pvalue must be within the open interval (0, 1).\")\n    pvalue = np.clip(pvalue, 1e-10, 1 - 1e-10)  # Avoid numerical issues at extremes\n    return 1.0 / np.exp(logit(pvalue))  # evidence in favor of an effect\n</code></pre>"},{"location":"api/bayes/#samesame.bayes.as_pvalue","title":"<code>as_pvalue(bayes_factor)</code>","text":"<p>Convert a Bayes factor of a directional effect to a one-sided p-value.</p> <p>The Bayes factor quantifies evidence in favour of a directional effect over the null hypothesis of no such effect.</p> <p>Parameters:</p> Name Type Description Default <code>bayes_factor</code> <code>float | NDArray</code> <p>The Bayes factor(s) to be converted to p-value(s). Can be a single value or an array of values.</p> required <p>Returns:</p> Type Description <code>float | NDArray</code> <p>The corresponding p-value(s). The return type matches the input type.</p> See Also <p>as_pvalue : Convert a Bayes factor to a p-value.</p> Notes <p>The equivalence between the Bayes factor in favor of a directional effect and the one-sided p-value for the null of no such effect is discussed in [1]_.</p> References <p>.. [1] Marsman, Maarten, and Eric-Jan Wagenmakers. \"Three Insights from a    Bayesian Interpretation of the One-Sided P Value.\" Educational and    Psychological Measurement, vol. 77, no. 3, 2017, pp. 529\u2013539,    https://doi.org/10.1177/0013164416669201.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from samesame.bayes import as_pvalue\n&gt;&gt;&gt; as_pvalue(1)\nnp.float64(0.5)\n&gt;&gt;&gt; as_pvalue(np.array([19.0, 9.0, 1.0]))\narray([0.05, 0.1 , 0.5 ])\n</code></pre> Source code in <code>src/samesame/bayes.py</code> <pre><code>def as_pvalue(\n    bayes_factor: float | NDArray,\n) -&gt; float | NDArray:\n    \"\"\"\n    Convert a Bayes factor of a directional effect to a one-sided p-value.\n\n    The Bayes factor quantifies evidence in favour of a directional effect\n    over the null hypothesis of no such effect.\n\n    Parameters\n    ----------\n    bayes_factor : float | NDArray\n        The Bayes factor(s) to be converted to p-value(s). Can be a single\n        value or an array of values.\n\n    Returns\n    -------\n    float | NDArray\n        The corresponding p-value(s). The return type matches the input type.\n\n    See Also\n    --------\n    as_pvalue : Convert a Bayes factor to a p-value.\n\n    Notes\n    -----\n    The equivalence between the Bayes factor in favor of a directional effect\n    and the one-sided p-value for the null of no such effect is discussed in\n    [1]_.\n\n    References\n    ----------\n    .. [1] Marsman, Maarten, and Eric-Jan Wagenmakers. \"Three Insights from a\n       Bayesian Interpretation of the One-Sided P Value.\" *Educational and\n       Psychological Measurement*, vol. 77, no. 3, 2017, pp. 529\u2013539,\n       https://doi.org/10.1177/0013164416669201.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from samesame.bayes import as_pvalue\n    &gt;&gt;&gt; as_pvalue(1)\n    np.float64(0.5)\n    &gt;&gt;&gt; as_pvalue(np.array([19.0, 9.0, 1.0])) # doctest: +NORMALIZE_WHITESPACE\n    array([0.05, 0.1 , 0.5 ])\n    \"\"\"\n    if np.any(bayes_factor &lt;= 0):\n        raise ValueError(\"bayes_factor must be strictly positive.\")\n    bf_ = np.clip(bayes_factor, 1e-10, 1e10)  # Ensure numerical stability\n    pvalue = expit(-np.log(bf_))\n    return pvalue\n</code></pre>"},{"location":"api/bayes/#samesame.bayes.bayes_factor","title":"<code>bayes_factor(posterior, threshold=0.0, adjustment=0)</code>","text":"<p>Compute the Bayes factor for a test of direction given a threshold.</p> <p>The Bayes factor quantifies the evidence in favor of the hypothesis that the posterior distribution exceeds the given threshold compared to the hypothesis that it does not.</p> <p>Parameters:</p> Name Type Description Default <code>posterior</code> <code>NDArray</code> <p>An array of posterior samples.</p> required <code>threshold</code> <code>float</code> <p>The threshold value to test against. Default is 0.0.</p> <code>0.0</code> <code>adjustment</code> <code>(0, 1)</code> <p>Adjustment to apply to the Bayes factor calculation. Default is 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>float</code> <p>The computed Bayes factor.</p> See Also <p>as_pvalue : Convert a Bayes factor to a p-value.</p> <p>as_bf : Convert a p-value to a Bayes factor.</p> Notes <p>The Bayes factor is a measure of evidence which compares the likelihood of the data under two competing hypotheses. In this function, the numerator represents the proportion of posterior samples exceeding the threshold, while the denominator represents the proportion of samples not exceeding the threshold. If all samples exceed the threshold, the Bayes factor is set to infinity, indicating overwhelming evidence in favor of the hypothesis.</p> <p>The adjustment parameter allows for slight modifications to the Bayes factor calculation, which can be useful in specific contexts such as sensitivity analyses.</p> <p>See [1]_ for further theoretical details and practical implications of this approach.</p> References <p>.. [1] Marsman, Maarten, and Eric-Jan Wagenmakers. \"Three Insights from a    Bayesian Interpretation of the One-Sided P Value.\" Educational and    Psychological Measurement, vol. 77, no. 3, 2017, pp. 529-539.    doi:10.1177/0013164416669201.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from samesame.bayes import bayes_factor\n&gt;&gt;&gt; posterior_samples = np.array([0.2, 0.5, 0.8, 0.9])\n&gt;&gt;&gt; bayes_factor(posterior_samples, threshold=0.5)\nnp.float64(1.0)\n</code></pre> Source code in <code>src/samesame/bayes.py</code> <pre><code>def bayes_factor(\n    posterior: NDArray,\n    threshold: float = 0.0,\n    adjustment: Literal[0, 1] = 0,\n) -&gt; float:\n    \"\"\"\n    Compute the Bayes factor for a test of direction given a threshold.\n\n    The Bayes factor quantifies the evidence in favor of the hypothesis that\n    the posterior distribution exceeds the given threshold compared to the\n    hypothesis that it does not.\n\n    Parameters\n    ----------\n    posterior : NDArray\n        An array of posterior samples.\n    threshold : float, optional\n        The threshold value to test against. Default is 0.0.\n    adjustment : {0, 1}, optional\n        Adjustment to apply to the Bayes factor calculation. Default is 0.\n\n    Returns\n    -------\n    float\n        The computed Bayes factor.\n\n    See Also\n    --------\n    as_pvalue : Convert a Bayes factor to a p-value.\n\n    as_bf : Convert a p-value to a Bayes factor.\n\n    Notes\n    -----\n    The Bayes factor is a measure of evidence which compares the likelihood of\n    the data under two competing hypotheses. In this function, the numerator\n    represents the proportion of posterior samples exceeding the threshold,\n    while the denominator represents the proportion of samples not exceeding\n    the threshold. If all samples exceed the threshold, the Bayes factor is\n    set to infinity, indicating overwhelming evidence in favor of the\n    hypothesis.\n\n    The adjustment parameter allows for slight modifications to the Bayes\n    factor calculation, which can be useful in specific contexts such as\n    sensitivity analyses.\n\n    See [1]_ for further theoretical details and practical implications of\n    this approach.\n\n    References\n    ----------\n    .. [1] Marsman, Maarten, and Eric-Jan Wagenmakers. \"Three Insights from a\n       Bayesian Interpretation of the One-Sided P Value.\" *Educational and\n       Psychological Measurement*, vol. 77, no. 3, 2017, pp. 529-539.\n       doi:10.1177/0013164416669201.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from samesame.bayes import bayes_factor\n    &gt;&gt;&gt; posterior_samples = np.array([0.2, 0.5, 0.8, 0.9])\n    &gt;&gt;&gt; bayes_factor(posterior_samples, threshold=0.5)\n    np.float64(1.0)\n    \"\"\"\n    return _bayes_factor(posterior, threshold, adjustment)\n</code></pre>"},{"location":"api/ctst/","title":"ctst","text":"<p>Classifier two-sample tests (CTST) from binary classification metrics.</p> <p>The classifier two-sample test broadly consists of three steps: (1) training a classifier, (2) scoring the two samples and (3) turning a test statistic into a p-value from these scores. This test statistic can be the performance metric of a binary classifier such as the (weighted) area under the receiver operating characteristic curve, the Matthews correlation coefficient, and the (balanced) accuracy. This module tackles step (3).</p> References <p>.. [1] Lopez-Paz, David, and Maxime Oquab. \"Revisiting Classifier Two-Sample    Tests.\" International Conference on Learning Representations. 2017.</p> <p>.. [2] Friedman, Jerome. \"On multivariate goodness-of-fit and two-sample    testing.\" No. SLAC-PUB-10325. SLAC National Accelerator Laboratory (SLAC),    Menlo Park, CA (United States), 2004.</p> <p>.. [3] K\u00fcbler, Jonas M., et al. \"Automl two-sample test.\" Advances in Neural    Information Processing Systems 35 (2022): 15929-15941.</p> <p>.. [4] Ci\u00e9men\u00e7on, St\u00e9phan, Marine Depecker, and Nicolas Vayatis. \"AUC    optimization and the two-sample problem.\" Proceedings of the 23rd    International Conference on Neural Information Processing Systems. 2009.</p> <p>.. [5] Hediger, Simon, Loris Michel, and Jeffrey N\u00e4f. \"On the use of random    forest for two-sample testing.\" Computational Statistics &amp; Data Analysis    170 (2022): 107435.</p> <p>.. [6] Kim, Ilmun, et al. \"Classification accuracy as a proxy for two-sample    testing.\" Annals of Statistics 49.1 (2021): 411-434.</p>"},{"location":"api/ctst/#samesame.ctst.CTST","title":"<code>CTST</code>  <code>dataclass</code>","text":"<p>Classifier two-sample test (CTST) using a binary classification metric.</p> <p>This test compares scores (predictions) from two independent samples. Rejecting the null implies that scoring is not random and that the classifier is able to distinguish between the two samples.</p> <p>Attributes:</p> Name Type Description <code>actual</code> <code>NDArray</code> <p>Binary indicator for sample membership.</p> <code>predicted</code> <code>NDArray</code> <p>Estimated (predicted) scores for corresponding samples in <code>actual</code>.</p> <code>metric</code> <code>Callable</code> <p>A callable that conforms to scikit-learn metric API. This function must take two positional arguments e.g. <code>y_true</code> and <code>y_pred</code>.</p> <code>n_resamples</code> <code>(int, optional)</code> <p>Number of resampling iterations, by default 9999.</p> <code>rng</code> <code>(Generator, optional)</code> <p>Random number generator, by default np.random.default_rng().</p> <code>n_jobs</code> <code>(int, optional)</code> <p>Number of parallel jobs, by default 1.</p> <code>batch</code> <code>(int or None, optional)</code> <p>Batch size for parallel processing, by default None.</p> <code>alternative</code> <code>({'less', 'greater', 'two-sided'}, optional)</code> <p>Defines the alternative hypothesis. Default is 'two-sided'.</p> Notes <p>The null distribution is based on permutations. See <code>scipy.stats.permutation_test</code> for more details.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from sklearn.metrics import matthews_corrcoef, roc_auc_score\n&gt;&gt;&gt; from samesame.ctst import CTST\n&gt;&gt;&gt; actual = np.array([0, 1, 1, 0])\n&gt;&gt;&gt; scores = np.array([0.2, 0.8, 0.6, 0.4])\n&gt;&gt;&gt; ctst_mcc = CTST(actual, scores, metric=matthews_corrcoef)\n&gt;&gt;&gt; ctst_auc = CTST(actual, scores, metric=roc_auc_score)\n&gt;&gt;&gt; print(ctst_mcc.pvalue)\n&gt;&gt;&gt; print(ctst_auc.pvalue)\n&gt;&gt;&gt; ctst_ = CTST.from_samples(scores, scores, metric=roc_auc_score)\n&gt;&gt;&gt; isinstance(ctst_, CTST)\nTrue\n</code></pre> Source code in <code>src/samesame/ctst.py</code> <pre><code>@dataclass\nclass CTST:\n    \"\"\"\n    Classifier two-sample test (CTST) using a binary classification metric.\n\n    This test compares scores (predictions) from two independent samples.\n    Rejecting the null implies that scoring is not random and that the\n    classifier is able to distinguish between the two samples.\n\n    Attributes\n    ----------\n    actual : NDArray\n        Binary indicator for sample membership.\n    predicted : NDArray\n        Estimated (predicted) scores for corresponding samples in `actual`.\n    metric : Callable\n        A callable that conforms to scikit-learn metric API. This function\n        must take two positional arguments e.g. `y_true` and `y_pred`.\n    n_resamples : int, optional\n        Number of resampling iterations, by default 9999.\n    rng : np.random.Generator, optional\n        Random number generator, by default np.random.default_rng().\n    n_jobs : int, optional\n        Number of parallel jobs, by default 1.\n    batch : int or None, optional\n        Batch size for parallel processing, by default None.\n    alternative : {'less', 'greater', 'two-sided'}, optional\n        Defines the alternative hypothesis. Default is 'two-sided'.\n\n    Notes\n    -----\n    The null distribution is based on permutations.\n    See `scipy.stats.permutation_test` for more details.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from sklearn.metrics import matthews_corrcoef, roc_auc_score\n    &gt;&gt;&gt; from samesame.ctst import CTST\n    &gt;&gt;&gt; actual = np.array([0, 1, 1, 0])\n    &gt;&gt;&gt; scores = np.array([0.2, 0.8, 0.6, 0.4])\n    &gt;&gt;&gt; ctst_mcc = CTST(actual, scores, metric=matthews_corrcoef)\n    &gt;&gt;&gt; ctst_auc = CTST(actual, scores, metric=roc_auc_score)\n    &gt;&gt;&gt; print(ctst_mcc.pvalue) # doctest: +SKIP\n    &gt;&gt;&gt; print(ctst_auc.pvalue) # doctest: +SKIP\n    &gt;&gt;&gt; ctst_ = CTST.from_samples(scores, scores, metric=roc_auc_score)\n    &gt;&gt;&gt; isinstance(ctst_, CTST)\n    True\n    \"\"\"\n\n    actual: NDArray = field(repr=False)\n    predicted: NDArray = field(repr=False)\n    metric: Callable\n    n_resamples: int = 9999\n    rng: np.random.Generator = np.random.default_rng()\n    n_jobs: int = 1\n    batch: int | None = None\n    alternative: Literal[\"less\", \"greater\", \"two-sided\"] = \"two-sided\"\n\n    def __post_init__(self):\n        \"\"\"Validate inputs.\"\"\"\n        self.actual = column_or_1d(self.actual)\n        self.predicted = column_or_1d(self.predicted)\n        check_consistent_length(self.actual, self.predicted)\n        assert type_of_target(self.actual, \"actual\") == \"binary\"\n        type_predicted = type_of_target(self.predicted, \"predicted\")\n        assert type_predicted in (\n            \"binary\",\n            \"continuous\",\n            \"multiclass\",\n        ), f\"Expected 'predicted' to be binary or continuous, got {type_predicted}.\"\n        assert check_metric_function(self.metric), (\n            f\"'metric' expects a callable that conforms to scikit-learn metric. \"\n            f\"{signature(self.metric)=} does not.\"\n        )\n\n    @cached_property\n    def _result(self):\n        def statistic(*args):\n            return self.metric(args[0], args[1])\n\n        return permutation_test(\n            data=(self.actual, self.predicted),\n            statistic=statistic,\n            permutation_type=\"pairings\",\n            n_resamples=self.n_resamples,\n            alternative=self.alternative,\n            random_state=self.rng,\n        )\n\n    @cached_property\n    def statistic(self) -&gt; float:\n        \"\"\"\n        Compute the observed test statistic.\n\n        Returns\n        -------\n        float\n            The test statistic.\n\n        Notes\n        -----\n        The result is cached to avoid (expensive) recomputation.\n        \"\"\"\n        return self._result.statistic\n\n    @cached_property\n    def null(self) -&gt; NDArray:\n        \"\"\"\n        Compute the null distribution of the test statistic.\n\n        Notes\n        -----\n        The result is cached to avoid (expensive) recomputation since the\n        null distribution requires permutations.\n        \"\"\"\n        return self._result.null_distribution\n\n    @cached_property\n    def pvalue(self):\n        \"\"\"\n        Compute the p-value using permutations.\n\n        Notes\n        -----\n        The result is cached to avoid (expensive) recomputation.\n        \"\"\"\n        return self._result.pvalue\n\n    @classmethod\n    def from_samples(\n        cls,\n        first_sample: NDArray,\n        second_sample: NDArray,\n        metric: Callable,\n        n_resamples: int = 9999,\n        rng: np.random.Generator = np.random.default_rng(),\n        n_jobs: int = 1,\n        batch: int | None = None,\n        alternative: Literal[\"less\", \"greater\", \"two-sided\"] = \"two-sided\",\n    ):\n        \"\"\"\n        Create a CTST instance from two samples.\n\n        Parameters\n        ----------\n        first_sample : NDArray\n            First sample of scores. These can be binary or continuous.\n        second_sample : NDArray\n            Second sample of scores. These can be binary or continuous.\n\n        Returns\n        -------\n        CTST\n            An instance of the CTST class.\n        \"\"\"\n        assert type_of_target(first_sample) == type_of_target(second_sample)\n        samples = (first_sample, second_sample)\n        actual = assign_labels(samples)\n        predicted = concat_samples(samples)\n        return cls(\n            actual,\n            predicted,\n            metric,\n            n_resamples,\n            rng,\n            n_jobs,\n            batch,\n            alternative,\n        )\n</code></pre>"},{"location":"api/ctst/#samesame.ctst.CTST.null","title":"<code>null</code>  <code>cached</code> <code>property</code>","text":"<p>Compute the null distribution of the test statistic.</p> Notes <p>The result is cached to avoid (expensive) recomputation since the null distribution requires permutations.</p>"},{"location":"api/ctst/#samesame.ctst.CTST.pvalue","title":"<code>pvalue</code>  <code>cached</code> <code>property</code>","text":"<p>Compute the p-value using permutations.</p> Notes <p>The result is cached to avoid (expensive) recomputation.</p>"},{"location":"api/ctst/#samesame.ctst.CTST.statistic","title":"<code>statistic</code>  <code>cached</code> <code>property</code>","text":"<p>Compute the observed test statistic.</p> <p>Returns:</p> Type Description <code>float</code> <p>The test statistic.</p> Notes <p>The result is cached to avoid (expensive) recomputation.</p>"},{"location":"api/ctst/#samesame.ctst.CTST.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate inputs.</p> Source code in <code>src/samesame/ctst.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Validate inputs.\"\"\"\n    self.actual = column_or_1d(self.actual)\n    self.predicted = column_or_1d(self.predicted)\n    check_consistent_length(self.actual, self.predicted)\n    assert type_of_target(self.actual, \"actual\") == \"binary\"\n    type_predicted = type_of_target(self.predicted, \"predicted\")\n    assert type_predicted in (\n        \"binary\",\n        \"continuous\",\n        \"multiclass\",\n    ), f\"Expected 'predicted' to be binary or continuous, got {type_predicted}.\"\n    assert check_metric_function(self.metric), (\n        f\"'metric' expects a callable that conforms to scikit-learn metric. \"\n        f\"{signature(self.metric)=} does not.\"\n    )\n</code></pre>"},{"location":"api/ctst/#samesame.ctst.CTST.from_samples","title":"<code>from_samples(first_sample, second_sample, metric, n_resamples=9999, rng=np.random.default_rng(), n_jobs=1, batch=None, alternative='two-sided')</code>  <code>classmethod</code>","text":"<p>Create a CTST instance from two samples.</p> <p>Parameters:</p> Name Type Description Default <code>first_sample</code> <code>NDArray</code> <p>First sample of scores. These can be binary or continuous.</p> required <code>second_sample</code> <code>NDArray</code> <p>Second sample of scores. These can be binary or continuous.</p> required <p>Returns:</p> Type Description <code>CTST</code> <p>An instance of the CTST class.</p> Source code in <code>src/samesame/ctst.py</code> <pre><code>@classmethod\ndef from_samples(\n    cls,\n    first_sample: NDArray,\n    second_sample: NDArray,\n    metric: Callable,\n    n_resamples: int = 9999,\n    rng: np.random.Generator = np.random.default_rng(),\n    n_jobs: int = 1,\n    batch: int | None = None,\n    alternative: Literal[\"less\", \"greater\", \"two-sided\"] = \"two-sided\",\n):\n    \"\"\"\n    Create a CTST instance from two samples.\n\n    Parameters\n    ----------\n    first_sample : NDArray\n        First sample of scores. These can be binary or continuous.\n    second_sample : NDArray\n        Second sample of scores. These can be binary or continuous.\n\n    Returns\n    -------\n    CTST\n        An instance of the CTST class.\n    \"\"\"\n    assert type_of_target(first_sample) == type_of_target(second_sample)\n    samples = (first_sample, second_sample)\n    actual = assign_labels(samples)\n    predicted = concat_samples(samples)\n    return cls(\n        actual,\n        predicted,\n        metric,\n        n_resamples,\n        rng,\n        n_jobs,\n        batch,\n        alternative,\n    )\n</code></pre>"},{"location":"api/nit/","title":"nit","text":""},{"location":"api/nit/#samesame.nit.WeightedAUC","title":"<code>WeightedAUC</code>  <code>dataclass</code>","text":"<p>               Bases: <code>CTST</code></p> <p>Two-sample test for no adverse shift using the weighted AUC (WAUC).</p> <p>This test compares scores from two independent samples. We reject the null hypothesis of no adverse shift for unusually high values of the WAUC i.e. when the second sample is relatively worse than the first one. This is a robust nonparametric noninferiority test (NIT) with no pre-specified margin. It can be used, amongst other things, to detect dataset shift with outlier scores, hence the DSOS acronym.</p> <p>Attributes:</p> Name Type Description <code>actual</code> <code>NDArray</code> <p>Binary indicator for sample membership.</p> <code>predicted</code> <code>NDArray</code> <p>Estimated (predicted) scores for corresponding samples in <code>actual</code>.</p> <code>n_resamples</code> <code>(int, optional)</code> <p>Number of resampling iterations, by default 9999.</p> <code>rng</code> <code>(Generator, optional)</code> <p>Random number generator, by default np.random.default_rng().</p> <code>n_jobs</code> <code>(int, optional)</code> <p>Number of parallel jobs, by default 1.</p> <code>batch</code> <code>(int or None, optional)</code> <p>Batch size for parallel processing, by default None.</p> See Also <p>bayes.as_bf : Convert a one-sided p-value to a Bayes factor.</p> <p>bayes.as_pvalue : Convert a Bayes factor to a one-sided p-value.</p> Notes <p>The frequentist null distribution of the WAUC is based on permutations [1]. The Bayesian posterior distribution of the WAUC is based on the Bayesian bootstrap [2]. Because this is a one-tailed test of direction (it asks the question, 'are we worse off?'), we can convert a one-sided p-value into a Bayes factor and vice versa. We can also use these p-values for sequential testing [3].</p> <p>The test assumes that <code>predicted</code> are outlier scores and/or encode some notions of outlyingness; higher value of <code>predicted</code> indicates worse outcomes.</p> References <p>.. [1] Kamulete, Vathy M. \"Test for non-negligible adverse shifts.\"    Uncertainty in Artificial Intelligence. PMLR, 2022.</p> <p>.. [2] Gu, Jiezhun, Subhashis Ghosal, and Anindya Roy. \"Bayesian bootstrap    estimation of ROC curve.\" Statistics in medicine 27.26 (2008): 5407-5420.</p> <p>.. [3] Kamulete, Vathy M. \"Are you OK? A Bayesian Sequential Test for    Adverse Shift.\" 2025.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from samesame.nit import WeightedAUC\n&gt;&gt;&gt; # alternatively: from samesame.nit import DSOS\n&gt;&gt;&gt; actual = np.array([0, 1, 1, 0])\n&gt;&gt;&gt; scores = np.array([0.2, 0.8, 0.6, 0.4])\n&gt;&gt;&gt; wauc = WeightedAUC(actual, scores)\n&gt;&gt;&gt; print(wauc.pvalue)\n&gt;&gt;&gt; print(wauc.bayes_factor)\n&gt;&gt;&gt; wauc_ = WeightedAUC.from_samples(scores, scores)\n&gt;&gt;&gt; isinstance(wauc_, WeightedAUC)\nTrue\n</code></pre> Source code in <code>src/samesame/nit.py</code> <pre><code>@dataclass\nclass WeightedAUC(CTST):\n    \"\"\"\n    Two-sample test for no adverse shift using the weighted AUC (WAUC).\n\n    This test compares scores from two independent samples. We reject the\n    null hypothesis of no adverse shift for unusually high values of the WAUC\n    i.e. when the second sample is relatively worse than the first one. This\n    is a robust nonparametric noninferiority test (NIT) with no pre-specified\n    margin. It can be used, amongst other things, to detect dataset shift with\n    outlier scores, hence the DSOS acronym.\n\n    Attributes\n    ----------\n    actual : NDArray\n        Binary indicator for sample membership.\n    predicted : NDArray\n        Estimated (predicted) scores for corresponding samples in `actual`.\n    n_resamples : int, optional\n        Number of resampling iterations, by default 9999.\n    rng : np.random.Generator, optional\n        Random number generator, by default np.random.default_rng().\n    n_jobs : int, optional\n        Number of parallel jobs, by default 1.\n    batch : int or None, optional\n        Batch size for parallel processing, by default None.\n\n    See Also\n    --------\n    bayes.as_bf : Convert a one-sided p-value to a Bayes factor.\n\n    bayes.as_pvalue : Convert a Bayes factor to a one-sided p-value.\n\n    Notes\n    -----\n    The frequentist null distribution of the WAUC is based on permutations\n    [1]. The Bayesian posterior distribution of the WAUC is based on the\n    Bayesian bootstrap [2]. Because this is a one-tailed test of direction\n    (it asks the question, 'are we worse off?'), we can convert a one-sided\n    p-value into a Bayes factor and vice versa. We can also use these p-values\n    for sequential testing [3].\n\n    The test assumes that `predicted` are outlier scores and/or encode some\n    notions of outlyingness; higher value of `predicted` indicates worse\n    outcomes.\n\n    References\n    ----------\n    .. [1] Kamulete, Vathy M. \"Test for non-negligible adverse shifts.\"\n       Uncertainty in Artificial Intelligence. PMLR, 2022.\n\n    .. [2] Gu, Jiezhun, Subhashis Ghosal, and Anindya Roy. \"Bayesian bootstrap\n       estimation of ROC curve.\" Statistics in medicine 27.26 (2008): 5407-5420.\n\n    .. [3] Kamulete, Vathy M. \"Are you OK? A Bayesian Sequential Test for\n       Adverse Shift.\" 2025.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from samesame.nit import WeightedAUC\n    &gt;&gt;&gt; # alternatively: from samesame.nit import DSOS\n    &gt;&gt;&gt; actual = np.array([0, 1, 1, 0])\n    &gt;&gt;&gt; scores = np.array([0.2, 0.8, 0.6, 0.4])\n    &gt;&gt;&gt; wauc = WeightedAUC(actual, scores)\n    &gt;&gt;&gt; print(wauc.pvalue) # doctest: +SKIP\n    &gt;&gt;&gt; print(wauc.bayes_factor) # doctest: +SKIP\n    &gt;&gt;&gt; wauc_ = WeightedAUC.from_samples(scores, scores)\n    &gt;&gt;&gt; isinstance(wauc_, WeightedAUC)\n    True\n    \"\"\"\n\n    def __init__(\n        self,\n        actual: NDArray,\n        predicted: NDArray,\n        n_resamples: int = 9999,\n        rng: np.random.Generator = np.random.default_rng(),\n        n_jobs: int = 1,\n        batch: int | None = None,\n    ):\n        \"\"\"Initialize WeightedAUC.\"\"\"\n        super().__init__(\n            actual=actual,\n            predicted=predicted,\n            metric=wauc,\n            n_resamples=n_resamples,\n            rng=rng,\n            n_jobs=n_jobs,\n            batch=batch,\n            alternative=\"greater\",\n        )\n\n    @cached_property\n    def posterior(self) -&gt; NDArray:\n        \"\"\"\n        Compute the posterior distribution of the WAUC.\n\n        Returns\n        -------\n        NDArray\n            The posterior distribution of the WAUC.\n\n        Notes\n        -----\n        The result is cached to avoid (expensive) recomputation since the\n        posterior distribution uses the Bayesian bootstrap.\n        \"\"\"\n        return bayesian_posterior(\n            self.actual,\n            self.predicted,\n            self.metric,\n            self.n_resamples,\n            self.rng,\n        )\n\n    @cached_property\n    def bayes_factor(self):\n        \"\"\"\n        Compute the Bayes factor using the Bayesian bootstrap.\n\n        Notes\n        -----\n        The result is cached to avoid (expensive) recomputation.\n        \"\"\"\n        bayes_threshold = float(np.mean(self.null))\n        bf_ = _bayes_factor(self.posterior, bayes_threshold)\n        return bf_\n\n    @classmethod\n    def from_samples(\n        cls,\n        first_sample: NDArray,\n        second_sample: NDArray,\n        n_resamples: int = 9999,\n        rng: np.random.Generator = np.random.default_rng(),\n        n_jobs: int = 1,\n        batch: int | None = None,\n    ):\n        \"\"\"\n        Create a WeightedAUC instance from two samples.\n\n        Parameters\n        ----------\n        first_sample : NDArray\n            First sample of scores. These can be binary or continuous.\n        second_sample : NDArray\n            Second sample of scores. These can be binary or continuous.\n\n        Returns\n        -------\n        WeightedAUC\n            An instance of the WeightedAUC class.\n        \"\"\"\n        assert type_of_target(first_sample) == type_of_target(second_sample)\n        samples = (first_sample, second_sample)\n        actual = assign_labels(samples)\n        predicted = concat_samples(samples)\n        return cls(\n            actual,\n            predicted,\n            n_resamples,\n            rng,\n            n_jobs,\n            batch,\n        )\n</code></pre>"},{"location":"api/nit/#samesame.nit.WeightedAUC.bayes_factor","title":"<code>bayes_factor</code>  <code>cached</code> <code>property</code>","text":"<p>Compute the Bayes factor using the Bayesian bootstrap.</p> Notes <p>The result is cached to avoid (expensive) recomputation.</p>"},{"location":"api/nit/#samesame.nit.WeightedAUC.posterior","title":"<code>posterior</code>  <code>cached</code> <code>property</code>","text":"<p>Compute the posterior distribution of the WAUC.</p> <p>Returns:</p> Type Description <code>NDArray</code> <p>The posterior distribution of the WAUC.</p> Notes <p>The result is cached to avoid (expensive) recomputation since the posterior distribution uses the Bayesian bootstrap.</p>"},{"location":"api/nit/#samesame.nit.WeightedAUC.__init__","title":"<code>__init__(actual, predicted, n_resamples=9999, rng=np.random.default_rng(), n_jobs=1, batch=None)</code>","text":"<p>Initialize WeightedAUC.</p> Source code in <code>src/samesame/nit.py</code> <pre><code>def __init__(\n    self,\n    actual: NDArray,\n    predicted: NDArray,\n    n_resamples: int = 9999,\n    rng: np.random.Generator = np.random.default_rng(),\n    n_jobs: int = 1,\n    batch: int | None = None,\n):\n    \"\"\"Initialize WeightedAUC.\"\"\"\n    super().__init__(\n        actual=actual,\n        predicted=predicted,\n        metric=wauc,\n        n_resamples=n_resamples,\n        rng=rng,\n        n_jobs=n_jobs,\n        batch=batch,\n        alternative=\"greater\",\n    )\n</code></pre>"},{"location":"api/nit/#samesame.nit.WeightedAUC.from_samples","title":"<code>from_samples(first_sample, second_sample, n_resamples=9999, rng=np.random.default_rng(), n_jobs=1, batch=None)</code>  <code>classmethod</code>","text":"<p>Create a WeightedAUC instance from two samples.</p> <p>Parameters:</p> Name Type Description Default <code>first_sample</code> <code>NDArray</code> <p>First sample of scores. These can be binary or continuous.</p> required <code>second_sample</code> <code>NDArray</code> <p>Second sample of scores. These can be binary or continuous.</p> required <p>Returns:</p> Type Description <code>WeightedAUC</code> <p>An instance of the WeightedAUC class.</p> Source code in <code>src/samesame/nit.py</code> <pre><code>@classmethod\ndef from_samples(\n    cls,\n    first_sample: NDArray,\n    second_sample: NDArray,\n    n_resamples: int = 9999,\n    rng: np.random.Generator = np.random.default_rng(),\n    n_jobs: int = 1,\n    batch: int | None = None,\n):\n    \"\"\"\n    Create a WeightedAUC instance from two samples.\n\n    Parameters\n    ----------\n    first_sample : NDArray\n        First sample of scores. These can be binary or continuous.\n    second_sample : NDArray\n        Second sample of scores. These can be binary or continuous.\n\n    Returns\n    -------\n    WeightedAUC\n        An instance of the WeightedAUC class.\n    \"\"\"\n    assert type_of_target(first_sample) == type_of_target(second_sample)\n    samples = (first_sample, second_sample)\n    actual = assign_labels(samples)\n    predicted = concat_samples(samples)\n    return cls(\n        actual,\n        predicted,\n        n_resamples,\n        rng,\n        n_jobs,\n        batch,\n    )\n</code></pre>"},{"location":"examples/distribution-shifts/","title":"Test for Distribution Shifts","text":""},{"location":"examples/distribution-shifts/#overview","title":"Overview","text":"<p>Given two datasets <code>sample_P</code> and <code>sample_Q</code> from distributions \\(P\\) and \\(Q\\), the goal is to estimate a \\(p\\)-value for the null hypothesis of equal distribution, \\(P=Q\\). <code>samesame</code> implements classifier two-sample tests (CTSTs) for this use case.</p> <p>Why CTSTs? They are powerful, modular (flexible), and easy to explain\u2014the elusive trifecta. For a deeper dive, see this discussion.</p>"},{"location":"examples/distribution-shifts/#how-ctsts-work","title":"How CTSTs Work","text":"<p>CTSTs leverage machine learning classifiers to test for distribution differences as follows:</p> <ol> <li>Label samples - Assign class labels to indicate sample membership (<code>sample_P</code> = positive class, <code>sample_Q</code> = negative class)</li> <li>Train classifier - Fit a classifier to predict sample labels based on features</li> <li>Evaluate performance - If the classifier achieves high accuracy, the samples are likely different; if accuracy is near random chance, they are likely similar</li> </ol> <p>This approach is versatile: you can use any classifier and any binary classification metric (AUC, balanced accuracy, etc.) without making strong distributional assumptions.</p>"},{"location":"examples/distribution-shifts/#data","title":"Data","text":"<p>To demonstrate CTSTs in action, let's create a synthetic dataset with two distinguishable distributions:</p> <pre><code>from sklearn.datasets import make_classification\n\n# Create a small dataset with clear class separation\nX, y = make_classification(\n    n_samples=100,\n    n_features=4,\n    n_classes=2,\n    random_state=123_456,\n)\n</code></pre> <p>The two samples are concatenated into a single feature matrix <code>X</code> with shape <code>(100, 4)</code>. The binary labels <code>y</code> indicate sample membership\u2014which observations belong to <code>sample_P</code> (positive class, <code>y=1</code>) and which belong to <code>sample_Q</code> (negative class, <code>y=0</code>).</p>"},{"location":"examples/distribution-shifts/#cross-fitted-predictions","title":"Cross-fitted Predictions","text":"<p>Cross-fitting estimates classifier performance on unseen data using out-of-sample predictions. This approach is more efficient than sample splitting, which typically uses 50% of data for training and 50% for testing\u2014resulting in a loss of statistical power. Cross-fitting and out-of-bag methods use more data for inference while preserving statistical validity.</p> <p>For more alternatives to sample splitting, see this paper.</p> <pre><code>from sklearn.ensemble import HistGradientBoostingClassifier\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.preprocessing import binarize\n\n# Get cross-fitted probability predictions (out-of-sample)\ny_gb = cross_val_predict(\n    estimator=HistGradientBoostingClassifier(random_state=123_456),\n    X=X,\n    y=y,\n    cv=10,\n    method='predict_proba',\n)[:, 1]  # Extract probabilities for the positive class\n\n# Binarize predictions for metrics that require binary labels\ny_gb_binary = binarize(y_gb.reshape(-1, 1), threshold=0.5).ravel()\n</code></pre>"},{"location":"examples/distribution-shifts/#classifier-two-sample-tests-ctsts","title":"Classifier Two-Sample Tests (CTSTs)","text":"<p>Now we run CTSTs using three different classification metrics.</p> <pre><code>from samesame.ctst import CTST\nfrom sklearn.metrics import balanced_accuracy_score, matthews_corrcoef, roc_auc_score\n\n# Define metrics to evaluate\nmetrics = [balanced_accuracy_score, matthews_corrcoef, roc_auc_score]\n\n# Run CTST for each metric\nfor metric in metrics:\n    # Use binary predictions for metrics that require them; probabilities for AUC\n    predicted = y_gb_binary if metric != roc_auc_score else y_gb\n    ctst = CTST(actual=y, predicted=predicted, metric=metric)\n    print(f\"{metric.__name__}\")\n    print(f\"\\t statistic: {ctst.statistic:.2f}\")\n    print(f\"\\t p-value: {ctst.pvalue:.2f}\")\n</code></pre> <p>Output:</p> <pre><code>balanced_accuracy_score\n     statistic: 0.86\n     p-value: 0.00\nmatthews_corrcoef\n     statistic: 0.72\n     p-value: 0.00\nroc_auc_score\n     statistic: 0.93\n     p-value: 0.00\n</code></pre> <p>In all three cases, we reject the null hypothesis of equal distribution (\\(P=Q\\)) at conventional significance levels. The <code>HistGradientBoostingClassifier</code> performed well across all metrics, providing strong evidence that <code>sample_P</code> and <code>sample_Q</code> come from different distributions.</p>"},{"location":"examples/distribution-shifts/#out-of-bag-predictions","title":"Out-of-Bag Predictions","text":"<p>An alternative to cross-fitting is using out-of-bag (OOB) predictions from ensemble methods like <code>RandomForestClassifier</code>. Both cross-fitted and OOB predictions mitigate the downsides of sample splitting.</p> <pre><code>from sklearn.ensemble import RandomForestClassifier\n\n# Train random forest with out-of-bag score enabled\nrf = RandomForestClassifier(\n    n_estimators=500,\n    random_state=123_456,\n    oob_score=True,\n    min_samples_leaf=10,\n)\n\n# Get out-of-bag decision function scores (probabilities)\ny_rf = rf.fit(X, y).oob_decision_function_[:, 1]\n\n# Binarize for metrics requiring binary predictions\ny_rf_binary = binarize(y_rf.reshape(-1, 1), threshold=0.5).ravel()\n\n# Run CTST for each metric\nfor metric in metrics:\n    predicted = y_rf_binary if metric != roc_auc_score else y_rf\n    ctst = CTST(actual=y, predicted=predicted, metric=metric)\n    print(f\"{metric.__name__}\")\n    print(f\"\\t statistic: {ctst.statistic:.2f}\")\n    print(f\"\\t p-value: {ctst.pvalue:.2f}\")\n</code></pre> <p>Output:</p> <pre><code>balanced_accuracy_score\n     statistic: 0.88\n     p-value: 0.00\nmatthews_corrcoef\n     statistic: 0.76\n     p-value: 0.00\nroc_auc_score\n     statistic: 0.94\n     p-value: 0.00\n</code></pre> <p>Again, we reject the null hypothesis of equal distribution. The <code>RandomForestClassifier</code> performed on par with the gradient boosting model, providing evidence of distribution shift. Both classifiers agree that the samples are distinguishable.</p>"},{"location":"examples/distribution-shifts/#interpreting-results","title":"Interpreting Results","text":"<p>Important distinction: A significant CTST indicates the distributions are different, but not necessarily that the difference is problematic. In production settings (e.g., model monitoring, data validation), you may want to use a noninferiority test instead to determine if the shift is substantive enough to warrant action.</p> <p>This example demonstrates the modularity of CTSTs:</p> <ul> <li>Classifiers: Any classifier works (here we used gradient boosting and random forests)</li> <li>Metrics: Any classification metric can be used (balanced accuracy, AUC, Matthews correlation coefficient, etc.)</li> <li>Inference methods: Both cross-fitting and out-of-bag predictions avoid the sample-splitting penalty</li> </ul> <p>This flexibility makes CTSTs useful for diverse applications without compromising statistical power for valid inference.</p>"},{"location":"examples/distribution-shifts/#explanations","title":"Explanations","text":"<p>To understand why the two samples are different, you can apply explainable methods to the fitted classifiers. For example, feature importance from <code>RandomForestClassifier</code> or model-agnostic techniques like SHAP can reveal which features contribute most to the distribution shift.</p> <pre><code># Example: Feature importance from random forest\nimport pandas as pd\n\nimportances = rf.feature_importances_\nfeature_names = [f\"Feature {i}\" for i in range(X.shape[1])]\nimportance_df = pd.DataFrame(\n    {'Feature': feature_names, 'Importance': importances}\n).sort_values('Importance', ascending=False)\nprint(importance_df)\n</code></pre> <p>This helps answer, which features are driving the distribution shift? This information is valuable for understanding root causes and deciding on remedial actions.</p>"},{"location":"examples/noninferiority/","title":"Noninferiority Test","text":"<p>Noninferiority tests (NITs) ask whether a new sample (or treatment) is not meaningfully worse than a reference. <code>samesame</code> implements D-SOS (Dataset Shift with Outlier Scores), a robust, nonparametric noninferiority test that operates on outlier scores and does not require a pre-specified margin.</p>"},{"location":"examples/noninferiority/#when-to-use-this","title":"When to use this","text":"<ul> <li>Use CTSTs when the question is, \"are the  two distributions different?\" \u2014 i.e., you want to detect any distributional  difference.</li> <li>Use NITs when the question is, \"is the new  sample not substantially worse than the reference?\" \u2014 i.e., you care about  adverse shifts rather than any difference.</li> </ul> <p>The two approaches address different scientific questions; they are complementary rather than interchangeable.</p>"},{"location":"examples/noninferiority/#d-sos-at-a-glance","title":"D-SOS at a glance","text":"<p>D-SOS transforms the problem of noninferiority testing (NITs) into a CTST by treating outlier scores as the classifier's predicted values, using a weighted AUC metric, and testing a one-sided alternative. The key advantages are:</p> <ul> <li>Nonparametric: no normality assumption required.</li> <li>No pre-specified margin needed: the method is robust to how \"meaningful\"  differences are defined in practice.</li> <li>Works with any sensible outlier scoring method (isolation forest, deep  models, domain-specific scores, etc.).</li> </ul> <p>The test focuses on whether the test sample contains disproportionately more high outlier scores than the reference, which aligns with the goal of detecting adverse shifts.</p>"},{"location":"examples/noninferiority/#prologue","title":"Prologue","text":"<p>The following clinical-trial style example illustrates the difference between classic noninferiority approaches and D-SOS. The motivating study (from SAS's case study) compares a new, cheaper drug \"Bowl\" to the standard \"Armanaleg.\"</p> <p>The original study uses parametric assumptions and a pre-specified margin. For D-SOS we only need outlier/discomfort scores that reflect worse outcomes as higher values.</p>"},{"location":"examples/noninferiority/#data","title":"Data","text":"<p>We convert reported relief scores so that higher numbers indicate worse outcomes, producing outlier/discomfort scores suitable for D-SOS.</p> <pre><code>import numpy as np\n\ndatalines = (\n  \"9 14 13 8 10 5 11 9 12 10 9 11 8 11 \"\n  \"4 8 11 16 12 10 9 10 13 12 11 13 9 4 \"\n  \"7 14 8 4 10 11 7 7 13 8 8 13 10 9 \"\n  \"12 9 11 10 12 7 8 5 10 7 13 12 13 11 \"\n  \"7 12 10 11 10 8 6 9 11 8 5 11 10 8\"\n).split()\nrelief = [float(s) for s in datalines]\ndiscomfort = [max(relief) - s for s in relief]\narmanaleg = np.array(discomfort[:28])\nbowl = np.array(discomfort[28:])\n</code></pre>"},{"location":"examples/noninferiority/#analysis","title":"Analysis","text":"<p>Below we run D-SOS using <code>DSOS.from_samples</code>, treating <code>armanaleg</code> as the reference (control) and <code>bowl</code> as the new treatment. We report the frequentist p-value and an optional Bayesian conversion of the Bayes factor.</p> <pre><code>from samesame.bayes import as_pvalue\nfrom samesame.nit import DSOS\n\ndsos = DSOS.from_samples(armanaleg, bowl)\nfrequentist = dsos.pvalue\nbayesian = as_pvalue(dsos.bayes_factor)\nprint(f\"Frequentist p-value: {frequentist:.4f}\")\nprint(f\"Bayesian p-value: {bayesian:.4f}\")\n</code></pre> <p>Typical output (reproduced from the example):</p> <pre><code>Frequentist p-value: 0.1215\nBayesian p-value: 0.1159\n</code></pre> <p>We fail to reject the null of no adverse shift \u2014 the new treatment (Bowl) is not shown to be meaningfully worse than the reference under the D-SOS criterion.</p> <p>This is consistent with the original analysis from classical two-sided or one-sided parametric noninferiority tests, which concludes:</p> <p>This suggests, as you\u2019d hoped, that the efficacy of Bowl is not appreciably worse than that of Armanaleg</p>"},{"location":"examples/noninferiority/#practical-recommendations","title":"Practical Recommendations","text":"<ul> <li>Choose an outlier score that captures the phenomenon you care about (e.g.,  clinical outcomes, high reconstruction error, extreme probability  values, etc.).</li> <li>Consider reporting both distributional and noninferiority results when  monitoring production systems: a distributional difference (e.g. CTST) does not  always imply a practically meaningful or adverse change (e.g. DSOS).</li> </ul>"}]}