{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#samesame","title":"samesame","text":"<p>Same, same but different ...</p> <p><code>samesame</code> implements classifier two-sample tests (CTSTs) and as a bonus extension, a noninferiority test (NIT). These tests are either missing or implemented with significant tradeoffs (looking at you, sample-splitting) in existing libraries.</p> <p><code>samesame</code> is versatile, extensible, lightweight, powerful, and agnostic to your inference strategy so long as it is valid (e.g. cross-fitting, sample splitting, etc.).</p>"},{"location":"#motivation","title":"Motivation","text":"<p><code>samesame</code> is for those who need statistical tests for:</p> <ul> <li>Data validation - Verify that data distributions meet expectations</li> <li>Model performance monitoring - Detect performance degradation over time</li> <li>Drift detection - Identify dataset shifts between training and production</li> <li>Statistical process control - Monitor system behavior and quality</li> <li>Covariate balance - Assess balance in observational studies</li> </ul> <p>A motivating example is available from the related R package <code>dsos</code>, which provides some of the same functionality.</p>"},{"location":"#installation","title":"Installation","text":"<p>To install, run the following command:</p> <pre><code>python -m pip install samesame\n</code></pre>"},{"location":"#quick-start","title":"Quick Start","text":"<p>This example demonstrates the key distinction between tests of equal distribution and noninferiority tests\u2014a critical difference for avoiding false alarms in production systems.</p> <p>Simulate outlier scores to test for no adverse shift:</p> <pre><code>from samesame.ctst import CTST\nfrom samesame.nit import DSOS\nfrom sklearn.metrics import roc_auc_score\nimport numpy as np\n\nn_size = 600\nrng = np.random.default_rng(123_456)\nos_train = rng.normal(size=n_size)\nos_test = rng.normal(size=n_size)\nnull_ctst = CTST.from_samples(os_train, os_test, metric=roc_auc_score)\nnull_dsos = DSOS.from_samples(os_train, os_test)\n</code></pre> <p>Test of equal distribution (CTST): Rejects the null of equal distributions</p> <pre><code>print(f\"{null_ctst.pvalue=:.4f}\")\n# null_ctst.pvalue=0.0358\n</code></pre> <p>Noninferiority test (DSOS): Fails to reject the null of no adverse shift</p> <pre><code>print(f\"{null_dsos.pvalue=:.4f}\")\n# null_dsos.pvalue=0.9500\n</code></pre> <p>Key insight: While the test sample (<code>os_test</code>) has a statistically different distribution from the training sample (<code>os_train</code>), it does not contain disproportionally more outliers. This distinction is exactly what <code>samesame</code> highlights\u2014many practitioners conflate \"different distribution\" with \"problematic shift,\" but <code>samesame</code> helps you distinguish between the two.</p>"},{"location":"#usage","title":"Usage","text":""},{"location":"#functionality","title":"Functionality","text":"<p>Below, you will find an overview of common modules in <code>samesame</code>.</p> Function Module Bayesian inference <code>samesame.bayes</code> Classifier two-sample tests (CTSTs) <code>samesame.ctst</code> Noninferiority tests (NITs) <code>samesame.nit</code>"},{"location":"#attributes","title":"Attributes","text":"<p>When the method is a statistical test, <code>samesame</code> saves (stores) the results of some potentially computationally intensive results in attributes. These attributes, when available, can be accessed as follows.</p> Attribute Description <code>.statistic</code> The test statistic for the hypothesis. <code>.null</code> The null distribution for the hypothesis. <code>.pvalue</code> The p-value for the hypothesis. <code>.posterior</code> The posterior distribution for the hypothesis. <code>.bayes_factor</code> The bayes factor for the hypothesis."},{"location":"#examples","title":"Examples","text":"<p>To get started, please see the examples in the docs.</p>"},{"location":"#dependencies","title":"Dependencies","text":"<p><code>samesame</code> has minimal dependencies beyond the Python standard library, making it a lightweight addition to most machine learning projects. It is built on top of, and fully compatible with, scikit-learn and numpy.</p>"},{"location":"api/bayes/","title":"bayes","text":"<p>Functions for working with p-values and Bayes factors.</p> <p>This module provides functions for converting between p-values and Bayes factors, as well as calculating Bayes factors from posterior distributions. These are useful for hypothesis testing and interpreting statistical evidence.</p>"},{"location":"api/bayes/#samesame.bayes.as_bf","title":"<code>as_bf(pvalue)</code>","text":"<p>Convert a one-sided p-value to a Bayes factor.</p> <p>This Bayes factor quantifies evidence in favour of a directional effect over the null hypothesis of no such effect.</p> <p>Parameters:</p> Name Type Description Default <code>pvalue</code> <code>NDArray | float</code> <p>The p-value(s) to be converted to Bayes factor(s). Can be a single value or an array of values.</p> required <p>Returns:</p> Type Description <code>NDArray | float</code> <p>The corresponding Bayes factor(s). The return type matches the input type.</p> See Also <p>as_pvalue : Convert a Bayes factor to a p-value.</p> Notes <p>The Bayes factor is derived from the one-sided p-value using a Bayesian interpretation, which quantifies evidence in favor of a directional effect over the null hypothesis of no such effect.</p> <p>See [1]_ for theoretical details and implications.</p> References <p>.. [1] Marsman, Maarten, and Eric-Jan Wagenmakers. \"Three Insights from    a Bayesian Interpretation of the One-Sided P Value.\" Educational and    Psychological Measurement, vol. 77, no. 3, 2017, pp. 529-539.    doi:10.1177/0013164416669201.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from samesame.bayes import as_bf\n&gt;&gt;&gt; as_bf(0.5)\nnp.float64(1.0)\n&gt;&gt;&gt; as_bf(np.array([0.05, 0.1, 0.5]))\narray([19., 9., 1.])\n</code></pre> Source code in <code>src/samesame/bayes.py</code> <pre><code>def as_bf(\n    pvalue: NDArray | float,\n) -&gt; NDArray | float:\n    \"\"\"\n    Convert a one-sided p-value to a Bayes factor.\n\n    This Bayes factor quantifies evidence in favour of a directional effect\n    over the null hypothesis of no such effect.\n\n    Parameters\n    ----------\n    pvalue : NDArray | float\n        The p-value(s) to be converted to Bayes factor(s). Can be a single\n        value or an array of values.\n\n    Returns\n    -------\n    NDArray | float\n        The corresponding Bayes factor(s). The return type matches the\n        input type.\n\n    See Also\n    --------\n    as_pvalue : Convert a Bayes factor to a p-value.\n\n    Notes\n    -----\n    The Bayes factor is derived from the one-sided p-value using a\n    Bayesian interpretation, which quantifies evidence in favor of a\n    directional effect over the null hypothesis of no such effect.\n\n    See [1]_ for theoretical details and implications.\n\n    References\n    ----------\n    .. [1] Marsman, Maarten, and Eric-Jan Wagenmakers. \"Three Insights from\n       a Bayesian Interpretation of the One-Sided P Value.\" *Educational and\n       Psychological Measurement*, vol. 77, no. 3, 2017, pp. 529-539.\n       doi:10.1177/0013164416669201.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from samesame.bayes import as_bf\n    &gt;&gt;&gt; as_bf(0.5)\n    np.float64(1.0)\n    &gt;&gt;&gt; as_bf(np.array([0.05, 0.1, 0.5])) # doctest: +NORMALIZE_WHITESPACE\n    array([19., 9., 1.])\n    \"\"\"\n    if np.any(np.logical_or(pvalue &gt;= 1, pvalue &lt;= 0)):\n        raise ValueError(\"pvalue must be within the open interval (0, 1).\")\n    pvalue = np.clip(pvalue, 1e-10, 1 - 1e-10)  # Avoid numerical issues at extremes\n    return 1.0 / np.exp(logit(pvalue))  # evidence in favor of an effect\n</code></pre>"},{"location":"api/bayes/#samesame.bayes.as_pvalue","title":"<code>as_pvalue(bayes_factor)</code>","text":"<p>Convert a Bayes factor of a directional effect to a one-sided p-value.</p> <p>The Bayes factor quantifies evidence in favour of a directional effect over the null hypothesis of no such effect.</p> <p>Parameters:</p> Name Type Description Default <code>bayes_factor</code> <code>float | NDArray</code> <p>The Bayes factor(s) to be converted to p-value(s). Can be a single value or an array of values.</p> required <p>Returns:</p> Type Description <code>float | NDArray</code> <p>The corresponding p-value(s). The return type matches the input type.</p> See Also <p>as_pvalue : Convert a Bayes factor to a p-value.</p> Notes <p>The equivalence between the Bayes factor in favor of a directional effect and the one-sided p-value for the null of no such effect is discussed in [1]_.</p> References <p>.. [1] Marsman, Maarten, and Eric-Jan Wagenmakers. \"Three Insights from a    Bayesian Interpretation of the One-Sided P Value.\" Educational and    Psychological Measurement, vol. 77, no. 3, 2017, pp. 529\u2013539,    https://doi.org/10.1177/0013164416669201.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from samesame.bayes import as_pvalue\n&gt;&gt;&gt; as_pvalue(1)\nnp.float64(0.5)\n&gt;&gt;&gt; as_pvalue(np.array([19.0, 9.0, 1.0]))\narray([0.05, 0.1 , 0.5 ])\n</code></pre> Source code in <code>src/samesame/bayes.py</code> <pre><code>def as_pvalue(\n    bayes_factor: float | NDArray,\n) -&gt; float | NDArray:\n    \"\"\"\n    Convert a Bayes factor of a directional effect to a one-sided p-value.\n\n    The Bayes factor quantifies evidence in favour of a directional effect\n    over the null hypothesis of no such effect.\n\n    Parameters\n    ----------\n    bayes_factor : float | NDArray\n        The Bayes factor(s) to be converted to p-value(s). Can be a single\n        value or an array of values.\n\n    Returns\n    -------\n    float | NDArray\n        The corresponding p-value(s). The return type matches the input type.\n\n    See Also\n    --------\n    as_pvalue : Convert a Bayes factor to a p-value.\n\n    Notes\n    -----\n    The equivalence between the Bayes factor in favor of a directional effect\n    and the one-sided p-value for the null of no such effect is discussed in\n    [1]_.\n\n    References\n    ----------\n    .. [1] Marsman, Maarten, and Eric-Jan Wagenmakers. \"Three Insights from a\n       Bayesian Interpretation of the One-Sided P Value.\" *Educational and\n       Psychological Measurement*, vol. 77, no. 3, 2017, pp. 529\u2013539,\n       https://doi.org/10.1177/0013164416669201.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from samesame.bayes import as_pvalue\n    &gt;&gt;&gt; as_pvalue(1)\n    np.float64(0.5)\n    &gt;&gt;&gt; as_pvalue(np.array([19.0, 9.0, 1.0])) # doctest: +NORMALIZE_WHITESPACE\n    array([0.05, 0.1 , 0.5 ])\n    \"\"\"\n    if np.any(bayes_factor &lt;= 0):\n        raise ValueError(\"bayes_factor must be strictly positive.\")\n    bf_ = np.clip(bayes_factor, 1e-10, 1e10)  # Ensure numerical stability\n    pvalue = expit(-np.log(bf_))\n    return pvalue\n</code></pre>"},{"location":"api/bayes/#samesame.bayes.bayes_factor","title":"<code>bayes_factor(posterior, threshold=0.0, adjustment=0)</code>","text":"<p>Compute the Bayes factor for a test of direction given a threshold.</p> <p>The Bayes factor quantifies the evidence in favor of the hypothesis that the posterior distribution exceeds the given threshold compared to the hypothesis that it does not.</p> <p>Parameters:</p> Name Type Description Default <code>posterior</code> <code>NDArray</code> <p>An array of posterior samples.</p> required <code>threshold</code> <code>float</code> <p>The threshold value to test against. Default is 0.0.</p> <code>0.0</code> <code>adjustment</code> <code>(0, 1)</code> <p>Adjustment to apply to the Bayes factor calculation. Default is 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>float</code> <p>The computed Bayes factor.</p> See Also <p>as_pvalue : Convert a Bayes factor to a p-value.</p> <p>as_bf : Convert a p-value to a Bayes factor.</p> Notes <p>The Bayes factor is a measure of evidence which compares the likelihood of the data under two competing hypotheses. In this function, the numerator represents the proportion of posterior samples exceeding the threshold, while the denominator represents the proportion of samples not exceeding the threshold. If all samples exceed the threshold, the Bayes factor is set to infinity, indicating overwhelming evidence in favor of the hypothesis.</p> <p>The adjustment parameter allows for slight modifications to the Bayes factor calculation, which can be useful in specific contexts such as sensitivity analyses.</p> <p>See [1]_ for further theoretical details and practical implications of this approach.</p> References <p>.. [1] Marsman, Maarten, and Eric-Jan Wagenmakers. \"Three Insights from a    Bayesian Interpretation of the One-Sided P Value.\" Educational and    Psychological Measurement, vol. 77, no. 3, 2017, pp. 529-539.    doi:10.1177/0013164416669201.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from samesame.bayes import bayes_factor\n&gt;&gt;&gt; posterior_samples = np.array([0.2, 0.5, 0.8, 0.9])\n&gt;&gt;&gt; bayes_factor(posterior_samples, threshold=0.5)\nnp.float64(1.0)\n</code></pre> Source code in <code>src/samesame/bayes.py</code> <pre><code>def bayes_factor(\n    posterior: NDArray,\n    threshold: float = 0.0,\n    adjustment: Literal[0, 1] = 0,\n) -&gt; float:\n    \"\"\"\n    Compute the Bayes factor for a test of direction given a threshold.\n\n    The Bayes factor quantifies the evidence in favor of the hypothesis that\n    the posterior distribution exceeds the given threshold compared to the\n    hypothesis that it does not.\n\n    Parameters\n    ----------\n    posterior : NDArray\n        An array of posterior samples.\n    threshold : float, optional\n        The threshold value to test against. Default is 0.0.\n    adjustment : {0, 1}, optional\n        Adjustment to apply to the Bayes factor calculation. Default is 0.\n\n    Returns\n    -------\n    float\n        The computed Bayes factor.\n\n    See Also\n    --------\n    as_pvalue : Convert a Bayes factor to a p-value.\n\n    as_bf : Convert a p-value to a Bayes factor.\n\n    Notes\n    -----\n    The Bayes factor is a measure of evidence which compares the likelihood of\n    the data under two competing hypotheses. In this function, the numerator\n    represents the proportion of posterior samples exceeding the threshold,\n    while the denominator represents the proportion of samples not exceeding\n    the threshold. If all samples exceed the threshold, the Bayes factor is\n    set to infinity, indicating overwhelming evidence in favor of the\n    hypothesis.\n\n    The adjustment parameter allows for slight modifications to the Bayes\n    factor calculation, which can be useful in specific contexts such as\n    sensitivity analyses.\n\n    See [1]_ for further theoretical details and practical implications of\n    this approach.\n\n    References\n    ----------\n    .. [1] Marsman, Maarten, and Eric-Jan Wagenmakers. \"Three Insights from a\n       Bayesian Interpretation of the One-Sided P Value.\" *Educational and\n       Psychological Measurement*, vol. 77, no. 3, 2017, pp. 529-539.\n       doi:10.1177/0013164416669201.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from samesame.bayes import bayes_factor\n    &gt;&gt;&gt; posterior_samples = np.array([0.2, 0.5, 0.8, 0.9])\n    &gt;&gt;&gt; bayes_factor(posterior_samples, threshold=0.5)\n    np.float64(1.0)\n    \"\"\"\n    return _bayes_factor(posterior, threshold, adjustment)\n</code></pre>"},{"location":"api/ctst/","title":"ctst","text":"<p>Classifier two-sample tests (CTST) from binary classification metrics.</p> <p>The classifier two-sample test broadly consists of three steps: (1) training a classifier, (2) scoring the two samples and (3) turning a test statistic into a p-value from these scores. This test statistic can be the performance metric of a binary classifier such as the (weighted) area under the receiver operating characteristic curve, the Matthews correlation coefficient, and the (balanced) accuracy. This module tackles step (3).</p> References <p>.. [1] Lopez-Paz, David, and Maxime Oquab. \"Revisiting Classifier Two-Sample    Tests.\" International Conference on Learning Representations. 2017.</p> <p>.. [2] Friedman, Jerome. \"On multivariate goodness-of-fit and two-sample    testing.\" No. SLAC-PUB-10325. SLAC National Accelerator Laboratory (SLAC),    Menlo Park, CA (United States), 2004.</p> <p>.. [3] K\u00fcbler, Jonas M., et al. \"Automl two-sample test.\" Advances in Neural    Information Processing Systems 35 (2022): 15929-15941.</p> <p>.. [4] Ci\u00e9men\u00e7on, St\u00e9phan, Marine Depecker, and Nicolas Vayatis. \"AUC    optimization and the two-sample problem.\" Proceedings of the 23rd    International Conference on Neural Information Processing Systems. 2009.</p> <p>.. [5] Hediger, Simon, Loris Michel, and Jeffrey N\u00e4f. \"On the use of random    forest for two-sample testing.\" Computational Statistics &amp; Data Analysis    170 (2022): 107435.</p> <p>.. [6] Kim, Ilmun, et al. \"Classification accuracy as a proxy for two-sample    testing.\" Annals of Statistics 49.1 (2021): 411-434.</p>"},{"location":"api/ctst/#samesame.ctst.CTST","title":"<code>CTST</code>  <code>dataclass</code>","text":"<p>Classifier two-sample test (CTST) using a binary classification metric.</p> <p>This test compares scores (predictions) from two independent samples. Rejecting the null implies that scoring is not random and that the classifier is able to distinguish between the two samples.</p> <p>Attributes:</p> Name Type Description <code>actual</code> <code>NDArray</code> <p>Binary indicator for sample membership.</p> <code>predicted</code> <code>NDArray</code> <p>Estimated (predicted) scores for corresponding samples in <code>actual</code>.</p> <code>metric</code> <code>Callable</code> <p>A callable that conforms to scikit-learn metric API. This function must take two positional arguments e.g. <code>y_true</code> and <code>y_pred</code>.</p> <code>n_resamples</code> <code>(int, optional)</code> <p>Number of resampling iterations, by default 9999.</p> <code>rng</code> <code>(Generator, optional)</code> <p>Random number generator, by default np.random.default_rng().</p> <code>n_jobs</code> <code>(int, optional)</code> <p>Number of parallel jobs, by default 1.</p> <code>batch</code> <code>(int or None, optional)</code> <p>Batch size for parallel processing, by default None.</p> <code>alternative</code> <code>({'less', 'greater', 'two-sided'}, optional)</code> <p>Defines the alternative hypothesis. Default is 'two-sided'.</p> Notes <p>The null distribution is based on permutations. See <code>scipy.stats.permutation_test</code> for more details.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from sklearn.metrics import matthews_corrcoef, roc_auc_score\n&gt;&gt;&gt; from samesame.ctst import CTST\n&gt;&gt;&gt; actual = np.array([0, 1, 1, 0])\n&gt;&gt;&gt; scores = np.array([0.2, 0.8, 0.6, 0.4])\n&gt;&gt;&gt; ctst_mcc = CTST(actual, scores, metric=matthews_corrcoef)\n&gt;&gt;&gt; ctst_auc = CTST(actual, scores, metric=roc_auc_score)\n&gt;&gt;&gt; print(ctst_mcc.pvalue)\n&gt;&gt;&gt; print(ctst_auc.pvalue)\n&gt;&gt;&gt; ctst_ = CTST.from_samples(scores, scores, metric=roc_auc_score)\n&gt;&gt;&gt; isinstance(ctst_, CTST)\nTrue\n</code></pre> Source code in <code>src/samesame/ctst.py</code> <pre><code>@dataclass\nclass CTST:\n    \"\"\"\n    Classifier two-sample test (CTST) using a binary classification metric.\n\n    This test compares scores (predictions) from two independent samples.\n    Rejecting the null implies that scoring is not random and that the\n    classifier is able to distinguish between the two samples.\n\n    Attributes\n    ----------\n    actual : NDArray\n        Binary indicator for sample membership.\n    predicted : NDArray\n        Estimated (predicted) scores for corresponding samples in `actual`.\n    metric : Callable\n        A callable that conforms to scikit-learn metric API. This function\n        must take two positional arguments e.g. `y_true` and `y_pred`.\n    n_resamples : int, optional\n        Number of resampling iterations, by default 9999.\n    rng : np.random.Generator, optional\n        Random number generator, by default np.random.default_rng().\n    n_jobs : int, optional\n        Number of parallel jobs, by default 1.\n    batch : int or None, optional\n        Batch size for parallel processing, by default None.\n    alternative : {'less', 'greater', 'two-sided'}, optional\n        Defines the alternative hypothesis. Default is 'two-sided'.\n\n    Notes\n    -----\n    The null distribution is based on permutations.\n    See `scipy.stats.permutation_test` for more details.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from sklearn.metrics import matthews_corrcoef, roc_auc_score\n    &gt;&gt;&gt; from samesame.ctst import CTST\n    &gt;&gt;&gt; actual = np.array([0, 1, 1, 0])\n    &gt;&gt;&gt; scores = np.array([0.2, 0.8, 0.6, 0.4])\n    &gt;&gt;&gt; ctst_mcc = CTST(actual, scores, metric=matthews_corrcoef)\n    &gt;&gt;&gt; ctst_auc = CTST(actual, scores, metric=roc_auc_score)\n    &gt;&gt;&gt; print(ctst_mcc.pvalue) # doctest: +SKIP\n    &gt;&gt;&gt; print(ctst_auc.pvalue) # doctest: +SKIP\n    &gt;&gt;&gt; ctst_ = CTST.from_samples(scores, scores, metric=roc_auc_score)\n    &gt;&gt;&gt; isinstance(ctst_, CTST)\n    True\n    \"\"\"\n\n    actual: NDArray = field(repr=False)\n    predicted: NDArray = field(repr=False)\n    metric: Callable\n    n_resamples: int = 9999\n    rng: np.random.Generator = np.random.default_rng()\n    n_jobs: int = 1\n    batch: int | None = None\n    alternative: Literal[\"less\", \"greater\", \"two-sided\"] = \"two-sided\"\n\n    def __post_init__(self):\n        \"\"\"Validate inputs.\"\"\"\n        self.actual = column_or_1d(self.actual)\n        self.predicted = column_or_1d(self.predicted)\n        check_consistent_length(self.actual, self.predicted)\n        assert type_of_target(self.actual, \"actual\") == \"binary\"\n        type_predicted = type_of_target(self.predicted, \"predicted\")\n        assert type_predicted in (\n            \"binary\",\n            \"continuous\",\n            \"multiclass\",\n        ), f\"Expected 'predicted' to be binary or continuous, got {type_predicted}.\"\n        assert check_metric_function(self.metric), (\n            f\"'metric' expects a callable that conforms to scikit-learn metric. \"\n            f\"{signature(self.metric)=} does not.\"\n        )\n\n    @cached_property\n    def _result(self):\n        def statistic(*args):\n            return self.metric(args[0], args[1])\n\n        return permutation_test(\n            data=(self.actual, self.predicted),\n            statistic=statistic,\n            permutation_type=\"pairings\",\n            n_resamples=self.n_resamples,\n            alternative=self.alternative,\n            random_state=self.rng,\n        )\n\n    @cached_property\n    def statistic(self) -&gt; float:\n        \"\"\"\n        Compute the observed test statistic.\n\n        Returns\n        -------\n        float\n            The test statistic.\n\n        Notes\n        -----\n        The result is cached to avoid (expensive) recomputation.\n        \"\"\"\n        return self._result.statistic\n\n    @cached_property\n    def null(self) -&gt; NDArray:\n        \"\"\"\n        Compute the null distribution of the test statistic.\n\n        Notes\n        -----\n        The result is cached to avoid (expensive) recomputation since the\n        null distribution requires permutations.\n        \"\"\"\n        return self._result.null_distribution\n\n    @cached_property\n    def pvalue(self):\n        \"\"\"\n        Compute the p-value using permutations.\n\n        Notes\n        -----\n        The result is cached to avoid (expensive) recomputation.\n        \"\"\"\n        return self._result.pvalue\n\n    @classmethod\n    def from_samples(\n        cls,\n        first_sample: NDArray,\n        second_sample: NDArray,\n        metric: Callable,\n        n_resamples: int = 9999,\n        rng: np.random.Generator = np.random.default_rng(),\n        n_jobs: int = 1,\n        batch: int | None = None,\n        alternative: Literal[\"less\", \"greater\", \"two-sided\"] = \"two-sided\",\n    ):\n        \"\"\"\n        Create a CTST instance from two samples.\n\n        Parameters\n        ----------\n        first_sample : NDArray\n            First sample of scores. These can be binary or continuous.\n        second_sample : NDArray\n            Second sample of scores. These can be binary or continuous.\n\n        Returns\n        -------\n        CTST\n            An instance of the CTST class.\n        \"\"\"\n        assert type_of_target(first_sample) == type_of_target(second_sample)\n        samples = (first_sample, second_sample)\n        actual = assign_labels(samples)\n        predicted = concat_samples(samples)\n        return cls(\n            actual,\n            predicted,\n            metric,\n            n_resamples,\n            rng,\n            n_jobs,\n            batch,\n            alternative,\n        )\n</code></pre>"},{"location":"api/ctst/#samesame.ctst.CTST.null","title":"<code>null</code>  <code>cached</code> <code>property</code>","text":"<p>Compute the null distribution of the test statistic.</p> Notes <p>The result is cached to avoid (expensive) recomputation since the null distribution requires permutations.</p>"},{"location":"api/ctst/#samesame.ctst.CTST.pvalue","title":"<code>pvalue</code>  <code>cached</code> <code>property</code>","text":"<p>Compute the p-value using permutations.</p> Notes <p>The result is cached to avoid (expensive) recomputation.</p>"},{"location":"api/ctst/#samesame.ctst.CTST.statistic","title":"<code>statistic</code>  <code>cached</code> <code>property</code>","text":"<p>Compute the observed test statistic.</p> <p>Returns:</p> Type Description <code>float</code> <p>The test statistic.</p> Notes <p>The result is cached to avoid (expensive) recomputation.</p>"},{"location":"api/ctst/#samesame.ctst.CTST.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate inputs.</p> Source code in <code>src/samesame/ctst.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Validate inputs.\"\"\"\n    self.actual = column_or_1d(self.actual)\n    self.predicted = column_or_1d(self.predicted)\n    check_consistent_length(self.actual, self.predicted)\n    assert type_of_target(self.actual, \"actual\") == \"binary\"\n    type_predicted = type_of_target(self.predicted, \"predicted\")\n    assert type_predicted in (\n        \"binary\",\n        \"continuous\",\n        \"multiclass\",\n    ), f\"Expected 'predicted' to be binary or continuous, got {type_predicted}.\"\n    assert check_metric_function(self.metric), (\n        f\"'metric' expects a callable that conforms to scikit-learn metric. \"\n        f\"{signature(self.metric)=} does not.\"\n    )\n</code></pre>"},{"location":"api/ctst/#samesame.ctst.CTST.from_samples","title":"<code>from_samples(first_sample, second_sample, metric, n_resamples=9999, rng=np.random.default_rng(), n_jobs=1, batch=None, alternative='two-sided')</code>  <code>classmethod</code>","text":"<p>Create a CTST instance from two samples.</p> <p>Parameters:</p> Name Type Description Default <code>first_sample</code> <code>NDArray</code> <p>First sample of scores. These can be binary or continuous.</p> required <code>second_sample</code> <code>NDArray</code> <p>Second sample of scores. These can be binary or continuous.</p> required <p>Returns:</p> Type Description <code>CTST</code> <p>An instance of the CTST class.</p> Source code in <code>src/samesame/ctst.py</code> <pre><code>@classmethod\ndef from_samples(\n    cls,\n    first_sample: NDArray,\n    second_sample: NDArray,\n    metric: Callable,\n    n_resamples: int = 9999,\n    rng: np.random.Generator = np.random.default_rng(),\n    n_jobs: int = 1,\n    batch: int | None = None,\n    alternative: Literal[\"less\", \"greater\", \"two-sided\"] = \"two-sided\",\n):\n    \"\"\"\n    Create a CTST instance from two samples.\n\n    Parameters\n    ----------\n    first_sample : NDArray\n        First sample of scores. These can be binary or continuous.\n    second_sample : NDArray\n        Second sample of scores. These can be binary or continuous.\n\n    Returns\n    -------\n    CTST\n        An instance of the CTST class.\n    \"\"\"\n    assert type_of_target(first_sample) == type_of_target(second_sample)\n    samples = (first_sample, second_sample)\n    actual = assign_labels(samples)\n    predicted = concat_samples(samples)\n    return cls(\n        actual,\n        predicted,\n        metric,\n        n_resamples,\n        rng,\n        n_jobs,\n        batch,\n        alternative,\n    )\n</code></pre>"},{"location":"api/nit/","title":"nit","text":""},{"location":"api/nit/#samesame.nit.WeightedAUC","title":"<code>WeightedAUC</code>  <code>dataclass</code>","text":"<p>               Bases: <code>CTST</code></p> <p>Two-sample test for no adverse shift using the weighted AUC (WAUC).</p> <p>This test compares scores from two independent samples. We reject the null hypothesis of no adverse shift for unusually high values of the WAUC i.e. when the second sample is relatively worse than the first one. This is a robust nonparametric noninferiority test (NIT) with no pre-specified margin. It can be used, amongst other things, to detect dataset shift with outlier scores, hence the DSOS acronym.</p> <p>Attributes:</p> Name Type Description <code>actual</code> <code>NDArray</code> <p>Binary indicator for sample membership.</p> <code>predicted</code> <code>NDArray</code> <p>Estimated (predicted) scores for corresponding samples in <code>actual</code>.</p> <code>n_resamples</code> <code>(int, optional)</code> <p>Number of resampling iterations, by default 9999.</p> <code>rng</code> <code>(Generator, optional)</code> <p>Random number generator, by default np.random.default_rng().</p> <code>n_jobs</code> <code>(int, optional)</code> <p>Number of parallel jobs, by default 1.</p> <code>batch</code> <code>(int or None, optional)</code> <p>Batch size for parallel processing, by default None.</p> See Also <p>bayes.as_bf : Convert a one-sided p-value to a Bayes factor.</p> <p>bayes.as_pvalue : Convert a Bayes factor to a one-sided p-value.</p> Notes <p>The frequentist null distribution of the WAUC is based on permutations [1]. The Bayesian posterior distribution of the WAUC is based on the Bayesian bootstrap [2]. Because this is a one-tailed test of direction (it asks the question, 'are we worse off?'), we can convert a one-sided p-value into a Bayes factor and vice versa. We can also use these p-values for sequential testing [3].</p> <p>The test assumes that <code>predicted</code> are outlier scores and/or encode some notions of outlyingness; higher value of <code>predicted</code> indicates worse outcomes.</p> References <p>.. [1] Kamulete, Vathy M. \"Test for non-negligible adverse shifts.\"    Uncertainty in Artificial Intelligence. PMLR, 2022.</p> <p>.. [2] Gu, Jiezhun, Subhashis Ghosal, and Anindya Roy. \"Bayesian bootstrap    estimation of ROC curve.\" Statistics in medicine 27.26 (2008): 5407-5420.</p> <p>.. [3] Kamulete, Vathy M. \"Are you OK? A Bayesian Sequential Test for    Adverse Shift.\" 2025.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from samesame.nit import WeightedAUC\n&gt;&gt;&gt; # alternatively: from samesame.nit import DSOS\n&gt;&gt;&gt; actual = np.array([0, 1, 1, 0])\n&gt;&gt;&gt; scores = np.array([0.2, 0.8, 0.6, 0.4])\n&gt;&gt;&gt; wauc = WeightedAUC(actual, scores)\n&gt;&gt;&gt; print(wauc.pvalue)\n&gt;&gt;&gt; print(wauc.bayes_factor)\n&gt;&gt;&gt; wauc_ = WeightedAUC.from_samples(scores, scores)\n&gt;&gt;&gt; isinstance(wauc_, WeightedAUC)\nTrue\n</code></pre> Source code in <code>src/samesame/nit.py</code> <pre><code>@dataclass\nclass WeightedAUC(CTST):\n    \"\"\"\n    Two-sample test for no adverse shift using the weighted AUC (WAUC).\n\n    This test compares scores from two independent samples. We reject the\n    null hypothesis of no adverse shift for unusually high values of the WAUC\n    i.e. when the second sample is relatively worse than the first one. This\n    is a robust nonparametric noninferiority test (NIT) with no pre-specified\n    margin. It can be used, amongst other things, to detect dataset shift with\n    outlier scores, hence the DSOS acronym.\n\n    Attributes\n    ----------\n    actual : NDArray\n        Binary indicator for sample membership.\n    predicted : NDArray\n        Estimated (predicted) scores for corresponding samples in `actual`.\n    n_resamples : int, optional\n        Number of resampling iterations, by default 9999.\n    rng : np.random.Generator, optional\n        Random number generator, by default np.random.default_rng().\n    n_jobs : int, optional\n        Number of parallel jobs, by default 1.\n    batch : int or None, optional\n        Batch size for parallel processing, by default None.\n\n    See Also\n    --------\n    bayes.as_bf : Convert a one-sided p-value to a Bayes factor.\n\n    bayes.as_pvalue : Convert a Bayes factor to a one-sided p-value.\n\n    Notes\n    -----\n    The frequentist null distribution of the WAUC is based on permutations\n    [1]. The Bayesian posterior distribution of the WAUC is based on the\n    Bayesian bootstrap [2]. Because this is a one-tailed test of direction\n    (it asks the question, 'are we worse off?'), we can convert a one-sided\n    p-value into a Bayes factor and vice versa. We can also use these p-values\n    for sequential testing [3].\n\n    The test assumes that `predicted` are outlier scores and/or encode some\n    notions of outlyingness; higher value of `predicted` indicates worse\n    outcomes.\n\n    References\n    ----------\n    .. [1] Kamulete, Vathy M. \"Test for non-negligible adverse shifts.\"\n       Uncertainty in Artificial Intelligence. PMLR, 2022.\n\n    .. [2] Gu, Jiezhun, Subhashis Ghosal, and Anindya Roy. \"Bayesian bootstrap\n       estimation of ROC curve.\" Statistics in medicine 27.26 (2008): 5407-5420.\n\n    .. [3] Kamulete, Vathy M. \"Are you OK? A Bayesian Sequential Test for\n       Adverse Shift.\" 2025.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from samesame.nit import WeightedAUC\n    &gt;&gt;&gt; # alternatively: from samesame.nit import DSOS\n    &gt;&gt;&gt; actual = np.array([0, 1, 1, 0])\n    &gt;&gt;&gt; scores = np.array([0.2, 0.8, 0.6, 0.4])\n    &gt;&gt;&gt; wauc = WeightedAUC(actual, scores)\n    &gt;&gt;&gt; print(wauc.pvalue) # doctest: +SKIP\n    &gt;&gt;&gt; print(wauc.bayes_factor) # doctest: +SKIP\n    &gt;&gt;&gt; wauc_ = WeightedAUC.from_samples(scores, scores)\n    &gt;&gt;&gt; isinstance(wauc_, WeightedAUC)\n    True\n    \"\"\"\n\n    def __init__(\n        self,\n        actual: NDArray,\n        predicted: NDArray,\n        n_resamples: int = 9999,\n        rng: np.random.Generator = np.random.default_rng(),\n        n_jobs: int = 1,\n        batch: int | None = None,\n    ):\n        \"\"\"Initialize WeightedAUC.\"\"\"\n        super().__init__(\n            actual=actual,\n            predicted=predicted,\n            metric=wauc,\n            n_resamples=n_resamples,\n            rng=rng,\n            n_jobs=n_jobs,\n            batch=batch,\n            alternative=\"greater\",\n        )\n\n    @cached_property\n    def posterior(self) -&gt; NDArray:\n        \"\"\"\n        Compute the posterior distribution of the WAUC.\n\n        Returns\n        -------\n        NDArray\n            The posterior distribution of the WAUC.\n\n        Notes\n        -----\n        The result is cached to avoid (expensive) recomputation since the\n        posterior distribution uses the Bayesian bootstrap.\n        \"\"\"\n        return bayesian_posterior(\n            self.actual,\n            self.predicted,\n            self.metric,\n            self.n_resamples,\n            self.rng,\n        )\n\n    @cached_property\n    def bayes_factor(self):\n        \"\"\"\n        Compute the Bayes factor using the Bayesian bootstrap.\n\n        Notes\n        -----\n        The result is cached to avoid (expensive) recomputation.\n        \"\"\"\n        bayes_threshold = float(np.mean(self.null))\n        bf_ = _bayes_factor(self.posterior, bayes_threshold)\n        return bf_\n\n    @classmethod\n    def from_samples(\n        cls,\n        first_sample: NDArray,\n        second_sample: NDArray,\n        n_resamples: int = 9999,\n        rng: np.random.Generator = np.random.default_rng(),\n        n_jobs: int = 1,\n        batch: int | None = None,\n    ):\n        \"\"\"\n        Create a WeightedAUC instance from two samples.\n\n        Parameters\n        ----------\n        first_sample : NDArray\n            First sample of scores. These can be binary or continuous.\n        second_sample : NDArray\n            Second sample of scores. These can be binary or continuous.\n\n        Returns\n        -------\n        WeightedAUC\n            An instance of the WeightedAUC class.\n        \"\"\"\n        assert type_of_target(first_sample) == type_of_target(second_sample)\n        samples = (first_sample, second_sample)\n        actual = assign_labels(samples)\n        predicted = concat_samples(samples)\n        return cls(\n            actual,\n            predicted,\n            n_resamples,\n            rng,\n            n_jobs,\n            batch,\n        )\n</code></pre>"},{"location":"api/nit/#samesame.nit.WeightedAUC.bayes_factor","title":"<code>bayes_factor</code>  <code>cached</code> <code>property</code>","text":"<p>Compute the Bayes factor using the Bayesian bootstrap.</p> Notes <p>The result is cached to avoid (expensive) recomputation.</p>"},{"location":"api/nit/#samesame.nit.WeightedAUC.posterior","title":"<code>posterior</code>  <code>cached</code> <code>property</code>","text":"<p>Compute the posterior distribution of the WAUC.</p> <p>Returns:</p> Type Description <code>NDArray</code> <p>The posterior distribution of the WAUC.</p> Notes <p>The result is cached to avoid (expensive) recomputation since the posterior distribution uses the Bayesian bootstrap.</p>"},{"location":"api/nit/#samesame.nit.WeightedAUC.__init__","title":"<code>__init__(actual, predicted, n_resamples=9999, rng=np.random.default_rng(), n_jobs=1, batch=None)</code>","text":"<p>Initialize WeightedAUC.</p> Source code in <code>src/samesame/nit.py</code> <pre><code>def __init__(\n    self,\n    actual: NDArray,\n    predicted: NDArray,\n    n_resamples: int = 9999,\n    rng: np.random.Generator = np.random.default_rng(),\n    n_jobs: int = 1,\n    batch: int | None = None,\n):\n    \"\"\"Initialize WeightedAUC.\"\"\"\n    super().__init__(\n        actual=actual,\n        predicted=predicted,\n        metric=wauc,\n        n_resamples=n_resamples,\n        rng=rng,\n        n_jobs=n_jobs,\n        batch=batch,\n        alternative=\"greater\",\n    )\n</code></pre>"},{"location":"api/nit/#samesame.nit.WeightedAUC.from_samples","title":"<code>from_samples(first_sample, second_sample, n_resamples=9999, rng=np.random.default_rng(), n_jobs=1, batch=None)</code>  <code>classmethod</code>","text":"<p>Create a WeightedAUC instance from two samples.</p> <p>Parameters:</p> Name Type Description Default <code>first_sample</code> <code>NDArray</code> <p>First sample of scores. These can be binary or continuous.</p> required <code>second_sample</code> <code>NDArray</code> <p>Second sample of scores. These can be binary or continuous.</p> required <p>Returns:</p> Type Description <code>WeightedAUC</code> <p>An instance of the WeightedAUC class.</p> Source code in <code>src/samesame/nit.py</code> <pre><code>@classmethod\ndef from_samples(\n    cls,\n    first_sample: NDArray,\n    second_sample: NDArray,\n    n_resamples: int = 9999,\n    rng: np.random.Generator = np.random.default_rng(),\n    n_jobs: int = 1,\n    batch: int | None = None,\n):\n    \"\"\"\n    Create a WeightedAUC instance from two samples.\n\n    Parameters\n    ----------\n    first_sample : NDArray\n        First sample of scores. These can be binary or continuous.\n    second_sample : NDArray\n        Second sample of scores. These can be binary or continuous.\n\n    Returns\n    -------\n    WeightedAUC\n        An instance of the WeightedAUC class.\n    \"\"\"\n    assert type_of_target(first_sample) == type_of_target(second_sample)\n    samples = (first_sample, second_sample)\n    actual = assign_labels(samples)\n    predicted = concat_samples(samples)\n    return cls(\n        actual,\n        predicted,\n        n_resamples,\n        rng,\n        n_jobs,\n        batch,\n    )\n</code></pre>"},{"location":"examples/covariate-balance/","title":"Covariate Balance","text":""},{"location":"examples/covariate-balance/#overview","title":"Overview","text":"<p>In observational studies, treatments are not randomly assigned\u2014making it critical to assess whether treatment groups have similar distributions of baseline (pre-treatment) covariates. Covariate balance measures whether the treatment and control groups look similar on observed characteristics. Poor balance can introduce bias in causal estimates.</p> <p><code>samesame</code> provides classifier two-sample tests (CTSTs) as a flexible tool for assessing balance. The key idea: if a classifier can reliably distinguish treatment from control based on covariates alone, the groups are imbalanced.</p>"},{"location":"examples/covariate-balance/#when-to-use-this","title":"When to use this","text":"<ul> <li>After matching or weighting: Verify that your matching or inverse probability weighting (IPW) strategy actually balanced the groups.</li> <li>Sensitivity analysis: Test balance across many covariate combinations without specifying a single summary metric.</li> <li>Production monitoring: Check that covariate distributions remain stable over time in observational systems.</li> </ul>"},{"location":"examples/covariate-balance/#example-testing-balance-after-matching","title":"Example: Testing balance after matching","text":"<p>Below is a practical example using the <code>lalonde</code> dataset from the <code>MatchIt</code> R package, which compares an experimental job training program (treatment) to a control group on baseline covariates like age, education, race, marital status, and prior earnings.</p>"},{"location":"examples/covariate-balance/#load-data-and-compute-propensity-scores","title":"Load data and compute propensity scores","text":"<pre><code>import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\n\n# Load lalonde dataset from MatchIt R package\nurl = \"https://raw.githubusercontent.com/kosukeimai/MatchIt/master/data/lalonde.tab\"\n\n# Read the first line to extract column headers, then read the rest as data\nwith pd.io.common.urlopen(url) as response:\n    # Read first line as column headers\n    header_line = response.readline().decode('utf-8').strip().split('\\t')\n    # Read remaining lines as data\n    lalonde = pd.read_table(response, sep='\\t', header=None, names=header_line)# Display basic info\nprint(f\"Dataset shape: {lalonde.shape}\")\nprint(lalonde.head())\n\n# Covariates and treatment\ncovariates = ['age', 'educ', 'race', 'married', 'nodegree', 're74', 're75']\nX = lalonde[covariates].copy()\n\n# Convert categorical (race) to dummies\nX = pd.get_dummies(X, columns=['race'], drop_first=False)\nX = X.astype(float)\n\ntreatment = lalonde['treat'].values\n</code></pre>"},{"location":"examples/covariate-balance/#before-matching-testing-imbalance","title":"Before matching: testing imbalance","text":"<pre><code>from samesame.ctst import CTST\nfrom sklearn.metrics import roc_auc_score\n\n# Fit a simple logistic regression classifier on raw (unmatched) data\nclf = LogisticRegression(random_state=42, max_iter=1000)\nclf.fit(X, treatment)\ny_pred_proba = clf.predict_proba(X)[:, 1]\n\n# Test for imbalance using CTST\nctst_before = CTST(actual=treatment, predicted=y_pred_proba, metric=roc_auc_score)\nprint(f\"Before matching:\")\nprint(f\"  AUC: {ctst_before.statistic:.3f}\")\nprint(f\"  p-value: {ctst_before.pvalue:.4f}\")\n</code></pre> <p>Example output:</p> <pre><code>Before matching:\n  AUC: 0.711\n  p-value: 0.0000\n</code></pre> <p>High AUC (&gt; 0.5) and low p-value indicate the groups are imbalanced\u2014the classifier can reliably predict treatment assignment from covariates alone.</p>"},{"location":"examples/covariate-balance/#after-matching-testing-balance","title":"After matching: testing balance","text":"<p>Perform 1:1 nearest-neighbor matching on propensity scores:</p> <pre><code>from sklearn.neighbors import NearestNeighbors\n\n# Compute propensity scores\npropensity_model = LogisticRegression(random_state=42, max_iter=1000)\npropensity_model.fit(X, treatment)\npropensity_scores = propensity_model.predict_proba(X)[:, 1]\n\n# 1:1 nearest-neighbor matching on propensity scores (matching treated to control)\ntreatment_idx = np.where(treatment == 1)[0]\ncontrol_idx = np.where(treatment == 0)[0]\n\n# Find nearest neighbors in control group for each treated unit\nnbrs = NearestNeighbors(n_neighbors=1, algorithm='ball_tree').fit(\n    propensity_scores[control_idx].reshape(-1, 1)\n)\ndistances, indices = nbrs.kneighbors(propensity_scores[treatment_idx].reshape(-1, 1))\n\n# Create matched dataset\nmatched_treatment_idx = treatment_idx\nmatched_control_idx = control_idx[indices.ravel()]\nmatched_idx = np.concatenate([matched_treatment_idx, matched_control_idx])\n\nX_matched = X.iloc[matched_idx].reset_index(drop=True)\ntreatment_matched = treatment[matched_idx]\n\nprint(f\"Matched sample size: {len(matched_idx)} (treatment: {(treatment_matched == 1).sum()}, \"\n      f\"control: {(treatment_matched == 0).sum()})\")\n\n# Test balance in matched data\nclf_matched = LogisticRegression(random_state=42, max_iter=1000)\nclf_matched.fit(X_matched, treatment_matched)\ny_pred_proba_matched = clf_matched.predict_proba(X_matched)[:, 1]\n\nctst_after = CTST(actual=treatment_matched, predicted=y_pred_proba_matched, metric=roc_auc_score)\nprint(f\"After matching:\")\nprint(f\"  AUC: {ctst_after.statistic:.3f}\")\nprint(f\"  p-value: {ctst_after.pvalue:.4f}\")\n</code></pre> <p>Example output:</p> <pre><code>Matched sample size: 370 (treatment: 185, control: 185)\nAfter matching:\n  AUC: 0.503\n  p-value: 0.7854\n</code></pre> <p>After matching, AUC close to 0.5 and high p-value indicate the groups are now balanced\u2014the classifier cannot reliably distinguish treatment from control using covariates alone.</p>"},{"location":"examples/covariate-balance/#using-different-classifiers-and-metrics","title":"Using different classifiers and metrics","text":"<p>Just as in the CTST examples, you can use any classifier and metric:</p> <pre><code>from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import balanced_accuracy_score\n\n# Use a more flexible classifier (random forest)\nrf = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=5)\nrf.fit(X_matched, matched_treatment)\ny_pred_binary = rf.predict(X_matched)\n\n# Use balanced accuracy instead of AUC\nctst_rf = CTST(actual=matched_treatment, predicted=y_pred_binary, metric=balanced_accuracy_score)\nprint(f\"Balanced Accuracy (Random Forest): {ctst_rf.statistic:.3f}, p-value: {ctst_rf.pvalue:.4f}\")\n</code></pre>"},{"location":"examples/covariate-balance/#practical-recommendations","title":"Practical recommendations","text":"<ul> <li>Choose meaningful metrics: Use AUC or balanced accuracy; these are robust to class imbalance and easy to interpret.</li> <li>Test covariate subsets: Apply CTSTs to different covariate subsets to identify which covariates remain imbalanced.</li> <li>Combine with domain knowledge: Statistical balance does not guarantee causal validity; use CTSTs alongside subject-matter expertise.</li> <li>Report before and after: Show balance statistics (e.g., mean differences, AUC) alongside CTST results for transparency.</li> </ul>"},{"location":"examples/covariate-balance/#related-resources","title":"Related resources","text":"<p>For additional reading on balance assessment and related methods:</p> <ul> <li>MatchIt - R package with comprehensive balance diagnostics</li> <li>cobalt - R package for covariate balance assessment</li> <li>pysmatch - Python matching library with balance checks</li> <li>Classification Permutation Test (CPT) - Original paper on classification-based balance testing</li> </ul>"},{"location":"examples/credit-example/","title":"Credit (HELOC) Dataset","text":"<p>This example demonstrates how <code>samesame</code> can be used on a real-world credit dataset to detect both dataset shift (when the data distribution changes between training and deployment) and performance degradation (when models make worse predictions). We use the HELOC (Home Equity Line of Credit) dataset from TableShift, as described in the HELOC dataset documentation.</p>"},{"location":"examples/credit-example/#when-to-use-this","title":"When to use this","text":"<ul> <li>Dataset shift: Your training data comes from one population but your model is deployed on a different population. You need to detect when this happens.</li> <li>Performance monitoring: You need to know if the model predictions (e.g., default probabilities) have shifted in a harmful (adverse) way over time.</li> </ul> <p>This example shows both approaches applied to credit risk scoring.</p>"},{"location":"examples/credit-example/#context","title":"Context","text":"<p>The HELOC dataset comes from the FICO Community Explainable AI Challenge, an open-source dataset containing features derived from anonymized credit bureau data. It includes credit history and risk assessment data. The dataset naturally splits into two populations based on <code>ExternalRiskEstimate</code>:</p> <ol> <li>Low credit-risk segment (<code>External Risk Estimate &gt; 63</code>): Lower risk customers.</li> <li>High credit-risk segment (<code>External Risk Estimate \u2264 63</code>): Higher risk customers.</li> </ol> <p>We'll treat the low-risk segment as training data and the high-risk segment as a held-out deployment domain, simulating a real-world scenario where a model trained on one population is deployed to a different population with different risk profiles.</p>"},{"location":"examples/credit-example/#dataset-preparation","title":"Dataset Preparation","text":"<pre><code>import re\nimport pandas as pd\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import roc_auc_score\n\nfrom samesame.ctst import CTST\nfrom samesame.nit import DSOS\n\n# Fetch the HELOC dataset from OpenML\nfico = fetch_openml(data_id=45554, as_frame=True)\nX, y = fico.data, fico.target\n\n# Identify the External Risk Estimate column (case-insensitive lookup)\n# This column is used to split the data into two distinct populations\nre_obj = re.compile(r\"external.*risk.*estimate\", flags=re.I)\ncol_candidates = (c for c in X.columns if re_obj.search(c))\ncol_split = next(col_candidates, None)\nif col_split is None:\n    raise ValueError(\"External Risk Estimate column not found\")\n\n# Split data by external risk threshold (63 is the standard cutoff)\nthreshold = 63\nmask_high = X[col_split].astype(float) &gt; threshold\n\n# Training domain: low credit-risk customers (high external risk estimates)\nX_train = X[mask_high].reset_index(drop=True)\ny_train = y[mask_high].reset_index(drop=True)\n\n# Deployment domain: high credit-risk customers (low external risk estimates)\n# This represents the domain shift scenario\nX_test = X[~mask_high].reset_index(drop=True)\ny_test = y[~mask_high].reset_index(drop=True)\n\nprint(f\"Training domain: {len(X_train)} samples (low external risk)\")\nprint(f\"Held-out domain: {len(X_test)} samples (high external risk)\")\n</code></pre> <p>Output:</p> <pre><code># Training domain: 7683 samples (low external risk)\n# Held-out domain: 2188 samples (high external risk)\n</code></pre>"},{"location":"examples/credit-example/#dataset-shift","title":"Dataset Shift","text":"<p>Question: Are the two domains statistically different?</p> <p>Method: We train a classifier to distinguish between the two domains. A high-performing classifier indicates meaningful differences. We use out-of-bag (OOB) predictions to avoid data leakage and obtain valid p-values in keeping with the Classifier Two-Sample Test (CTST) framework.</p> <pre><code># Create binary domain labels: 0 = training, 1 = held-out (deployment)\nmembership = pd.Series([0] * len(X_train) + [1] * len(X_test))\nX_concat = pd.concat([X_train, X_test], ignore_index=True)\n\n# Train a RandomForest classifier to distinguish between domains\n# Key settings:\n#   - oob_score=True: Enables OOB predictions for unbiased model evaluation\n#   - n_estimators=500: Number of trees (higher = more robust estimates)\n#   - min_samples_leaf=10: Minimum samples per leaf (prevents overfitting)\nrf_domain = RandomForestClassifier(\n    n_estimators=500,\n    oob_score=True,\n    random_state=12345,\n    min_samples_leaf=10,\n)\nrf_domain.fit(X_concat, membership)\n\n# Extract OOB predicted probabilities (probability of being in held-out domain)\noob_scores = rf_domain.oob_decision_function_[:, 1]\n\n# Run Classifier Two-Sample Test (CTST) using AUC as the test metric\n# CTST tests whether the classifier's discrimination ability is statistically significant\nctst = CTST(actual=membership.values, predicted=oob_scores, metric=roc_auc_score)\nprint(f\"Dataset Shift Detection (CTST)\")\nprint(f\"  Statistic: {ctst.statistic:.4f}\")\nprint(f\"  p-value: {ctst.pvalue:.4f}\")\n</code></pre> <p>Output:</p> <pre><code># Dataset Shift Detection (CTST)\n#   Statistic: 1.0000\n#   p-value: 0.0002\n</code></pre> <p>Interpretation: The p-value = 0.0002 is highly significant (p &lt; 0.05), indicating the training and held-out domains are statistically different. This is expected since we deliberately split the data by risk profile. A classifier that can reliably distinguish between these domains reveals that their underlying feature distributions differ substantially.</p>"},{"location":"examples/credit-example/#feature-importance","title":"Feature Importance","text":"<p>To understand which features drive the domain shift, we can examine feature importance:</p> <pre><code>importances = rf_domain.feature_importances_\nfeature_names = X_concat.columns\nfeat_imp = pd.Series(importances, index=feature_names).sort_values(ascending=False)\nprint(\"\\nTop 5 features distinguishing the domains:\")\nprint(feat_imp.head(5))\n</code></pre> <p>Output:</p> <pre><code>Top 5 features distinguishing the domains:\nExternalRiskEstimate          0.642400\nMSinceMostRecentDelq          0.069394\nMaxDelq2PublicRecLast12M      0.064526\nNetFractionRevolvingBurden    0.050656\nPercentTradesNeverDelq        0.042478\ndtype: float64\n</code></pre> <p>These features reveal which characteristics differ most between the two populations. <code>ExternalRiskEstimate</code> dominates because it was the splitting criterion. However, other features like <code>MSinceMostRecentDelq</code> and <code>MaxDelq2PublicRecLast12M</code> also show notable importance, indicating the populations differ beyond just the splitting variable. This suggests covariate shift: the feature distributions differ, but the label distribution may respond differently.</p>"},{"location":"examples/credit-example/#performance-degradation","title":"Performance Degradation","text":"<p>Question: Has the model's prediction behavior changed in a way that suggests degradation?</p> <p>Method: We train a credit risk model on the training domain to predict loan default (<code>y</code>). Then we compare the model's predicted default probabilities between the training and deployment domains. The Dataset-Shift-with-Outlier-Scores (DSOS) test detects whether the tail of the distribution has shifted, indicating more worse outcomes.</p> <pre><code># Train a credit risk model on the training domain to predict loan default\n# Convert labels from categorical (Good/Bad) to numeric (0/1)\nloan_status = y_train.map({'Good': 0, 'Bad': 1}).values\n\n# Train RandomForest to predict probability of default\nrf_bad = RandomForestClassifier(\n    n_estimators=500,\n    oob_score=True,\n    random_state=12345,\n    min_samples_leaf=10,\n)\nrf_bad.fit(X_train, loan_status)\n\n# Extract predicted default probabilities\n# - Training domain: Use OOB predictions (unbiased, non-resubstitution)\n# - Deployment domain: Use standard predictions on held-out data\nbad_train = rf_bad.oob_decision_function_[:, 1].ravel()\nbad_test = rf_bad.predict_proba(X_test)[:, 1].ravel()\n\n# Apply Degree-of-Shift Outcome-Shift (DSOS) test\n# Tests whether the distribution of predictions has shifted toward worse outcomes\ndsos = DSOS.from_samples(bad_train, bad_test)\nprint(f\"\\nPerformance Degradation (DSOS)\")\nprint(f\"  Statistic: {dsos.statistic:.4f}\")\nprint(f\"  p-value: {dsos.pvalue:.4f}\")\n</code></pre> <p>Output:</p> <pre><code># Performance Degradation (DSOS)\n#   Statistic: 0.2483\n#   p-value: 0.0001\n</code></pre> <p>Interpretation: The p-value = 0.0001 is highly significant, indicating the model's predicted default probabilities have shifted substantially upward in the deployment domain. This reveals performance degradation: the model now predicts higher risk for the new population. Unlike a t-test (which measures mean shift), DSOS focuses on whether the tail of the distribution has shifted, capturing whether more extreme high-risk predictions are now common\u2014the key indicator of adverse performance.</p>"},{"location":"examples/credit-example/#interpreting-the-results","title":"Interpreting the Results","text":"<p>In this example, we observe:</p> <ul> <li>CTST (p = 0.0002, rejected): The held-out domain is significantly different from training. The feature distributions have shifted due to our intentional split by risk profile.</li> <li>DSOS (p = 0.0001, rejected): The model's predicted default probabilities have shifted upward. This indicates performance degradation\u2014the model predicts substantially higher risk for the new population, suggesting real concerns about model reliability in production.</li> <li>Feature importance: <code>ExternalRiskEstimate</code> dominates, but other features also differ between populations, indicating both direct and indirect covariate shifts.</li> </ul>"},{"location":"examples/credit-example/#key-insights","title":"Key Insights","text":"<p>Dataset Shift vs. Performance Degradation. The CTST and DSOS tests answer complementary questions. CTST tests whether the feature distributions differ between domains\u2014a significant result indicates covariate shift has occurred. DSOS tests whether the model's predictions have shifted toward worse outcomes\u2014a significant result means the model's behavior has degraded. Together, they paint a complete picture: CTST detects whether a problem exists, while DSOS confirms whether it affects your deployed model.</p> <p>Four Possible Scenarios. In production, four combinations are possible and each suggests a different action:</p> <ol> <li>CTST non-sig, DSOS non-sig: Domains are similar and the model performs consistently. \u2713 Safe to operate.</li> <li>CTST sig, DSOS non-sig: Domains differ but the model generalizes well across both. \u2713 Monitor closely but no immediate action needed.</li> <li>CTST sig, DSOS sig: Domain shift has degraded model performance. \u2717 Retrain or recalibrate.</li> <li>CTST non-sig, DSOS sig: A subtle shift is affecting predictions despite similar feature distributions. \u2717 Investigate underlying causes.</li> </ol> <p>Feature-Level Understanding. Feature importance identifies which specific features drive the shift, enabling targeted interventions. Rather than retraining the entire model, you can focus on recalibration, collecting more data for important features, or implementing domain adaptation techniques on key predictors.</p>"},{"location":"examples/credit-example/#practical-recommendations","title":"Practical Recommendations","text":"<p>1. Implement Regular Monitoring. Compute CTST and DSOS on production data batches periodically\u2014monthly or quarterly depending on your deployment frequency and business risk tolerance. Automated pipelines work best: compute statistics continuously and log results for easy review. This early detection enables proactive intervention before performance degrades significantly.</p> <p>2. Set Clear Action Thresholds. Define decision rules based on p-values and automate responses. For example: if p &lt; 0.01, trigger alerts and schedule urgent review; if 0.01 \u2264 p &lt; 0.05, log the flag and prepare retraining pipelines for review; if p &gt; 0.10, continue normal operation. Document these thresholds and the rationale behind them so all stakeholders understand when and why action is taken.</p> <p>3. Investigate Root Causes When Tests Are Significant. Don't just react to test results\u2014understand what's driving them. Compute feature importance from your domain-distinguishing model to identify which features shifted most. Then analyze the actual data: examine distributions, percentiles, and summary statistics for each important feature. Determine whether the shift represents natural business drift (e.g., seasonal patterns, changing customer base) or a data quality issue that needs correction.</p> <p>4. Choose Metrics That Don't Require Labels. A major advantage of CTST and DSOS is that they work without true labels in production. Good proxies for monitoring include predicted probabilities (as in this example), model confidence scores, feature distributions, and derived metrics like average predicted default rate. This makes continuous monitoring feasible even when ground truth takes weeks or months to collect.</p> <p>5. Plan Remediation Based on Severity and Impact. Once you've identified a significant shift, your response depends on business context. For natural drift over time, simply monitor closely without intervention. For meaningful shifts, consider recalibrating the model to adjust for distribution changes. For substantial shifts affecting business outcomes, retrain on more recent data. In complex scenarios, explore domain adaptation techniques that learn to bridge the gap between training and deployment distributions.</p>"},{"location":"examples/distribution-shifts/","title":"Test for Distribution Shifts","text":""},{"location":"examples/distribution-shifts/#overview","title":"Overview","text":"<p>Given two datasets <code>sample_P</code> and <code>sample_Q</code> from distributions \\(P\\) and \\(Q\\), the goal is to estimate a \\(p\\)-value for the null hypothesis of equal distribution, \\(P=Q\\). <code>samesame</code> implements classifier two-sample tests (CTSTs) for this use case.</p> <p>Why CTSTs? They are powerful, modular (flexible), and easy to explain\u2014the elusive trifecta. For a deeper dive, see this discussion.</p>"},{"location":"examples/distribution-shifts/#how-ctsts-work","title":"How CTSTs Work","text":"<p>CTSTs leverage machine learning classifiers to test for distribution differences as follows:</p> <ol> <li>Label samples - Assign class labels to indicate sample membership (<code>sample_P</code> = positive class, <code>sample_Q</code> = negative class)</li> <li>Train classifier - Fit a classifier to predict sample labels based on features</li> <li>Evaluate performance - If the classifier achieves high accuracy, the samples are likely different; if accuracy is near random chance, they are likely similar</li> </ol> <p>This approach is versatile: you can use any classifier and any binary classification metric (AUC, balanced accuracy, etc.) without making strong distributional assumptions.</p>"},{"location":"examples/distribution-shifts/#data","title":"Data","text":"<p>To demonstrate CTSTs in action, let's create a synthetic dataset with two distinguishable distributions:</p> <pre><code>from sklearn.datasets import make_classification\n\n# Create a small dataset with clear class separation\nX, y = make_classification(\n    n_samples=100,\n    n_features=4,\n    n_classes=2,\n    random_state=123_456,\n)\n</code></pre> <p>The two samples are concatenated into a single feature matrix <code>X</code> with shape <code>(100, 4)</code>. The binary labels <code>y</code> indicate sample membership\u2014which observations belong to <code>sample_P</code> (positive class, <code>y=1</code>) and which belong to <code>sample_Q</code> (negative class, <code>y=0</code>).</p>"},{"location":"examples/distribution-shifts/#cross-fitted-predictions","title":"Cross-fitted Predictions","text":"<p>Cross-fitting estimates classifier performance on unseen data using out-of-sample predictions. This approach is more efficient than sample splitting, which typically uses 50% of data for training and 50% for testing\u2014resulting in a loss of statistical power. Cross-fitting and out-of-bag methods use more data for inference while preserving statistical validity.</p> <p>For more alternatives to sample splitting, see this paper.</p> <pre><code>from sklearn.ensemble import HistGradientBoostingClassifier\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.preprocessing import binarize\n\n# Get cross-fitted probability predictions (out-of-sample)\ny_gb = cross_val_predict(\n    estimator=HistGradientBoostingClassifier(random_state=123_456),\n    X=X,\n    y=y,\n    cv=10,\n    method='predict_proba',\n)[:, 1]  # Extract probabilities for the positive class\n\n# Binarize predictions for metrics that require binary labels\ny_gb_binary = binarize(y_gb.reshape(-1, 1), threshold=0.5).ravel()\n</code></pre>"},{"location":"examples/distribution-shifts/#classifier-two-sample-tests-ctsts","title":"Classifier Two-Sample Tests (CTSTs)","text":"<p>Now we run CTSTs using three different classification metrics.</p> <pre><code>from samesame.ctst import CTST\nfrom sklearn.metrics import balanced_accuracy_score, matthews_corrcoef, roc_auc_score\n\n# Define metrics to evaluate\nmetrics = [balanced_accuracy_score, matthews_corrcoef, roc_auc_score]\n\n# Run CTST for each metric\nfor metric in metrics:\n    # Use binary predictions for metrics that require them; probabilities for AUC\n    predicted = y_gb_binary if metric != roc_auc_score else y_gb\n    ctst = CTST(actual=y, predicted=predicted, metric=metric)\n    print(f\"{metric.__name__}\")\n    print(f\"\\t statistic: {ctst.statistic:.2f}\")\n    print(f\"\\t p-value: {ctst.pvalue:.2f}\")\n</code></pre> <p>Output:</p> <pre><code>balanced_accuracy_score\n     statistic: 0.86\n     p-value: 0.00\nmatthews_corrcoef\n     statistic: 0.72\n     p-value: 0.00\nroc_auc_score\n     statistic: 0.93\n     p-value: 0.00\n</code></pre> <p>In all three cases, we reject the null hypothesis of equal distribution (\\(P=Q\\)) at conventional significance levels. The <code>HistGradientBoostingClassifier</code> performed well across all metrics, providing strong evidence that <code>sample_P</code> and <code>sample_Q</code> come from different distributions.</p>"},{"location":"examples/distribution-shifts/#out-of-bag-predictions","title":"Out-of-Bag Predictions","text":"<p>An alternative to cross-fitting is using out-of-bag (OOB) predictions from ensemble methods like <code>RandomForestClassifier</code>. Both cross-fitted and OOB predictions mitigate the downsides of sample splitting.</p> <pre><code>from sklearn.ensemble import RandomForestClassifier\n\n# Train random forest with out-of-bag score enabled\nrf = RandomForestClassifier(\n    n_estimators=500,\n    random_state=123_456,\n    oob_score=True,\n    min_samples_leaf=10,\n)\n\n# Get out-of-bag decision function scores (probabilities)\ny_rf = rf.fit(X, y).oob_decision_function_[:, 1]\n\n# Binarize for metrics requiring binary predictions\ny_rf_binary = binarize(y_rf.reshape(-1, 1), threshold=0.5).ravel()\n\n# Run CTST for each metric\nfor metric in metrics:\n    predicted = y_rf_binary if metric != roc_auc_score else y_rf\n    ctst = CTST(actual=y, predicted=predicted, metric=metric)\n    print(f\"{metric.__name__}\")\n    print(f\"\\t statistic: {ctst.statistic:.2f}\")\n    print(f\"\\t p-value: {ctst.pvalue:.2f}\")\n</code></pre> <p>Output:</p> <pre><code>balanced_accuracy_score\n     statistic: 0.88\n     p-value: 0.00\nmatthews_corrcoef\n     statistic: 0.76\n     p-value: 0.00\nroc_auc_score\n     statistic: 0.94\n     p-value: 0.00\n</code></pre> <p>Again, we reject the null hypothesis of equal distribution. The <code>RandomForestClassifier</code> performed on par with the gradient boosting model, providing evidence of distribution shift. Both classifiers agree that the samples are distinguishable.</p>"},{"location":"examples/distribution-shifts/#interpreting-results","title":"Interpreting Results","text":"<p>Important distinction: A significant CTST indicates the distributions are different, but not necessarily that the difference is problematic. In production settings (e.g., model monitoring, data validation), you may want to use a noninferiority test instead to determine if the shift is substantive enough to warrant action.</p> <p>This example demonstrates the modularity of CTSTs:</p> <ul> <li>Classifiers: Any classifier works (here we used gradient boosting and random forests)</li> <li>Metrics: Any classification metric can be used (balanced accuracy, AUC, Matthews correlation coefficient, etc.)</li> <li>Inference methods: Both cross-fitting and out-of-bag predictions avoid the sample-splitting penalty</li> </ul> <p>This flexibility makes CTSTs useful for diverse applications without compromising statistical power for valid inference.</p>"},{"location":"examples/distribution-shifts/#explanations","title":"Explanations","text":"<p>To understand why the two samples are different, you can apply explainable methods to the fitted classifiers. For example, feature importance from <code>RandomForestClassifier</code> or model-agnostic techniques like SHAP can reveal which features contribute most to the distribution shift.</p> <pre><code># Example: Feature importance from random forest\nimport pandas as pd\n\nimportances = rf.feature_importances_\nfeature_names = [f\"Feature {i}\" for i in range(X.shape[1])]\nimportance_df = pd.DataFrame(\n    {'Feature': feature_names, 'Importance': importances}\n).sort_values('Importance', ascending=False)\nprint(importance_df)\n</code></pre> <p>This helps answer, which features are driving the distribution shift? This information is valuable for understanding root causes and deciding on remedial actions.</p>"},{"location":"examples/noninferiority/","title":"Noninferiority Test","text":"<p>Noninferiority tests (NITs) ask whether a new sample (or treatment) is not meaningfully worse than a reference. <code>samesame</code> implements D-SOS (Dataset Shift with Outlier Scores), a robust, nonparametric noninferiority test that operates on outlier scores and does not require a pre-specified margin.</p>"},{"location":"examples/noninferiority/#when-to-use-this","title":"When to use this","text":"<ul> <li>Use CTSTs when the question is, \"are the  two distributions different?\" \u2014 i.e., you want to detect any distributional  difference.</li> <li>Use NITs when the question is, \"is the new  sample not substantially worse than the reference?\" \u2014 i.e., you care about  adverse shifts rather than any difference.</li> </ul> <p>The two approaches address different scientific questions; they are complementary rather than interchangeable.</p>"},{"location":"examples/noninferiority/#d-sos-at-a-glance","title":"D-SOS at a glance","text":"<p>D-SOS transforms the problem of noninferiority testing (NITs) into a CTST by treating outlier scores as the classifier's predicted values, using a weighted AUC metric, and testing a one-sided alternative. The key advantages are:</p> <ul> <li>Nonparametric: no normality assumption required.</li> <li>No pre-specified margin needed: the method is robust to how \"meaningful\"  differences are defined in practice.</li> <li>Works with any sensible outlier scoring method (isolation forest, deep  models, domain-specific scores, etc.).</li> </ul> <p>The test focuses on whether the test sample contains disproportionately more high outlier scores than the reference, which aligns with the goal of detecting adverse shifts.</p>"},{"location":"examples/noninferiority/#prologue","title":"Prologue","text":"<p>The following clinical-trial style example illustrates the difference between classic noninferiority approaches and D-SOS. The motivating study (from SAS's case study) compares a new, cheaper drug \"Bowl\" to the standard \"Armanaleg.\"</p> <p>The original study uses parametric assumptions and a pre-specified margin. For D-SOS we only need outlier/discomfort scores that reflect worse outcomes as higher values.</p>"},{"location":"examples/noninferiority/#data","title":"Data","text":"<p>We convert reported relief scores so that higher numbers indicate worse outcomes, producing outlier/discomfort scores suitable for D-SOS.</p> <pre><code>import numpy as np\n\ndatalines = (\n  \"9 14 13 8 10 5 11 9 12 10 9 11 8 11 \"\n  \"4 8 11 16 12 10 9 10 13 12 11 13 9 4 \"\n  \"7 14 8 4 10 11 7 7 13 8 8 13 10 9 \"\n  \"12 9 11 10 12 7 8 5 10 7 13 12 13 11 \"\n  \"7 12 10 11 10 8 6 9 11 8 5 11 10 8\"\n).split()\nrelief = [float(s) for s in datalines]\ndiscomfort = [max(relief) - s for s in relief]\narmanaleg = np.array(discomfort[:28])\nbowl = np.array(discomfort[28:])\n</code></pre>"},{"location":"examples/noninferiority/#analysis","title":"Analysis","text":"<p>Below we run D-SOS using <code>DSOS.from_samples</code>, treating <code>armanaleg</code> as the reference (control) and <code>bowl</code> as the new treatment. We report the frequentist p-value and an optional Bayesian conversion of the Bayes factor.</p> <pre><code>from samesame.bayes import as_pvalue\nfrom samesame.nit import DSOS\n\ndsos = DSOS.from_samples(armanaleg, bowl)\nfrequentist = dsos.pvalue\nbayesian = as_pvalue(dsos.bayes_factor)\nprint(f\"Frequentist p-value: {frequentist:.4f}\")\nprint(f\"Bayesian p-value: {bayesian:.4f}\")\n</code></pre> <p>Typical output (reproduced from the example):</p> <pre><code>Frequentist p-value: 0.1215\nBayesian p-value: 0.1159\n</code></pre> <p>We fail to reject the null of no adverse shift \u2014 the new treatment (Bowl) is not shown to be meaningfully worse than the reference under the D-SOS criterion.</p> <p>This is consistent with the original analysis from classical two-sided or one-sided parametric noninferiority tests, which concludes:</p> <p>This suggests, as you\u2019d hoped, that the efficacy of Bowl is not appreciably worse than that of Armanaleg</p>"},{"location":"examples/noninferiority/#practical-recommendations","title":"Practical Recommendations","text":"<ul> <li>Choose an outlier score that captures the phenomenon you care about (e.g.,  clinical outcomes, high reconstruction error, extreme probability  values, etc.).</li> <li>Consider reporting both distributional and noninferiority results when  monitoring production systems: a distributional difference (e.g. CTST) does not  always imply a practically meaningful or adverse change (e.g. DSOS).</li> </ul>"}]}