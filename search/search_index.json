{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#samesame","title":"samesame","text":"<p>Same, same but different ...</p> <p><code>samesame</code> implements classifier two-sample tests (CTSTs) and as a bonus  extension, a noninferiority test (NIT). </p> <p>These were either missing or implemented with some tradeoffs  (looking at you, sample-splitting) in existing libraries. And so,  <code>samesame</code> fills in the gaps :)</p>"},{"location":"#motivation","title":"Motivation","text":"<p>What is <code>samesame</code> good for? It is for data (model) validation, performance monitoring, drift detection (dataset shift), statistical process control, and so on and so forth. </p> <p>Want more?  Here you go. This motivating example comes from the related R package  <code>dsos</code>.</p>"},{"location":"#installation","title":"Installation","text":"<p>To install, run the following command:</p> <pre><code>python -m pip install samesame\n</code></pre>"},{"location":"#usage","title":"Usage","text":""},{"location":"#functionality","title":"Functionality","text":"<p>Below, you will find an overview of common modules in <code>samesame</code>. </p> Function Module Bayesian inference <code>samesame.bayes</code> Classifier two-sample tests (CTSTs) <code>samesame.ctst</code> Noninferiority tests (NITs) <code>samesame.nit</code>"},{"location":"#attributes","title":"Attributes","text":"<p>When the method is a statistical test, <code>samesame</code> saves (stores) the results of some potentially computationally intensive results in attributes. These attributes, when available, can be accessed as follows. </p> Attribute Description <code>.statistic</code> The test statistic for the hypothesis. <code>.null</code> The null distribution for the hypothesis. <code>.pvalue</code> The p-value for the hypothesis. <code>.posterior</code> The posterior distribution for the hypothesis. <code>.bayes_factor</code> The bayes factor for the hypothesis."},{"location":"#examples","title":"Examples","text":"<p>To get started, please see the examples in the docs.</p>"},{"location":"#dependencies","title":"Dependencies","text":"<p><code>samesame</code> has few dependencies beyond the standard library. It will  probably work with some older Python versions. It is, in short, a lightweight dependency for most machine learning projects.<code>samesame</code> is built on top of, and is compatible with, scikit-learn and numpy.</p>"},{"location":"examples/distribution-shifts/","title":"Test for distribution shifts","text":"<p>Given two datasets <code>sample_P</code> and <code>sample_Q</code> from distributions \\(P\\) and  \\(Q\\), the goal is to estimate a \\(p\\)-value for the null hypothesis of equal distribution \\(P=Q\\). <code>samesame</code> implements classifier two-sample tests (CTSTs) for this use case. But why, you ask? I wax lyrical about CTSTs here.  The tl;dr is they are powerful, modular (flexible) and easy to explain, the  elusive trifecta.</p> <p>CTSTs assign your samples to different classes (you give them labels). <code>sample_P</code>  is the positive class, and <code>sample_Q</code>, the negative one or vice versa. Then, you  fit your favorite classifier to see if it can reliably predict the labels. If it  can, it means the two samples are probably different. If it cannot, the two  are more likely similar enough.</p>"},{"location":"examples/distribution-shifts/#data","title":"Data","text":"<p>Let's see it in action. First, generate some data.</p> <p><pre><code>from sklearn.datasets import make_classification\n# Create a small dataset\nX, y = make_classification(\n    n_samples=100,\n    n_features=4,\n    n_classes=2,\n    random_state=123_456,\n)\n</code></pre> The two samples, <code>sample_P</code> and <code>sample_Q</code>, are concatenated into a single dataset, <code>X</code>. The binary labels, <code>y</code>, indicate sample membership, which examples belong to which sample.</p>"},{"location":"examples/distribution-shifts/#cross-fitted-predictions","title":"Cross-fitted predictions","text":"<p>Let's run the tests using cross-fitting. Cross-fitting estimates the performance of a model on unseen data by using out-of-sample predictions.</p> <pre><code>from sklearn.ensemble import HistGradientBoostingClassifier\nfrom sklearn.model_selection import cross_val_predict\n# Get cross-fitted predictions\ny_gb = cross_val_predict(\n    estimator=HistGradientBoostingClassifier(random_state=123_456),\n    X=X,\n    y=y,\n    cv=10,\n    method='predict_proba',\n)[:, 1]  # Get probabilities for the positive class\n# Binarize predictions to be compatible with some metrics\nfrom sklearn.preprocessing import binarize\ny_gb_binary = binarize(y_gb.reshape(-1, 1), threshold=0.5).ravel()\n</code></pre>"},{"location":"examples/distribution-shifts/#classifier-two-sample-tests-ctsts","title":"Classifier two-sample tests (CTSTs)","text":"<p>We can turn binary performance metrics into statistical tests like so:</p> <pre><code>from samesame.ctst import CTST\nfrom sklearn.metrics import balanced_accuracy_score, matthews_corrcoef, roc_auc_score\n\nmetrics = [balanced_accuracy_score, matthews_corrcoef, roc_auc_score]\nfor metric in metrics:\n    predicted = y_gb_binary if metric != roc_auc_score else y_gb\n    ctst = CTST(actual=y, predicted=predicted, metric=metric)\n    print(f\"{metric.__name__}\")\n    print(f\"\\t statistic: {ctst.statistic:.2f}\")\n    print(f\"\\t p-value: {ctst.pvalue:.2f}\")\n</code></pre> <pre><code>balanced_accuracy_score\n     statistic: 0.86\n     p-value: 0.00\nmatthews_corrcoef\n     statistic: 0.72\n     p-value: 0.00\nroc_auc_score\n     statistic: 0.93\n     p-value: 0.00\n</code></pre> <p>In all 3 cases, we reject the null hypothesis of equal distribution \\(P=Q\\). The classifier, <code>HistGradientBoostingClassifier</code>, was able to distinguish between the two samples.</p>"},{"location":"examples/distribution-shifts/#out-of-bag-predictions","title":"Out-of-bag predictions","text":"<p>Instead of cross-fitting, we can also use out-of-bag predictions. Both cross-fitted and out-of-bag predictions don't lose (too many) samples to estimation whereas sample splitting typically uses half the data for  estimation and half for inference, which incurs a loss of statistical power.</p> <pre><code>from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(\n    n_estimators=500,\n    random_state=123_456,\n    oob_score=True,\n    min_samples_leaf=10,\n)\n# Get out-of-bag predictions\ny_rf = rf.fit(X, y).oob_decision_function_[:, 1]\ny_rf_binary = binarize(y_rf.reshape(-1, 1), threshold=0.5).ravel()\nfor metric in metrics:\n    predicted = y_rf_binary if metric != roc_auc_score else y_rf\n    ctst = CTST(actual=y, predicted=predicted, metric=metric)\n    print(f\"{metric.__name__}\")\n    print(f\"\\t statistic: {ctst.statistic:.2f}\")\n    print(f\"\\t p-value: {ctst.pvalue:.2f}\")\n</code></pre> <p>As before, we reject the null hypothesis of equal distribution \\(P=Q\\). The classifier, <code>RandomForestClassifier</code>, was able to tell apart the two samples.</p> <pre><code>balanced_accuracy_score\n     statistic: 0.88\n     p-value: 0.00\nmatthews_corrcoef\n     statistic: 0.76\n     p-value: 0.00\nroc_auc_score\n     statistic: 0.94\n     p-value: 0.00\n</code></pre>"},{"location":"examples/distribution-shifts/#explanations","title":"Explanations","text":"<p>To explain these results, we can use explainable methods on the fitted classifiers above e.g. <code>rf</code>. These help answer questions such as which features contribute the most to distribution shift (feature importance).</p>"},{"location":"examples/distribution-shifts/#conclusion","title":"Conclusion","text":"<p>And voila\u0300! You have successfully run classifier two-sample tests (CTSTs). Note the flexibility (modularity) of the approach. You can use both  cross-fitted and out-of-bag predictions, instead of sample splitting, for inference. You can use any classifier (e.g., <code>HistGradientBoostingClassifier</code>,  <code>RandomForestClassifier</code>, etc.), and any binary classification metric (e.g., <code>AUC</code>, <code>balanced accuracy</code>, <code>Matthews correlation coefficient</code>, etc.).</p>"},{"location":"examples/noninferiority/","title":"Noninferiority Test","text":"<p>Sometimes the pertinent question is, are we worse off? And when that is so, statistical tests of equal distribution and of mean difference are not the best tools for the job (see here).</p> <p>Noninferiority tests can help but these tests come with their own challenges. Typically, we need to define a pre-specified margin, the minimum meaningful difference needed to sound the alarm. This can be difficult, and controversial, to set in advance even for domain experts.</p>"},{"location":"examples/noninferiority/#d-sos","title":"D-SOS","text":"<p>Enter D-SOS,  short for dataset shift with outlier scores. D-SOS is a robust nonparametric noninferiority test that does not require a pre-specified margin. It tests the null of no adverse shift based on outlier scores i.e. it checks whether the new sample is not substantively worse than the old sample, and not if the two are equal as tests of equal distributions do. This two-sample comparison assumes that we have both a training set, the reference  distribution of outlier scores, and a test set.</p>"},{"location":"examples/noninferiority/#prologue","title":"Prologue","text":"<p>An example best illustrates how to use the method. The  case study, reproduced below, is from a clinical trial.</p> <p>You are a consulting statistician at a pharmaceutical company, charged with designing a study of your company\u2019s new arthritis drug, SASGoBowlFor\u2019Em (abbreviated as \u201cBowl\u201d). Your boss realizes that Bowl is unlikely to demonstrate better efficacy than the gold standard, Armanaleg, but its lower cost will make it an attractive alternative for consumers as long as you can show that the efficacy is about the same.</p> <p>Your boss communicates the following study plans to you:</p> <ul> <li>The outcome to be measured is a \u201crelief score,\u201d which ranges from 0 to 20 and is assumed to be approximately normally distributed.</li> <li>Subjects are to be allocated to Armanaleg and Bowl at a ratio of 2 to 3, respectively.</li> <li>The relief score is to be assessed after four weeks on the treatment.</li> <li>Bowl is expected to be slightly less effective than Armanaleg, with a mean relief score of 9.5 compared to 10 for Armanaleg.</li> <li>The minimally acceptable decrease in relief score is considered to be 2 units, corresponding to a 20% decrease, assuming a mean relief score of 10 for Armanaleg.</li> <li>The standard deviation of the relief score is expected to be approximately 2.25 for each treatment. Common standard deviation will be assumed in the data analysis.</li> <li>The sample size should be sufficient to produce an 85% chance of a significant result\u2014that is, a power of 0.85\u2014at a 0.05 significance level.</li> </ul> <p>While quite a bit of this context is helpful and needed to run Schuirmann\u2019s classic method of two one-sided tests, this is not required for D-SOS. The latter assumes no parametric form for the data (normality), and does not require a pre-specified margin (2 units decrease in relief score).</p>"},{"location":"examples/noninferiority/#data","title":"Data","text":"<p>D-SOS works with outlier scores so we turn these \"relief scores\" into \"discomfort scores\" so that the higher the score, the worse the outcome.</p> <pre><code>import numpy as np\n\ndatalines = \"9 14 13 8 10 5 11 9 12 10 9 11 8 11 \\\n4 8 11 16 12 10 9 10 13 12 11 13 9 4 \\\n7 14 8 4 10 11 7 7 13 8 8 13 10 9 \\\n12 9 11 10 12 7 8 5 10 7 13 12 13 11 \\\n7 12 10 11 10 8 6 9 11 8 5 11 10 8\".split()\nrelief = [float(s) for s in datalines]\ndiscomfort = [max(relief) - s for s in relief]\narmanaleg = np.array(discomfort[:28])\nbowl = np.array(discomfort[28:])\n</code></pre>"},{"location":"examples/noninferiority/#analysis","title":"Analysis","text":"<p>To run the test, we specify <code>armanaleg</code> as the control (reference/first sample) and <code>bowl</code> as the new treatment (second sample).</p> <p><pre><code>from samesame.bayes import as_pvalue\nfrom samesame.nit import DSOS\n# alternatively: from samesame.nit import WeightedAUC as DSOS\n\ndsos = DSOS.from_samples(armanaleg, bowl)\nfrequentist = dsos.pvalue\nbayesian = as_pvalue(dsos.bayes_factor)\n</code></pre> ... And the results? Drumroll, please. We fail to reject the null of no adverse shift. That is, we are not worse off with the new treatment.</p> <pre><code>print(f\"Frequentist p-value: {frequentist:.4f}\")\nprint(f\"Bayesian p-value: {bayesian:.4f}\")\n</code></pre> <pre><code>Frequentist p-value: 0.1215\nBayesian p-value: 0.1159\n</code></pre> <p>This is consistent with the original analysis which rejects the null  (a different null than the D-SOS null!) with a p-value of \\(p = 0.0192\\) and concludes:</p> <p>This suggests, as you\u2019d hoped, that the efficacy of Bowl is not appreciably worse than that of Armanaleg</p>"},{"location":"examples/noninferiority/#epilogue","title":"Epilogue","text":"<p>... did you catch it? Under the hood, the D-SOS test is a classifier two-sample test (CTST). It uses outlier scores as predicted values, the weighted AUC as the performance metric and tests against a one-sided alternative (p-value) instead of a two-sided one.</p>"}]}