{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#samesame","title":"samesame","text":"<p>Same, same but different ...</p> <p><code>samesame</code> implements classifier two-sample tests (CTSTs) and as a bonus extension, a noninferiority test (NIT). These tests are either missing or implemented with significant tradeoffs (looking at you, sample-splitting) in existing libraries.</p> <p><code>samesame</code> is versatile, extensible, lightweight, powerful, and agnostic to your inference strategy so long as it is valid (e.g. cross-fitting, sample splitting, etc.).</p>"},{"location":"#motivation","title":"Motivation","text":"<p><code>samesame</code> is for those who need statistical tests for:</p> <ul> <li>Data validation - Verify that data distributions meet expectations</li> <li>Model performance monitoring - Detect performance degradation over time</li> <li>Drift detection - Identify dataset shifts between training and production</li> <li>Statistical process control - Monitor system behavior and quality</li> </ul> <p>A motivating example is available from the related R package <code>dsos</code>, which provides some of the same functionality.</p>"},{"location":"#installation","title":"Installation","text":"<p>To install, run the following command:</p> <pre><code>python -m pip install samesame\n</code></pre>"},{"location":"#quick-start","title":"Quick Start","text":"<p>This example demonstrates the key distinction between tests of equal distribution and noninferiority tests\u2014a critical difference for avoiding false alarms in production systems.</p> <p>Simulate outlier scores to test for no adverse shift:</p> <pre><code>from samesame.ctst import CTST\nfrom samesame.nit import DSOS\nfrom sklearn.metrics import roc_auc_score\nimport numpy as np\n\nn_size = 600\nrng = np.random.default_rng(123_456)\nos_train = rng.normal(size=n_size)\nos_test = rng.normal(size=n_size)\nnull_ctst = CTST.from_samples(os_train, os_test, metric=roc_auc_score)\nnull_dsos = DSOS.from_samples(os_train, os_test)\n</code></pre> <p>Test of equal distribution (CTST): Rejects the null of equal distributions</p> <pre><code>print(f\"{null_ctst.pvalue=:.4f}\")\n# null_ctst.pvalue=0.0358\n</code></pre> <p>Noninferiority test (DSOS): Fails to reject the null of no adverse shift</p> <pre><code>print(f\"{null_dsos.pvalue=:.4f}\")\n# null_dsos.pvalue=0.9500\n</code></pre> <p>Key insight: While the test sample (<code>os_test</code>) has a statistically different distribution from the training sample (<code>os_train</code>), it does not contain disproportionally more outliers. This distinction is exactly what <code>samesame</code> highlights\u2014many practitioners conflate \"different distribution\" with \"problematic shift,\" but <code>samesame</code> helps you distinguish between the two.</p>"},{"location":"#usage","title":"Usage","text":""},{"location":"#functionality","title":"Functionality","text":"<p>Below, you will find an overview of common modules in <code>samesame</code>.</p> Function Module Bayesian inference <code>samesame.bayes</code> Classifier two-sample tests (CTSTs) <code>samesame.ctst</code> Noninferiority tests (NITs) <code>samesame.nit</code> Out-of-distribution (OOD) detection <code>samesame.ood</code>"},{"location":"#attributes","title":"Attributes","text":"<p>When the method is a statistical test, <code>samesame</code> saves (stores) the results of some potentially computationally intensive results in attributes. These attributes, when available, can be accessed as follows.</p> Attribute Description <code>.statistic</code> The test statistic for the hypothesis. <code>.null</code> The null distribution for the hypothesis. <code>.pvalue</code> The p-value for the hypothesis. <code>.posterior</code> The posterior distribution for the hypothesis. <code>.bayes_factor</code> The bayes factor for the hypothesis."},{"location":"#examples","title":"Examples","text":"<p>To get started, please see the examples in the docs.</p>"},{"location":"#dependencies","title":"Dependencies","text":"<p><code>samesame</code> has minimal dependencies beyond the Python standard library, making it a lightweight addition to most machine learning projects. It is built on top of, and fully compatible with, scikit-learn and numpy.</p>"},{"location":"api/bayes/","title":"bayes","text":"<p>Functions for working with p-values and Bayes factors.</p> <p>This module provides functions for converting between p-values and Bayes factors, as well as calculating Bayes factors from posterior distributions. These are useful for hypothesis testing and interpreting statistical evidence.</p>"},{"location":"api/bayes/#samesame.bayes.as_bf","title":"<code>as_bf(pvalue)</code>","text":"<p>Convert a one-sided p-value to a Bayes factor.</p> <p>This Bayes factor quantifies evidence in favour of a directional effect over the null hypothesis of no such effect.</p> <p>Parameters:</p> Name Type Description Default <code>pvalue</code> <code>NDArray | float</code> <p>The p-value(s) to be converted to Bayes factor(s). Can be a single value or an array of values.</p> required <p>Returns:</p> Type Description <code>NDArray | float</code> <p>The corresponding Bayes factor(s). The return type matches the input type.</p> See Also <p>as_pvalue : Convert a Bayes factor to a p-value.</p> Notes <p>The Bayes factor is derived from the one-sided p-value using a Bayesian interpretation, which quantifies evidence in favor of a directional effect over the null hypothesis of no such effect.</p> <p>See [1]_ for theoretical details and implications.</p> References <p>.. [1] Marsman, Maarten, and Eric-Jan Wagenmakers. \"Three Insights from    a Bayesian Interpretation of the One-Sided P Value.\" Educational and    Psychological Measurement, vol. 77, no. 3, 2017, pp. 529-539.    doi:10.1177/0013164416669201.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from samesame.bayes import as_bf\n&gt;&gt;&gt; as_bf(0.5)\nnp.float64(1.0)\n&gt;&gt;&gt; as_bf(np.array([0.05, 0.1, 0.5]))\narray([19., 9., 1.])\n</code></pre> Source code in <code>src/samesame/bayes.py</code> <pre><code>def as_bf(\n    pvalue: NDArray | float,\n) -&gt; NDArray | float:\n    \"\"\"\n    Convert a one-sided p-value to a Bayes factor.\n\n    This Bayes factor quantifies evidence in favour of a directional effect\n    over the null hypothesis of no such effect.\n\n    Parameters\n    ----------\n    pvalue : NDArray | float\n        The p-value(s) to be converted to Bayes factor(s). Can be a single\n        value or an array of values.\n\n    Returns\n    -------\n    NDArray | float\n        The corresponding Bayes factor(s). The return type matches the\n        input type.\n\n    See Also\n    --------\n    as_pvalue : Convert a Bayes factor to a p-value.\n\n    Notes\n    -----\n    The Bayes factor is derived from the one-sided p-value using a\n    Bayesian interpretation, which quantifies evidence in favor of a\n    directional effect over the null hypothesis of no such effect.\n\n    See [1]_ for theoretical details and implications.\n\n    References\n    ----------\n    .. [1] Marsman, Maarten, and Eric-Jan Wagenmakers. \"Three Insights from\n       a Bayesian Interpretation of the One-Sided P Value.\" *Educational and\n       Psychological Measurement*, vol. 77, no. 3, 2017, pp. 529-539.\n       doi:10.1177/0013164416669201.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from samesame.bayes import as_bf\n    &gt;&gt;&gt; as_bf(0.5)\n    np.float64(1.0)\n    &gt;&gt;&gt; as_bf(np.array([0.05, 0.1, 0.5])) # doctest: +NORMALIZE_WHITESPACE\n    array([19., 9., 1.])\n    \"\"\"\n    if np.any(np.logical_or(pvalue &gt;= 1, pvalue &lt;= 0)):\n        raise ValueError(\"pvalue must be within the open interval (0, 1).\")\n    pvalue = np.clip(pvalue, 1e-10, 1 - 1e-10)  # Avoid numerical issues at extremes\n    return 1.0 / np.exp(logit(pvalue))  # evidence in favor of an effect\n</code></pre>"},{"location":"api/bayes/#samesame.bayes.as_pvalue","title":"<code>as_pvalue(bayes_factor)</code>","text":"<p>Convert a Bayes factor of a directional effect to a one-sided p-value.</p> <p>The Bayes factor quantifies evidence in favour of a directional effect over the null hypothesis of no such effect.</p> <p>Parameters:</p> Name Type Description Default <code>bayes_factor</code> <code>float | NDArray</code> <p>The Bayes factor(s) to be converted to p-value(s). Can be a single value or an array of values.</p> required <p>Returns:</p> Type Description <code>float | NDArray</code> <p>The corresponding p-value(s). The return type matches the input type.</p> See Also <p>as_pvalue : Convert a Bayes factor to a p-value.</p> Notes <p>The equivalence between the Bayes factor in favor of a directional effect and the one-sided p-value for the null of no such effect is discussed in [1]_.</p> References <p>.. [1] Marsman, Maarten, and Eric-Jan Wagenmakers. \"Three Insights from a    Bayesian Interpretation of the One-Sided P Value.\" Educational and    Psychological Measurement, vol. 77, no. 3, 2017, pp. 529\u2013539,    https://doi.org/10.1177/0013164416669201.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from samesame.bayes import as_pvalue\n&gt;&gt;&gt; as_pvalue(1)\nnp.float64(0.5)\n&gt;&gt;&gt; as_pvalue(np.array([19.0, 9.0, 1.0]))\narray([0.05, 0.1 , 0.5 ])\n</code></pre> Source code in <code>src/samesame/bayes.py</code> <pre><code>def as_pvalue(\n    bayes_factor: float | NDArray,\n) -&gt; float | NDArray:\n    \"\"\"\n    Convert a Bayes factor of a directional effect to a one-sided p-value.\n\n    The Bayes factor quantifies evidence in favour of a directional effect\n    over the null hypothesis of no such effect.\n\n    Parameters\n    ----------\n    bayes_factor : float | NDArray\n        The Bayes factor(s) to be converted to p-value(s). Can be a single\n        value or an array of values.\n\n    Returns\n    -------\n    float | NDArray\n        The corresponding p-value(s). The return type matches the input type.\n\n    See Also\n    --------\n    as_pvalue : Convert a Bayes factor to a p-value.\n\n    Notes\n    -----\n    The equivalence between the Bayes factor in favor of a directional effect\n    and the one-sided p-value for the null of no such effect is discussed in\n    [1]_.\n\n    References\n    ----------\n    .. [1] Marsman, Maarten, and Eric-Jan Wagenmakers. \"Three Insights from a\n       Bayesian Interpretation of the One-Sided P Value.\" *Educational and\n       Psychological Measurement*, vol. 77, no. 3, 2017, pp. 529\u2013539,\n       https://doi.org/10.1177/0013164416669201.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from samesame.bayes import as_pvalue\n    &gt;&gt;&gt; as_pvalue(1)\n    np.float64(0.5)\n    &gt;&gt;&gt; as_pvalue(np.array([19.0, 9.0, 1.0])) # doctest: +NORMALIZE_WHITESPACE\n    array([0.05, 0.1 , 0.5 ])\n    \"\"\"\n    if np.any(bayes_factor &lt;= 0):\n        raise ValueError(\"bayes_factor must be strictly positive.\")\n    bf_ = np.clip(bayes_factor, 1e-10, 1e10)  # Ensure numerical stability\n    pvalue = expit(-np.log(bf_))\n    return pvalue\n</code></pre>"},{"location":"api/bayes/#samesame.bayes.bayes_factor","title":"<code>bayes_factor(posterior, threshold=0.0, adjustment=0)</code>","text":"<p>Compute the Bayes factor for a test of direction given a threshold.</p> <p>The Bayes factor quantifies the evidence in favor of the hypothesis that the posterior distribution exceeds the given threshold compared to the hypothesis that it does not.</p> <p>Parameters:</p> Name Type Description Default <code>posterior</code> <code>NDArray</code> <p>An array of posterior samples.</p> required <code>threshold</code> <code>float</code> <p>The threshold value to test against. Default is 0.0.</p> <code>0.0</code> <code>adjustment</code> <code>(0, 1)</code> <p>Adjustment to apply to the Bayes factor calculation. Default is 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>float</code> <p>The computed Bayes factor.</p> See Also <p>as_pvalue : Convert a Bayes factor to a p-value.</p> <p>as_bf : Convert a p-value to a Bayes factor.</p> Notes <p>The Bayes factor is a measure of evidence which compares the likelihood of the data under two competing hypotheses. In this function, the numerator represents the proportion of posterior samples exceeding the threshold, while the denominator represents the proportion of samples not exceeding the threshold. If all samples exceed the threshold, the Bayes factor is set to infinity, indicating overwhelming evidence in favor of the hypothesis.</p> <p>The adjustment parameter allows for slight modifications to the Bayes factor calculation, which can be useful in specific contexts such as sensitivity analyses.</p> <p>See [1]_ for further theoretical details and practical implications of this approach.</p> References <p>.. [1] Marsman, Maarten, and Eric-Jan Wagenmakers. \"Three Insights from a    Bayesian Interpretation of the One-Sided P Value.\" Educational and    Psychological Measurement, vol. 77, no. 3, 2017, pp. 529-539.    doi:10.1177/0013164416669201.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from samesame.bayes import bayes_factor\n&gt;&gt;&gt; posterior_samples = np.array([0.2, 0.5, 0.8, 0.9])\n&gt;&gt;&gt; bayes_factor(posterior_samples, threshold=0.5)\nnp.float64(1.0)\n</code></pre> Source code in <code>src/samesame/bayes.py</code> <pre><code>def bayes_factor(\n    posterior: NDArray,\n    threshold: float = 0.0,\n    adjustment: Literal[0, 1] = 0,\n) -&gt; float:\n    \"\"\"\n    Compute the Bayes factor for a test of direction given a threshold.\n\n    The Bayes factor quantifies the evidence in favor of the hypothesis that\n    the posterior distribution exceeds the given threshold compared to the\n    hypothesis that it does not.\n\n    Parameters\n    ----------\n    posterior : NDArray\n        An array of posterior samples.\n    threshold : float, optional\n        The threshold value to test against. Default is 0.0.\n    adjustment : {0, 1}, optional\n        Adjustment to apply to the Bayes factor calculation. Default is 0.\n\n    Returns\n    -------\n    float\n        The computed Bayes factor.\n\n    See Also\n    --------\n    as_pvalue : Convert a Bayes factor to a p-value.\n\n    as_bf : Convert a p-value to a Bayes factor.\n\n    Notes\n    -----\n    The Bayes factor is a measure of evidence which compares the likelihood of\n    the data under two competing hypotheses. In this function, the numerator\n    represents the proportion of posterior samples exceeding the threshold,\n    while the denominator represents the proportion of samples not exceeding\n    the threshold. If all samples exceed the threshold, the Bayes factor is\n    set to infinity, indicating overwhelming evidence in favor of the\n    hypothesis.\n\n    The adjustment parameter allows for slight modifications to the Bayes\n    factor calculation, which can be useful in specific contexts such as\n    sensitivity analyses.\n\n    See [1]_ for further theoretical details and practical implications of\n    this approach.\n\n    References\n    ----------\n    .. [1] Marsman, Maarten, and Eric-Jan Wagenmakers. \"Three Insights from a\n       Bayesian Interpretation of the One-Sided P Value.\" *Educational and\n       Psychological Measurement*, vol. 77, no. 3, 2017, pp. 529-539.\n       doi:10.1177/0013164416669201.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from samesame.bayes import bayes_factor\n    &gt;&gt;&gt; posterior_samples = np.array([0.2, 0.5, 0.8, 0.9])\n    &gt;&gt;&gt; bayes_factor(posterior_samples, threshold=0.5)\n    np.float64(1.0)\n    \"\"\"\n    return _bayes_factor(posterior, threshold, adjustment)\n</code></pre>"},{"location":"api/ctst/","title":"ctst","text":"<p>Classifier two-sample tests (CTST) from binary classification metrics.</p> <p>The classifier two-sample test broadly consists of three steps: (1) training a classifier, (2) scoring the two samples and (3) turning a test statistic into a p-value from these scores. This test statistic can be the performance metric of a binary classifier such as the (weighted) area under the receiver operating characteristic curve, the Matthews correlation coefficient, and the (balanced) accuracy. This module tackles step (3).</p> References <p>.. [1] Lopez-Paz, David, and Maxime Oquab. \"Revisiting Classifier Two-Sample    Tests.\" International Conference on Learning Representations. 2017.</p> <p>.. [2] Friedman, Jerome. \"On multivariate goodness-of-fit and two-sample    testing.\" No. SLAC-PUB-10325. SLAC National Accelerator Laboratory (SLAC),    Menlo Park, CA (United States), 2004.</p> <p>.. [3] K\u00fcbler, Jonas M., et al. \"Automl two-sample test.\" Advances in Neural    Information Processing Systems 35 (2022): 15929-15941.</p> <p>.. [4] Ci\u00e9men\u00e7on, St\u00e9phan, Marine Depecker, and Nicolas Vayatis. \"AUC    optimization and the two-sample problem.\" Proceedings of the 23rd    International Conference on Neural Information Processing Systems. 2009.</p> <p>.. [5] Hediger, Simon, Loris Michel, and Jeffrey N\u00e4f. \"On the use of random    forest for two-sample testing.\" Computational Statistics &amp; Data Analysis    170 (2022): 107435.</p> <p>.. [6] Kim, Ilmun, et al. \"Classification accuracy as a proxy for two-sample    testing.\" Annals of Statistics 49.1 (2021): 411-434.</p>"},{"location":"api/ctst/#samesame.ctst.CTST","title":"<code>CTST</code>  <code>dataclass</code>","text":"<p>Classifier two-sample test (CTST) using a binary classification metric.</p> <p>This test compares scores (predictions) from two independent samples. Rejecting the null implies that scoring is not random and that the classifier is able to distinguish between the two samples.</p> <p>Attributes:</p> Name Type Description <code>actual</code> <code>NDArray</code> <p>Binary indicator for sample membership.</p> <code>predicted</code> <code>NDArray</code> <p>Estimated (predicted) scores for corresponding samples in <code>actual</code>.</p> <code>metric</code> <code>Callable</code> <p>A callable that conforms to scikit-learn metric API. This function must take two positional arguments e.g. <code>y_true</code> and <code>y_pred</code>.</p> <code>n_resamples</code> <code>(int, optional)</code> <p>Number of resampling iterations, by default 9999.</p> <code>rng</code> <code>(Generator, optional)</code> <p>Random number generator, by default np.random.default_rng().</p> <code>n_jobs</code> <code>(int, optional)</code> <p>Number of parallel jobs, by default 1.</p> <code>batch</code> <code>(int or None, optional)</code> <p>Batch size for parallel processing, by default None.</p> <code>alternative</code> <code>({'less', 'greater', 'two-sided'}, optional)</code> <p>Defines the alternative hypothesis. Default is 'two-sided'.</p> Notes <p>The null distribution is based on permutations. See <code>scipy.stats.permutation_test</code> for more details.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from sklearn.metrics import matthews_corrcoef, roc_auc_score\n&gt;&gt;&gt; from samesame.ctst import CTST\n&gt;&gt;&gt; actual = np.array([0, 1, 1, 0])\n&gt;&gt;&gt; scores = np.array([0.2, 0.8, 0.6, 0.4])\n&gt;&gt;&gt; ctst_mcc = CTST(actual, scores, metric=matthews_corrcoef)\n&gt;&gt;&gt; ctst_auc = CTST(actual, scores, metric=roc_auc_score)\n&gt;&gt;&gt; print(ctst_mcc.pvalue)\n&gt;&gt;&gt; print(ctst_auc.pvalue)\n&gt;&gt;&gt; ctst_ = CTST.from_samples(scores, scores, metric=roc_auc_score)\n&gt;&gt;&gt; isinstance(ctst_, CTST)\nTrue\n</code></pre> Source code in <code>src/samesame/ctst.py</code> <pre><code>@dataclass\nclass CTST:\n    \"\"\"\n    Classifier two-sample test (CTST) using a binary classification metric.\n\n    This test compares scores (predictions) from two independent samples.\n    Rejecting the null implies that scoring is not random and that the\n    classifier is able to distinguish between the two samples.\n\n    Attributes\n    ----------\n    actual : NDArray\n        Binary indicator for sample membership.\n    predicted : NDArray\n        Estimated (predicted) scores for corresponding samples in `actual`.\n    metric : Callable\n        A callable that conforms to scikit-learn metric API. This function\n        must take two positional arguments e.g. `y_true` and `y_pred`.\n    n_resamples : int, optional\n        Number of resampling iterations, by default 9999.\n    rng : np.random.Generator, optional\n        Random number generator, by default np.random.default_rng().\n    n_jobs : int, optional\n        Number of parallel jobs, by default 1.\n    batch : int or None, optional\n        Batch size for parallel processing, by default None.\n    alternative : {'less', 'greater', 'two-sided'}, optional\n        Defines the alternative hypothesis. Default is 'two-sided'.\n\n    Notes\n    -----\n    The null distribution is based on permutations.\n    See `scipy.stats.permutation_test` for more details.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from sklearn.metrics import matthews_corrcoef, roc_auc_score\n    &gt;&gt;&gt; from samesame.ctst import CTST\n    &gt;&gt;&gt; actual = np.array([0, 1, 1, 0])\n    &gt;&gt;&gt; scores = np.array([0.2, 0.8, 0.6, 0.4])\n    &gt;&gt;&gt; ctst_mcc = CTST(actual, scores, metric=matthews_corrcoef)\n    &gt;&gt;&gt; ctst_auc = CTST(actual, scores, metric=roc_auc_score)\n    &gt;&gt;&gt; print(ctst_mcc.pvalue) # doctest: +SKIP\n    &gt;&gt;&gt; print(ctst_auc.pvalue) # doctest: +SKIP\n    &gt;&gt;&gt; ctst_ = CTST.from_samples(scores, scores, metric=roc_auc_score)\n    &gt;&gt;&gt; isinstance(ctst_, CTST)\n    True\n    \"\"\"\n\n    actual: NDArray = field(repr=False)\n    predicted: NDArray = field(repr=False)\n    metric: Callable\n    n_resamples: int = 9999\n    rng: np.random.Generator = np.random.default_rng()\n    n_jobs: int = 1\n    batch: int | None = None\n    alternative: Literal[\"less\", \"greater\", \"two-sided\"] = \"two-sided\"\n\n    def __post_init__(self):\n        \"\"\"Validate inputs.\"\"\"\n        self.actual = column_or_1d(self.actual)\n        self.predicted = column_or_1d(self.predicted)\n        check_consistent_length(self.actual, self.predicted)\n        assert type_of_target(self.actual, \"actual\") == \"binary\"\n        type_predicted = type_of_target(self.predicted, \"predicted\")\n        assert type_predicted in (\n            \"binary\",\n            \"continuous\",\n            \"multiclass\",\n        ), f\"Expected 'predicted' to be binary or continuous, got {type_predicted}.\"\n        assert check_metric_function(self.metric), (\n            f\"'metric' expects a callable that conforms to scikit-learn metric. \"\n            f\"{signature(self.metric)=} does not.\"\n        )\n\n    @cached_property\n    def _result(self):\n        def statistic(*args):\n            return self.metric(args[0], args[1])\n\n        return permutation_test(\n            data=(self.actual, self.predicted),\n            statistic=statistic,\n            permutation_type=\"pairings\",\n            n_resamples=self.n_resamples,\n            alternative=self.alternative,\n            random_state=self.rng,\n        )\n\n    @cached_property\n    def statistic(self) -&gt; float:\n        \"\"\"\n        Compute the observed test statistic.\n\n        Returns\n        -------\n        float\n            The test statistic.\n\n        Notes\n        -----\n        The result is cached to avoid (expensive) recomputation.\n        \"\"\"\n        return self._result.statistic\n\n    @cached_property\n    def null(self) -&gt; NDArray:\n        \"\"\"\n        Compute the null distribution of the test statistic.\n\n        Notes\n        -----\n        The result is cached to avoid (expensive) recomputation since the\n        null distribution requires permutations.\n        \"\"\"\n        return self._result.null_distribution\n\n    @cached_property\n    def pvalue(self):\n        \"\"\"\n        Compute the p-value using permutations.\n\n        Notes\n        -----\n        The result is cached to avoid (expensive) recomputation.\n        \"\"\"\n        return self._result.pvalue\n\n    @classmethod\n    def from_samples(\n        cls,\n        first_sample: NDArray,\n        second_sample: NDArray,\n        metric: Callable,\n        n_resamples: int = 9999,\n        rng: np.random.Generator = np.random.default_rng(),\n        n_jobs: int = 1,\n        batch: int | None = None,\n        alternative: Literal[\"less\", \"greater\", \"two-sided\"] = \"two-sided\",\n    ):\n        \"\"\"\n        Create a CTST instance from two samples.\n\n        Parameters\n        ----------\n        first_sample : NDArray\n            First sample of scores. These can be binary or continuous.\n        second_sample : NDArray\n            Second sample of scores. These can be binary or continuous.\n\n        Returns\n        -------\n        CTST\n            An instance of the CTST class.\n        \"\"\"\n        assert type_of_target(first_sample) == type_of_target(second_sample)\n        samples = (first_sample, second_sample)\n        actual = assign_labels(samples)\n        predicted = concat_samples(samples)\n        return cls(\n            actual,\n            predicted,\n            metric,\n            n_resamples,\n            rng,\n            n_jobs,\n            batch,\n            alternative,\n        )\n</code></pre>"},{"location":"api/ctst/#samesame.ctst.CTST.null","title":"<code>null</code>  <code>cached</code> <code>property</code>","text":"<p>Compute the null distribution of the test statistic.</p> Notes <p>The result is cached to avoid (expensive) recomputation since the null distribution requires permutations.</p>"},{"location":"api/ctst/#samesame.ctst.CTST.pvalue","title":"<code>pvalue</code>  <code>cached</code> <code>property</code>","text":"<p>Compute the p-value using permutations.</p> Notes <p>The result is cached to avoid (expensive) recomputation.</p>"},{"location":"api/ctst/#samesame.ctst.CTST.statistic","title":"<code>statistic</code>  <code>cached</code> <code>property</code>","text":"<p>Compute the observed test statistic.</p> <p>Returns:</p> Type Description <code>float</code> <p>The test statistic.</p> Notes <p>The result is cached to avoid (expensive) recomputation.</p>"},{"location":"api/ctst/#samesame.ctst.CTST.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate inputs.</p> Source code in <code>src/samesame/ctst.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Validate inputs.\"\"\"\n    self.actual = column_or_1d(self.actual)\n    self.predicted = column_or_1d(self.predicted)\n    check_consistent_length(self.actual, self.predicted)\n    assert type_of_target(self.actual, \"actual\") == \"binary\"\n    type_predicted = type_of_target(self.predicted, \"predicted\")\n    assert type_predicted in (\n        \"binary\",\n        \"continuous\",\n        \"multiclass\",\n    ), f\"Expected 'predicted' to be binary or continuous, got {type_predicted}.\"\n    assert check_metric_function(self.metric), (\n        f\"'metric' expects a callable that conforms to scikit-learn metric. \"\n        f\"{signature(self.metric)=} does not.\"\n    )\n</code></pre>"},{"location":"api/ctst/#samesame.ctst.CTST.from_samples","title":"<code>from_samples(first_sample, second_sample, metric, n_resamples=9999, rng=np.random.default_rng(), n_jobs=1, batch=None, alternative='two-sided')</code>  <code>classmethod</code>","text":"<p>Create a CTST instance from two samples.</p> <p>Parameters:</p> Name Type Description Default <code>first_sample</code> <code>NDArray</code> <p>First sample of scores. These can be binary or continuous.</p> required <code>second_sample</code> <code>NDArray</code> <p>Second sample of scores. These can be binary or continuous.</p> required <p>Returns:</p> Type Description <code>CTST</code> <p>An instance of the CTST class.</p> Source code in <code>src/samesame/ctst.py</code> <pre><code>@classmethod\ndef from_samples(\n    cls,\n    first_sample: NDArray,\n    second_sample: NDArray,\n    metric: Callable,\n    n_resamples: int = 9999,\n    rng: np.random.Generator = np.random.default_rng(),\n    n_jobs: int = 1,\n    batch: int | None = None,\n    alternative: Literal[\"less\", \"greater\", \"two-sided\"] = \"two-sided\",\n):\n    \"\"\"\n    Create a CTST instance from two samples.\n\n    Parameters\n    ----------\n    first_sample : NDArray\n        First sample of scores. These can be binary or continuous.\n    second_sample : NDArray\n        Second sample of scores. These can be binary or continuous.\n\n    Returns\n    -------\n    CTST\n        An instance of the CTST class.\n    \"\"\"\n    assert type_of_target(first_sample) == type_of_target(second_sample)\n    samples = (first_sample, second_sample)\n    actual = assign_labels(samples)\n    predicted = concat_samples(samples)\n    return cls(\n        actual,\n        predicted,\n        metric,\n        n_resamples,\n        rng,\n        n_jobs,\n        batch,\n        alternative,\n    )\n</code></pre>"},{"location":"api/nit/","title":"nit","text":""},{"location":"api/nit/#samesame.nit.WeightedAUC","title":"<code>WeightedAUC</code>  <code>dataclass</code>","text":"<p>               Bases: <code>CTST</code></p> <p>Two-sample test for no adverse shift using the weighted AUC (WAUC).</p> <p>This test compares scores from two independent samples. We reject the null hypothesis of no adverse shift for unusually high values of the WAUC i.e. when the second sample is relatively worse than the first one. This is a robust nonparametric noninferiority test (NIT) with no pre-specified margin. It can be used, amongst other things, to detect dataset shift with outlier scores, hence the DSOS acronym.</p> <p>Attributes:</p> Name Type Description <code>actual</code> <code>NDArray</code> <p>Binary indicator for sample membership.</p> <code>predicted</code> <code>NDArray</code> <p>Estimated (predicted) scores for corresponding samples in <code>actual</code>.</p> <code>n_resamples</code> <code>(int, optional)</code> <p>Number of resampling iterations, by default 9999.</p> <code>rng</code> <code>(Generator, optional)</code> <p>Random number generator, by default np.random.default_rng().</p> <code>n_jobs</code> <code>(int, optional)</code> <p>Number of parallel jobs, by default 1.</p> <code>batch</code> <code>(int or None, optional)</code> <p>Batch size for parallel processing, by default None.</p> See Also <p>bayes.as_bf : Convert a one-sided p-value to a Bayes factor.</p> <p>bayes.as_pvalue : Convert a Bayes factor to a one-sided p-value.</p> Notes <p>The frequentist null distribution of the WAUC is based on permutations [1]. The Bayesian posterior distribution of the WAUC is based on the Bayesian bootstrap [2]. Because this is a one-tailed test of direction (it asks the question, 'are we worse off?'), we can convert a one-sided p-value into a Bayes factor and vice versa. We can also use these p-values for sequential testing [3].</p> <p>The test assumes that <code>predicted</code> are outlier scores and/or encode some notions of outlyingness; higher value of <code>predicted</code> indicates worse outcomes.</p> References <p>.. [1] Kamulete, Vathy M. \"Test for non-negligible adverse shifts.\"    Uncertainty in Artificial Intelligence. PMLR, 2022.</p> <p>.. [2] Gu, Jiezhun, Subhashis Ghosal, and Anindya Roy. \"Bayesian bootstrap    estimation of ROC curve.\" Statistics in medicine 27.26 (2008): 5407-5420.</p> <p>.. [3] Kamulete, Vathy M. \"Are you OK? A Bayesian Sequential Test for    Adverse Shift.\" 2025.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from samesame.nit import WeightedAUC\n&gt;&gt;&gt; # alternatively: from samesame.nit import DSOS\n&gt;&gt;&gt; actual = np.array([0, 1, 1, 0])\n&gt;&gt;&gt; scores = np.array([0.2, 0.8, 0.6, 0.4])\n&gt;&gt;&gt; wauc = WeightedAUC(actual, scores)\n&gt;&gt;&gt; print(wauc.pvalue)\n&gt;&gt;&gt; print(wauc.bayes_factor)\n&gt;&gt;&gt; wauc_ = WeightedAUC.from_samples(scores, scores)\n&gt;&gt;&gt; isinstance(wauc_, WeightedAUC)\nTrue\n</code></pre> Source code in <code>src/samesame/nit.py</code> <pre><code>@dataclass\nclass WeightedAUC(CTST):\n    \"\"\"\n    Two-sample test for no adverse shift using the weighted AUC (WAUC).\n\n    This test compares scores from two independent samples. We reject the\n    null hypothesis of no adverse shift for unusually high values of the WAUC\n    i.e. when the second sample is relatively worse than the first one. This\n    is a robust nonparametric noninferiority test (NIT) with no pre-specified\n    margin. It can be used, amongst other things, to detect dataset shift with\n    outlier scores, hence the DSOS acronym.\n\n    Attributes\n    ----------\n    actual : NDArray\n        Binary indicator for sample membership.\n    predicted : NDArray\n        Estimated (predicted) scores for corresponding samples in `actual`.\n    n_resamples : int, optional\n        Number of resampling iterations, by default 9999.\n    rng : np.random.Generator, optional\n        Random number generator, by default np.random.default_rng().\n    n_jobs : int, optional\n        Number of parallel jobs, by default 1.\n    batch : int or None, optional\n        Batch size for parallel processing, by default None.\n\n    See Also\n    --------\n    bayes.as_bf : Convert a one-sided p-value to a Bayes factor.\n\n    bayes.as_pvalue : Convert a Bayes factor to a one-sided p-value.\n\n    Notes\n    -----\n    The frequentist null distribution of the WAUC is based on permutations\n    [1]. The Bayesian posterior distribution of the WAUC is based on the\n    Bayesian bootstrap [2]. Because this is a one-tailed test of direction\n    (it asks the question, 'are we worse off?'), we can convert a one-sided\n    p-value into a Bayes factor and vice versa. We can also use these p-values\n    for sequential testing [3].\n\n    The test assumes that `predicted` are outlier scores and/or encode some\n    notions of outlyingness; higher value of `predicted` indicates worse\n    outcomes.\n\n    References\n    ----------\n    .. [1] Kamulete, Vathy M. \"Test for non-negligible adverse shifts.\"\n       Uncertainty in Artificial Intelligence. PMLR, 2022.\n\n    .. [2] Gu, Jiezhun, Subhashis Ghosal, and Anindya Roy. \"Bayesian bootstrap\n       estimation of ROC curve.\" Statistics in medicine 27.26 (2008): 5407-5420.\n\n    .. [3] Kamulete, Vathy M. \"Are you OK? A Bayesian Sequential Test for\n       Adverse Shift.\" 2025.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from samesame.nit import WeightedAUC\n    &gt;&gt;&gt; # alternatively: from samesame.nit import DSOS\n    &gt;&gt;&gt; actual = np.array([0, 1, 1, 0])\n    &gt;&gt;&gt; scores = np.array([0.2, 0.8, 0.6, 0.4])\n    &gt;&gt;&gt; wauc = WeightedAUC(actual, scores)\n    &gt;&gt;&gt; print(wauc.pvalue) # doctest: +SKIP\n    &gt;&gt;&gt; print(wauc.bayes_factor) # doctest: +SKIP\n    &gt;&gt;&gt; wauc_ = WeightedAUC.from_samples(scores, scores)\n    &gt;&gt;&gt; isinstance(wauc_, WeightedAUC)\n    True\n    \"\"\"\n\n    def __init__(\n        self,\n        actual: NDArray,\n        predicted: NDArray,\n        n_resamples: int = 9999,\n        rng: np.random.Generator = np.random.default_rng(),\n        n_jobs: int = 1,\n        batch: int | None = None,\n    ):\n        \"\"\"Initialize WeightedAUC.\"\"\"\n        super().__init__(\n            actual=actual,\n            predicted=predicted,\n            metric=wauc,\n            n_resamples=n_resamples,\n            rng=rng,\n            n_jobs=n_jobs,\n            batch=batch,\n            alternative=\"greater\",\n        )\n\n    @cached_property\n    def posterior(self) -&gt; NDArray:\n        \"\"\"\n        Compute the posterior distribution of the WAUC.\n\n        Returns\n        -------\n        NDArray\n            The posterior distribution of the WAUC.\n\n        Notes\n        -----\n        The result is cached to avoid (expensive) recomputation since the\n        posterior distribution uses the Bayesian bootstrap.\n        \"\"\"\n        return bayesian_posterior(\n            self.actual,\n            self.predicted,\n            self.metric,\n            self.n_resamples,\n            self.rng,\n        )\n\n    @cached_property\n    def bayes_factor(self):\n        \"\"\"\n        Compute the Bayes factor using the Bayesian bootstrap.\n\n        Notes\n        -----\n        The result is cached to avoid (expensive) recomputation.\n        \"\"\"\n        bayes_threshold = float(np.mean(self.null))\n        bf_ = _bayes_factor(self.posterior, bayes_threshold)\n        return bf_\n\n    @classmethod\n    def from_samples(\n        cls,\n        first_sample: NDArray,\n        second_sample: NDArray,\n        n_resamples: int = 9999,\n        rng: np.random.Generator = np.random.default_rng(),\n        n_jobs: int = 1,\n        batch: int | None = None,\n    ):\n        \"\"\"\n        Create a WeightedAUC instance from two samples.\n\n        Parameters\n        ----------\n        first_sample : NDArray\n            First sample of scores. These can be binary or continuous.\n        second_sample : NDArray\n            Second sample of scores. These can be binary or continuous.\n\n        Returns\n        -------\n        WeightedAUC\n            An instance of the WeightedAUC class.\n        \"\"\"\n        assert type_of_target(first_sample) == type_of_target(second_sample)\n        samples = (first_sample, second_sample)\n        actual = assign_labels(samples)\n        predicted = concat_samples(samples)\n        return cls(\n            actual,\n            predicted,\n            n_resamples,\n            rng,\n            n_jobs,\n            batch,\n        )\n</code></pre>"},{"location":"api/nit/#samesame.nit.WeightedAUC.bayes_factor","title":"<code>bayes_factor</code>  <code>cached</code> <code>property</code>","text":"<p>Compute the Bayes factor using the Bayesian bootstrap.</p> Notes <p>The result is cached to avoid (expensive) recomputation.</p>"},{"location":"api/nit/#samesame.nit.WeightedAUC.posterior","title":"<code>posterior</code>  <code>cached</code> <code>property</code>","text":"<p>Compute the posterior distribution of the WAUC.</p> <p>Returns:</p> Type Description <code>NDArray</code> <p>The posterior distribution of the WAUC.</p> Notes <p>The result is cached to avoid (expensive) recomputation since the posterior distribution uses the Bayesian bootstrap.</p>"},{"location":"api/nit/#samesame.nit.WeightedAUC.__init__","title":"<code>__init__(actual, predicted, n_resamples=9999, rng=np.random.default_rng(), n_jobs=1, batch=None)</code>","text":"<p>Initialize WeightedAUC.</p> Source code in <code>src/samesame/nit.py</code> <pre><code>def __init__(\n    self,\n    actual: NDArray,\n    predicted: NDArray,\n    n_resamples: int = 9999,\n    rng: np.random.Generator = np.random.default_rng(),\n    n_jobs: int = 1,\n    batch: int | None = None,\n):\n    \"\"\"Initialize WeightedAUC.\"\"\"\n    super().__init__(\n        actual=actual,\n        predicted=predicted,\n        metric=wauc,\n        n_resamples=n_resamples,\n        rng=rng,\n        n_jobs=n_jobs,\n        batch=batch,\n        alternative=\"greater\",\n    )\n</code></pre>"},{"location":"api/nit/#samesame.nit.WeightedAUC.from_samples","title":"<code>from_samples(first_sample, second_sample, n_resamples=9999, rng=np.random.default_rng(), n_jobs=1, batch=None)</code>  <code>classmethod</code>","text":"<p>Create a WeightedAUC instance from two samples.</p> <p>Parameters:</p> Name Type Description Default <code>first_sample</code> <code>NDArray</code> <p>First sample of scores. These can be binary or continuous.</p> required <code>second_sample</code> <code>NDArray</code> <p>Second sample of scores. These can be binary or continuous.</p> required <p>Returns:</p> Type Description <code>WeightedAUC</code> <p>An instance of the WeightedAUC class.</p> Source code in <code>src/samesame/nit.py</code> <pre><code>@classmethod\ndef from_samples(\n    cls,\n    first_sample: NDArray,\n    second_sample: NDArray,\n    n_resamples: int = 9999,\n    rng: np.random.Generator = np.random.default_rng(),\n    n_jobs: int = 1,\n    batch: int | None = None,\n):\n    \"\"\"\n    Create a WeightedAUC instance from two samples.\n\n    Parameters\n    ----------\n    first_sample : NDArray\n        First sample of scores. These can be binary or continuous.\n    second_sample : NDArray\n        Second sample of scores. These can be binary or continuous.\n\n    Returns\n    -------\n    WeightedAUC\n        An instance of the WeightedAUC class.\n    \"\"\"\n    assert type_of_target(first_sample) == type_of_target(second_sample)\n    samples = (first_sample, second_sample)\n    actual = assign_labels(samples)\n    predicted = concat_samples(samples)\n    return cls(\n        actual,\n        predicted,\n        n_resamples,\n        rng,\n        n_jobs,\n        batch,\n    )\n</code></pre>"},{"location":"api/ood/","title":"ood","text":"<p>Functions for Out-of-Distribution (OOD) detection.</p> <p>These post-hoc OOD detection methods are for pre-trained supervised models, which leverage the information from the entire logit space to enhance in-distribution (ID) and out-of-distribution (OOD) separability.</p> References <p>Liang, J., Hou, R., Hu, M., Chang, H., Shan, S., &amp; Chen, X. (2025). Revisiting Logit Distributions for Reliable Out-of-Distribution Detection. arXiv:2510.20134v1. https://arxiv.org/html/2510.20134v1</p>"},{"location":"api/ood/#samesame.ood.logit_gap","title":"<code>logit_gap(logits)</code>","text":"<p>LogitGap OOD detection score.</p> <p>Computes the average gap between the maximum logit and remaining logits for each sample. This method leverages the observation that in-distribution (ID) samples tend to have higher maximum logits with lower non-maximum logits, while out-of-distribution (OOD) samples exhibit flatter logit distributions.</p> <p>The scoring function is defined as:</p> <p>.. math::</p> <pre><code>S_{\\text{LogitGap}}(x; f) = \\frac{1}{K-1}\n\\sum_{j=2}^{K} (z'_1 - z'_j)\n</code></pre> <p>where :math:<code>z'_1</code> is the maximum logit and :math:<code>z'_j</code> are logits sorted in descending order. Higher scores indicate higher confidence for ID samples.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>NDArray</code> <p>Array of shape (n_samples, n_classes) containing raw logits from a pre-trained classification model.</p> required <p>Returns:</p> Type Description <code>NDArray</code> <p>Array of shape (n_samples,) containing OOD scores. Higher scores indicate higher likelihood of being in-distribution.</p> See Also <p>max_logit : Simple baseline using only the maximum logit value.</p> Notes <p>The LogitGap method is motivated by the observation that ID samples exhibit more pronounced logit distributions (higher maximum logit with lower non-maximum logits), while OOD samples show flatter distributions (smaller gaps between logits).</p> <p>The implementation computes the average gap efficiently by calculating the difference between the maximum logit and the mean of all other logits: max_logit - mean(other_logits). This is mathematically equivalent to the definition in Equation (4).</p> <p>This method requires no additional training or calibration and can be applied as a post-hoc scoring function to any pre-trained classification model. It demonstrates superior performance compared to MaxLogit baseline across various benchmark datasets.</p> <p>The function expects raw logits (pre-softmax values) rather than probabilities. Using logits directly preserves more information about the model's confidence distribution.</p> References <p>.. [1] Liang, Jiachen, et al.    \"Revisiting Logit Distributions for Reliable Out-of-Distribution Detection.\"    The Thirty-ninth Annual Conference on Neural Information Processing Systems.    2025.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; logits = np.array([[5.0, 1.0, 0.5], [2.0, 2.1, 1.9]])\n&gt;&gt;&gt; scores = logit_gap(logits)\n&gt;&gt;&gt; print(scores)\n[4.25 0.15]\n</code></pre> Source code in <code>src/samesame/ood.py</code> <pre><code>def logit_gap(logits: NDArray) -&gt; NDArray:\n    \"\"\"LogitGap OOD detection score.\n\n    Computes the average gap between the maximum logit and remaining logits\n    for each sample. This method leverages the observation that\n    in-distribution (ID) samples tend to have higher maximum logits with\n    lower non-maximum logits, while out-of-distribution (OOD) samples\n    exhibit flatter logit distributions.\n\n    The scoring function is defined as:\n\n    .. math::\n\n        S_{\\\\text{LogitGap}}(x; f) = \\\\frac{1}{K-1}\n        \\\\sum_{j=2}^{K} (z'_1 - z'_j)\n\n    where :math:`z'_1` is the maximum logit and :math:`z'_j` are logits\n    sorted in descending order. Higher scores indicate higher confidence\n    for ID samples.\n\n    Parameters\n    ----------\n    logits : NDArray\n        Array of shape (n_samples, n_classes) containing raw logits from\n        a pre-trained classification model.\n\n    Returns\n    -------\n    NDArray\n        Array of shape (n_samples,) containing OOD scores. Higher scores\n        indicate higher likelihood of being in-distribution.\n\n    See Also\n    --------\n    max_logit : Simple baseline using only the maximum logit value.\n\n    Notes\n    -----\n    The LogitGap method is motivated by the observation that ID samples\n    exhibit more pronounced logit distributions (higher maximum logit with\n    lower non-maximum logits), while OOD samples show flatter distributions\n    (smaller gaps between logits).\n\n    The implementation computes the average gap efficiently by calculating\n    the difference between the maximum logit and the mean of all other\n    logits: max_logit - mean(other_logits). This is mathematically\n    equivalent to the definition in Equation (4).\n\n    This method requires no additional training or calibration and can be\n    applied as a post-hoc scoring function to any pre-trained classification\n    model. It demonstrates superior performance compared to MaxLogit baseline\n    across various benchmark datasets.\n\n    The function expects raw logits (pre-softmax values) rather than\n    probabilities. Using logits directly preserves more information about\n    the model's confidence distribution.\n\n    References\n    ----------\n    .. [1] Liang, Jiachen, et al.\n       \"Revisiting Logit Distributions for Reliable Out-of-Distribution Detection.\"\n       The Thirty-ninth Annual Conference on Neural Information Processing Systems.\n       2025.\n\n    Examples\n    --------\n    &gt;&gt;&gt; logits = np.array([[5.0, 1.0, 0.5], [2.0, 2.1, 1.9]])\n    &gt;&gt;&gt; scores = logit_gap(logits)  # doctest: +SKIP\n    &gt;&gt;&gt; print(scores)\n    [4.25 0.15]\n    \"\"\"\n    logits = np.asarray(logits, dtype=np.float32)\n\n    if logits.ndim != 2:\n        raise ValueError(\n            f\"logits must be 2D array of shape (n_samples, n_classes), \"\n            f\"got shape {logits.shape}\"\n        )\n\n    _, n_classes = logits.shape\n\n    if n_classes &lt; 2:\n        raise ValueError(f\"logits must have at least 2 classes, got {n_classes}\")\n\n    if not np.isfinite(logits).all():\n        raise ValueError(\"logits contain NaN or infinite values\")\n\n    # Sort logits in descending order for each sample\n    sorted_logits = np.sort(logits, axis=1)[:, ::-1]\n\n    # Get maximum logit (z_1')\n    max_logit_val = sorted_logits[:, 0]\n\n    # Compute average gap: (1/(K-1)) * sum(z_1' - z_j') for j=2 to K\n    # This is equivalent to: z_1' - mean(z_2' to z_K')\n    avg_other_logits = np.mean(sorted_logits[:, 1:], axis=1)\n\n    scores = max_logit_val - avg_other_logits\n\n    return scores\n</code></pre>"},{"location":"api/ood/#samesame.ood.max_logit","title":"<code>max_logit(logits)</code>","text":"<p>MaxLogit OOD detection score (baseline method).</p> <p>Computes the maximum logit value for each sample. This is a simple baseline that uses only the most confident prediction, disregarding information from other classes.</p> <p>The scoring function is defined as:</p> <p>.. math::</p> <pre><code>S_{\\text{MaxLogit}}(x; f) = \\max_k z_k\n</code></pre> <p>where :math:<code>z_k</code> is the logit for class k. Higher scores indicate higher confidence for the predicted class.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>NDArray</code> <p>Array of shape (n_samples, n_classes) containing raw logits from a pre-trained classification model.</p> required <p>Returns:</p> Type Description <code>NDArray</code> <p>Array of shape (n_samples,) containing OOD scores (maximum logits). Higher scores indicate higher confidence, but with limited discriminative power for OOD detection compared to LogitGap.</p> See Also <p>logit_gap : Improved OOD detection using logit gap.</p> Notes <p>MaxLogit is included as a baseline for comparison. The paper demonstrates that LogitGap achieves significantly better OOD detection performance by leveraging information from all logits rather than just the maximum.</p> <p>This method has several limitations: - It only uses information from the top predicted class - It ignores the distribution of non-maximum logits - It provides limited discriminative power between ID and OOD samples</p> <p>MaxLogit is conceptually similar to Maximum Softmax Probability (MSP) but operates directly on logits rather than probabilities. Both methods are widely used baselines in OOD detection literature.</p> <p>Despite its simplicity, MaxLogit serves as a reasonable baseline and requires no additional computation beyond extracting the maximum value from the logit vector. It can be useful for computational efficiency when more sophisticated methods are not necessary.</p> References <p>.. [1] Liang, Jiachen, et al.    \"Revisiting Logit Distributions for Reliable Out-of-Distribution Detection.\"    The Thirty-ninth Annual Conference on Neural Information Processing Systems.    2025.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; logits = np.array([[5.0, 1.0, 0.5], [2.0, 2.1, 1.9]])\n&gt;&gt;&gt; scores = max_logit(logits)\n&gt;&gt;&gt; print(scores)\n[5.0 2.1]\n</code></pre> Source code in <code>src/samesame/ood.py</code> <pre><code>def max_logit(logits: NDArray) -&gt; NDArray:\n    \"\"\"MaxLogit OOD detection score (baseline method).\n\n    Computes the maximum logit value for each sample. This is a simple\n    baseline that uses only the most confident prediction, disregarding\n    information from other classes.\n\n    The scoring function is defined as:\n\n    .. math::\n\n        S_{\\\\text{MaxLogit}}(x; f) = \\\\max_k z_k\n\n    where :math:`z_k` is the logit for class k. Higher scores indicate\n    higher confidence for the predicted class.\n\n    Parameters\n    ----------\n    logits : NDArray\n        Array of shape (n_samples, n_classes) containing raw logits from\n        a pre-trained classification model.\n\n    Returns\n    -------\n    NDArray\n        Array of shape (n_samples,) containing OOD scores (maximum logits).\n        Higher scores indicate higher confidence, but with limited\n        discriminative power for OOD detection compared to LogitGap.\n\n    See Also\n    --------\n    logit_gap : Improved OOD detection using logit gap.\n\n    Notes\n    -----\n    MaxLogit is included as a baseline for comparison. The paper\n    demonstrates that LogitGap achieves significantly better OOD detection\n    performance by leveraging information from all logits rather than just\n    the maximum.\n\n    This method has several limitations:\n    - It only uses information from the top predicted class\n    - It ignores the distribution of non-maximum logits\n    - It provides limited discriminative power between ID and OOD samples\n\n    MaxLogit is conceptually similar to Maximum Softmax Probability (MSP)\n    but operates directly on logits rather than probabilities. Both methods\n    are widely used baselines in OOD detection literature.\n\n    Despite its simplicity, MaxLogit serves as a reasonable baseline and\n    requires no additional computation beyond extracting the maximum value\n    from the logit vector. It can be useful for computational efficiency\n    when more sophisticated methods are not necessary.\n\n    References\n    ----------\n    .. [1] Liang, Jiachen, et al.\n       \"Revisiting Logit Distributions for Reliable Out-of-Distribution Detection.\"\n       The Thirty-ninth Annual Conference on Neural Information Processing Systems.\n       2025.\n\n    Examples\n    --------\n    &gt;&gt;&gt; logits = np.array([[5.0, 1.0, 0.5], [2.0, 2.1, 1.9]])\n    &gt;&gt;&gt; scores = max_logit(logits)  # doctest: +SKIP\n    &gt;&gt;&gt; print(scores)\n    [5.0 2.1]\n    \"\"\"\n    logits = np.asarray(logits, dtype=np.float32)\n\n    if logits.ndim != 2:\n        raise ValueError(\n            f\"logits must be 2D array of shape (n_samples, n_classes), \"\n            f\"got shape {logits.shape}\"\n        )\n\n    _, n_classes = logits.shape\n\n    if n_classes &lt; 2:\n        raise ValueError(f\"logits must have at least 2 classes, got {n_classes}\")\n\n    if not np.isfinite(logits).all():\n        raise ValueError(\"logits contain NaN or infinite values\")\n\n    # Simply return the maximum logit for each sample\n    scores = np.max(logits, axis=1)\n\n    return scores\n</code></pre>"},{"location":"examples/credit-example/","title":"Credit (HELOC) Dataset","text":"<p>This example demonstrates testing for dataset shift (covariate shift) and performance degradation (outcome shift) using the HELOC dataset from TableShift. We apply both CTST and DSOS to a credit risk scenario.</p>"},{"location":"examples/credit-example/#when-to-use-this","title":"When to use this","text":"<ul> <li>Dataset shift (CTST): Detect when feature distributions differ between training and deployment</li> <li>Performance degradation (DSOS): Detect when predictions shift toward worse outcomes</li> </ul> <p>See Distribution Shifts for CTST basics and Noninferiority for DSOS fundamentals.</p>"},{"location":"examples/credit-example/#setup","title":"Setup","text":"<p>The HELOC dataset (FICO Explainable AI Challenge) contains credit bureau features. We split it by <code>ExternalRiskEstimate</code> to simulate training on one population and deploying to another:</p> <ul> <li>Training set (<code>ExternalRiskEstimate &gt; 63</code>): 7,683 low-risk customers</li> <li>Test set (<code>ExternalRiskEstimate \u2264 63</code>): 2,188 high-risk customers</li> </ul> <p>This split creates a domain shift scenario typical in production credit modeling.</p>"},{"location":"examples/credit-example/#data-loading","title":"Data Loading","text":"<pre><code>import re\nimport pandas as pd\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import roc_auc_score\n\nfrom samesame.ctst import CTST\nfrom samesame.nit import DSOS\n\n# Fetch HELOC dataset\nfico = fetch_openml(data_id=45554, as_frame=True)\nX, y = fico.data, fico.target\n\n# Find ExternalRiskEstimate column and split data\nre_obj = re.compile(r\"external.*risk.*estimate\", flags=re.I)\ncol_split = next((c for c in X.columns if re_obj.search(c)), None)\nmask_high = X[col_split].astype(float) &gt; 63\n\nX_train = X[mask_high].reset_index(drop=True)\ny_train = y[mask_high].reset_index(drop=True)\nX_test = X[~mask_high].reset_index(drop=True)\ny_test = y[~mask_high].reset_index(drop=True)\n\nprint(f\"Training set: {len(X_train)} samples\")\nprint(f\"Test set: {len(X_test)} samples\")\n</code></pre> <p>Output:</p> <pre><code># Training set: 7683 samples (low external risk)\n# Test set: 2188 samples (high external risk)\n</code></pre>"},{"location":"examples/credit-example/#dataset-shift-ctst","title":"Dataset Shift (CTST)","text":"<p>Question: Are the feature distributions different?</p> <p>Method: Train a classifier to distinguish training from test samples. High performance indicates distributional differences. We use OOB predictions for valid inference.</p> <pre><code># Create domain labels and concatenate data\nsplit = pd.Series([0] * len(X_train) + [1] * len(X_test))\nX_concat = pd.concat([X_train, X_test], ignore_index=True)\n\n# Train RandomForest with OOB predictions\nrf_domain = RandomForestClassifier(\n    n_estimators=500,\n    oob_score=True,\n    random_state=12345,\n    min_samples_leaf=10,\n)\nrf_domain.fit(X_concat, split)\noob_scores = rf_domain.oob_decision_function_[:, 1]\n\n# Run CTST\nctst = CTST(actual=split.values, predicted=oob_scores, metric=roc_auc_score)\nprint(f\"Dataset Shift Detection (CTST)\")\nprint(f\"  Statistic: {ctst.statistic:.4f}\")\nprint(f\"  p-value: {ctst.pvalue:.4f}\")\n</code></pre> <p>Output:</p> <pre><code># Dataset Shift Detection (CTST)\n#   Statistic: 1.0000\n#   p-value: 0.0002\n</code></pre> <p>Result: p-value = 0.0002 (highly significant). The classifier easily distinguishes training from test samples, confirming substantial covariate shift between the two populations.</p>"},{"location":"examples/credit-example/#feature-importance","title":"Feature Importance","text":"<p>To understand which features drive the shift, we can examine feature importance:</p> <pre><code>importances = rf_domain.feature_importances_\nfeature_names = X_concat.columns\nfeat_imp = pd.Series(importances, index=feature_names).sort_values(ascending=False)\nprint(\"\\nTop 5 features distinguishing the datasets:\")\nprint(feat_imp.head(5))\n</code></pre> <p>Output:</p> <pre><code>Top 5 features distinguishing the domains:\nExternalRiskEstimate          0.642400\nMSinceMostRecentDelq          0.069394\nMaxDelq2PublicRecLast12M      0.064526\nNetFractionRevolvingBurden    0.050656\nPercentTradesNeverDelq        0.042478\ndtype: float64\n</code></pre> <p>These importances show which features differ most between the two populations. <code>ExternalRiskEstimate</code> dominates because it was the splitting criterion. However, other features like <code>MSinceMostRecentDelq</code> and <code>MaxDelq2PublicRecLast12M</code> also show some discriminatory power, beyond the splitting variable. Feature importance enables targeted interventions and informs further investigation and analysis.</p>"},{"location":"examples/credit-example/#performance-degradation-dsos","title":"Performance Degradation (DSOS)","text":"<p>Question: Have predictions shifted toward worse outcomes?</p> <p>Method: Train a credit risk model and compare predicted default probabilities between training and test sets. DSOS tests for adverse shifts in the prediction distribution.</p> <pre><code># Train credit risk model\nloan_status = y_train.map({'Good': 0, 'Bad': 1}).values\nrf_bad = RandomForestClassifier(\n    n_estimators=500,\n    oob_score=True,\n    random_state=12345,\n    min_samples_leaf=10,\n)\nrf_bad.fit(X_train, loan_status)\n\n# Get predicted default probabilities\nbad_train = rf_bad.oob_decision_function_[:, 1].ravel()\nbad_test = rf_bad.predict_proba(X_test)[:, 1].ravel()\n\n# Run DSOS test\ndsos = DSOS.from_samples(bad_train, bad_test)\nprint(f\"\\nPerformance Degradation (DSOS)\")\nprint(f\"  Statistic: {dsos.statistic:.4f}\")\nprint(f\"  p-value: {dsos.pvalue:.4f}\")\n</code></pre> <p>Output:</p> <pre><code># Performance Degradation (DSOS)\n#   Statistic: 0.2483\n#   p-value: 0.0001\n</code></pre> <p>Result: p-value = 0.0001 (highly significant). The model predicts substantially higher default risk for test samples. DSOS detects this adverse shift by focusing on the distribution tail\u2014whether extreme high-risk predictions are more common\u2014without requiring true labels.</p>"},{"location":"examples/credit-example/#interpreting-results","title":"Interpreting Results","text":"<p>CTST vs. DSOS: These tests answer complementary questions:</p> <ul> <li>CTST: Do feature distributions differ? (covariate shift)</li> <li>DSOS: Have predictions shifted adversely? (outcome shift)</li> </ul> <p>Action guidance:</p> <ul> <li>Both significant \u2192 Covariate shift causing performance issues. Retrain or recalibrate.</li> <li>Only DSOS significant \u2192 Label/concept shift without feature changes. Investigate root causes.</li> <li>Only CTST significant \u2192 Distribution changed but model still generalizes. Monitor closely.</li> <li>Neither significant \u2192 Safe to operate.</li> </ul>"},{"location":"examples/credit-example/#practical-recommendations","title":"Practical Recommendations","text":"<p>1. Monitor Regularly. Run CTST and DSOS on production batches (weekly/monthly). Automate alerting based on p-value thresholds (e.g., p &lt; 0.01 triggers review).</p> <p>2. Investigate Drivers. Use feature importance to identify which features drive shifts. Distinguish natural drift from data quality issues.</p> <p>3. No Labels Needed. Both tests work without ground truth in deployment, enabling continuous monitoring when labels are delayed.</p> <p>4. Respond Appropriately. Natural drift \u2192 monitor. Meaningful shift \u2192 recalibrate. Substantial shift \u2192 retrain.</p>"},{"location":"examples/credit-ood-detection/","title":"Credit (HELOC) Dataset: Out-of-Distribution Detection","text":"<p>This example is a continuation of the Credit Example. We reuse the same HELOC dataset and domain split, but replace outcome shift detection with out-of-distribution (OOD) detection to assess whether deployed samples diverge from in-sample data - the training distribution.</p>"},{"location":"examples/credit-ood-detection/#when-to-use-this","title":"When to use this","text":"<p>Out-of-distribution detection: You've trained a model on one population and deployed it on a different population. You need to assess whether the out-of-sample data resembles the training samples (in-distribution) or is different enough to warrant concern (out-of-distribution), without requiring ground truth labels.</p> <p>OOD detection methods leverage a measure of model confidence to flag unusual inputs. Density-based OOD detection relies on estimating input densities (often challenging in high dimensions), or some proxy thereof. When you have already trained a predictive model and want to monitor its performance on new data, rather than train an auxiliary model to score unusual inputs, repurposing the existing model's predictions into confidence scores is a practical approach. The latter is the focus of this example.</p>"},{"location":"examples/credit-ood-detection/#setup","title":"Setup","text":"<p>The HELOC dataset is split into two populations based on <code>ExternalRiskEstimate</code>:</p> <ul> <li>Training set (<code>ExternalRiskEstimate &gt; 63</code>): 7,683 samples, low-risk customers</li> <li>Test set (<code>ExternalRiskEstimate \u2264 63</code>): 2,188 samples, high-risk customers</li> </ul> <p>See Credit Example for full context and dataset loading details. We reuse the same data split here to focus on OOD detection.</p>"},{"location":"examples/credit-ood-detection/#data-loading","title":"Data Loading","text":"<p>To follow along, load the data as shown in Credit Example:</p> <pre><code>import re\nimport pandas as pd\nimport numpy as np\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import roc_auc_score\n\nfrom samesame.ood import logit_gap, max_logit\nfrom samesame.nit import DSOS\n\n# Fetch HELOC dataset and split by ExternalRiskEstimate &gt; 63\nfico = fetch_openml(data_id=45554, as_frame=True)\nX, y = fico.data, fico.target\n\nre_obj = re.compile(r\"external.*risk.*estimate\", flags=re.I)\ncol_split = next((c for c in X.columns if re_obj.search(c)), None)\nmask_high = X[col_split].astype(float) &gt; 63\n\nX_train = X[mask_high].reset_index(drop=True)\ny_train = y[mask_high].reset_index(drop=True)\nX_test = X[~mask_high].reset_index(drop=True)\ny_test = y[~mask_high].reset_index(drop=True)\n</code></pre> <p>Result: We have 7,683 training samples and 2,188 test samples, representing two distinct populations with different risk profiles.</p>"},{"location":"examples/credit-ood-detection/#out-of-distribution-detection","title":"Out-of-Distribution Detection","text":"<p>Question: How can we detect whether the test samples are unusual, compared to the training samples, without relying on labels? That is, looking at the inputs alone, without access to ground truth (default status) in the test samples.</p> <p>Method: We train a credit risk model on the training set and use the predicted probabilities to infer the model's confidence in each sample. These confidence scores serve as out-of-distribution (OOD) scores. Low confidence indicates the model finds the sample unusual (out-of-distribution), while high confidence indicates familiarity (in-distribution). Two methods are compared:</p> <ul> <li>LogitGap: Measures the average gap between the top logit and remaining logits\u2014a strong indicator of model confidence in domain membership.</li> <li>MaxLogit: Baseline using only the maximum logit\u2014simpler but with limited discriminative power.</li> </ul>"},{"location":"examples/credit-ood-detection/#training-model","title":"Training Model","text":"<p>First, we train a binary classification model on the training domain to predict loan default.</p> <pre><code># Train a credit risk model on the training domain to predict loan default\n# Convert labels from categorical (Good/Bad) to numeric (0/1)\nbad_mapping = {'Good': 0, 'Bad': 1}\nbad_train = y_train.map(bad_mapping).values\nbad_test = y_test.map(bad_mapping).values\nprint(f\"Training domain default rate: {np.mean(bad_train):.4f}\")\nprint(f\"Test domain default rate: {np.mean(bad_test):.4f}\")\n\n# Train RandomForest to predict probability of default\nrf_bad = RandomForestClassifier(\n    n_estimators=500,\n    oob_score=True,\n    random_state=12345,\n    min_samples_leaf=10,\n)\nrf_bad.fit(X_train, bad_train)\n</code></pre> <p>We predict probabilities, convert these to logits and compute OOD scores from these.</p> <pre><code>from scipy.special import logit\n\n# Extract predicted logits\n# - Training domain: Use OOB predictions (unbiased, non-resubstitution)\n# - Deployment domain: Use standard predictions on held-out data\n# This represents the unnormalized scores for class 0 and class 1\nlogits_train = logit(rf_bad.oob_decision_function_)\nlogits_test = logit(rf_bad.predict_proba(X_test))\n\n# Compute OOD scores using LogitGap (superior method)\nood_train_gap = logit_gap(logits_train)\nood_test_gap = logit_gap(logits_test)\n\n# Compute OOD scores using MaxLogit (baseline for comparison)\nood_train_max = max_logit(logits_train)\nood_test_max = max_logit(logits_test)\n</code></pre>"},{"location":"examples/credit-ood-detection/#testing-using-dsos","title":"Testing Using DSOS","text":"<p>Now we use the OOD scores as outlier scores in the DSOS (Dataset-Shift-with-Outlier-Scores) test. This tests whether the distribution of OOD scores has shifted between training and deployment domains.</p> <pre><code># Apply DSOS test using OOD scores (LogitGap) as outlier scores\n# The test detects whether the tail of the OOD score distribution has shifted\n# Low OOD scores in deployment indicate the model sees unfamiliar samples\ndsos_ood = DSOS.from_samples(-ood_train_gap, -ood_test_gap)\n\nprint(f\"\\nOOD Shift Detection (DSOS on LogitGap scores)\")\nprint(f\"  Statistic: {dsos_ood.statistic:.4f}\")\nprint(f\"  p-value: {dsos_ood.pvalue:.4f}\")\n</code></pre> <p>Output:</p> <pre><code># OOD Shift Detection (DSOS on LogitGap scores)\n#   Statistic: ...\n#   p-value: &lt; ...\n</code></pre> <ul> <li>What this means: ...</li> </ul>"},{"location":"examples/credit-ood-detection/#key-insights","title":"Key Insights","text":"<p>OOD Shift Detection via DSOS. While traditional shift detection focuses on feature distributions, DSOS on OOD scores focuses on model confidence. A significant DSOS result indicates the model's perceived familiarity with samples has changed\u2014a leading indicator that performance may degrade. This is particularly useful because:</p> <ul> <li>No labels required: OOD scores depend only on the trained model, not on ground truth</li> <li>Early warning: Model confidence shifts often precede actual performance degradation</li> <li>Interpretable: Low OOD scores directly indicate model uncertainty about domain membership</li> </ul> <p>When to Act. A significant DSOS result on OOD scores (p &lt; 0.05) suggests:</p> <ul> <li>The deployment population is substantially different from training</li> <li>The model is encountering unfamiliar patterns</li> <li>Monitoring should increase, and retraining should be considered if performance metrics confirm degradation</li> </ul>"},{"location":"examples/credit-ood-detection/#practical-recommendations","title":"Practical Recommendations","text":"<p>1. Monitor OOD Score Distributions Continuously. Compute OOD scores on new production batches and track their distribution over time. Apply the DSOS test to detect significant shifts in model confidence.</p> <p>2. Set Baseline OOD Thresholds. Establish percentile thresholds from your training domain to flag unusual samples.</p> <p>3. Combine with Other Monitoring Approaches. Use OOD-based DSOS alongside other monitoring strategies:</p> <ul> <li>Actual performance metrics (if labels eventually arrive)</li> <li>Feature distribution shifts</li> <li>Prediction distribution changes</li> </ul>"},{"location":"examples/credit-ood-detection/#summary","title":"Summary","text":"<p>This tutorial demonstrates how to use LogitGap OOD scores as outlier scores in the DSOS test to detect whether deployed samples are out-of-distribution relative to training. Key takeaways:</p> <ul> <li>OOD Scores: Quantify model confidence in domain membership using LogitGap (superior to MaxLogit baseline)</li> <li>DSOS Test: Detects significant shifts in the distribution of OOD scores between training and deployment</li> <li>Label-Free Monitoring: This approach requires only the trained model and new data\u2014no ground truth labels needed</li> <li>Actionable Signals: Significant DSOS results (p &lt; 0.05) indicate model uncertainty about the deployment population, warranting increased monitoring or retraining consideration.</li> </ul> <p>This methodology provides an early-warning system for out-of-distribution detection, enabling proactive intervention before performance degrades in production.</p>"},{"location":"examples/distribution-shifts/","title":"Test for Distribution Shifts","text":"<p>This is a concise walkthrough of classifier two-sample tests (CTST) to test whether two samples come from different distributions, using a small synthetic example. CTSTs are flexible (any classifier, any metric) and require few assumptions.</p>"},{"location":"examples/distribution-shifts/#data","title":"Data","text":"<pre><code>from sklearn.datasets import make_classification\n\nX, y = make_classification(\n    n_samples=100,\n    n_features=4,\n    n_classes=2,\n    random_state=123_456,\n)\n# y = 1 denotes sample_P, y = 0 denotes sample_Q\n</code></pre>"},{"location":"examples/distribution-shifts/#cross-fitted-ctst-recommended","title":"Cross-fitted CTST (recommended)","text":"<p>Use cross-fitted predictions to avoid sample-splitting and get valid p-values.</p> <pre><code>from sklearn.ensemble import HistGradientBoostingClassifier\nfrom sklearn.model_selection import cross_val_predict\nfrom samesame.ctst import CTST\nfrom sklearn.metrics import roc_auc_score\n\n# Out-of-sample predicted probabilities\ny_hat = cross_val_predict(\n    HistGradientBoostingClassifier(random_state=123_456),\n    X,\n    y,\n    cv=10,\n    method=\"predict_proba\",\n)[:, 1]\n\n# Run CTST with AUC\nctst = CTST(actual=y, predicted=y_hat, metric=roc_auc_score)\nprint(\"CTST (AUC)\")\nprint(f\"  statistic: {ctst.statistic:.2f}\")\nprint(f\"  p-value:   {ctst.pvalue:.4f}\")\n</code></pre> <p>Output:</p> <pre><code>CTST (AUC)\n    statistic: 0.93\n    p-value:   0.0002\n</code></pre> <p>Interpretation: A small p-value rejects \\(P=Q\\), indicating the samples differ. If p-value is large, evidence is insufficient to claim a shift.</p>"},{"location":"examples/distribution-shifts/#oob-alternative","title":"OOB Alternative","text":"<p>Out-of-bag (OOB) predictions from ensembles can replace cross-fitting when convenient.</p> <pre><code>from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(\n    n_estimators=500,\n    oob_score=True,\n    min_samples_leaf=10,\n    random_state=123_456,\n)\nrf.fit(X, y)\ny_oob = rf.oob_decision_function_[:, 1]\n\nctst_oob = CTST(actual=y, predicted=y_oob, metric=roc_auc_score)\nprint(\"CTST (OOB, AUC)\")\nprint(f\"  statistic: {ctst_oob.statistic:.2f}\")\nprint(f\"  p-value:   {ctst_oob.pvalue:.4f}\")\n</code></pre> <p>Output:</p> <pre><code>CTST (OOB, AUC)\n    statistic: 0.94\n    p-value:   0.0002\n</code></pre>"},{"location":"examples/distribution-shifts/#interpreting-results","title":"Interpreting Results","text":"<ul> <li>Small p-value \u2192 distributions differ (shift detected).</li> <li>Large p-value \u2192 insufficient evidence of shift.</li> <li>CTST says whether distributions differ, not why or how bad; pair with DSOS for adverse shift checks.</li> </ul>"},{"location":"examples/distribution-shifts/#tips","title":"Tips","text":"<ul> <li>Use cross-fitting or OOB predictions to boost statistical power and avoid training bias.</li> <li>Pick a metric aligned with your classifier output (AUC for probabilities; balanced accuracy for binary predictions).</li> <li>For explanations, inspect feature importance or SHAP on the CTST classifier to see what drives the shift.</li> <li>For adverse-shift questions, continue to Noninferiority or apply DSOS on relevant scores.</li> </ul>"},{"location":"examples/noninferiority/","title":"Noninferiority Test (D-SOS)","text":"<p>Noninferiority asks: is the new sample not meaningfully worse than the reference? <code>samesame</code> implements D-SOS (Dataset Shift with Outlier Scores), a nonparametric test on outlier scores with a one-sided alternative.</p>"},{"location":"examples/noninferiority/#when-to-use","title":"When to use","text":"<ul> <li>CTST (distribution difference): detect any distributional change</li> <li>D-SOS (adverse shift check): detect whether the new sample is worse (more high outlier scores)</li> </ul> <p>Use D-SOS when you care about harmful shifts rather than any difference.</p>"},{"location":"examples/noninferiority/#how-d-sos-works","title":"How D-SOS works","text":"<ul> <li>Treat outlier scores as classifier outputs</li> <li>Use a weighted AUC with a one-sided alternative (more high scores = worse)</li> <li>No parametric assumptions or preset margin needed</li> </ul>"},{"location":"examples/noninferiority/#example-clinical-trial","title":"Example: clinical trial","text":"<p>The original example compares two treatments for relief from leg discomfort. We repurpose the relief scores as outlier scores to test noninferiority of the Bowl treatment versus the Armanaleg (reference). We flip relief into discomfort scores so that higher means worse (outlier) and test if the new treatment is not worse than the reference.</p> <pre><code>import numpy as np\n\ndatalines = (\n  \"9 14 13 8 10 5 11 9 12 10 9 11 8 11 \"\n  \"4 8 11 16 12 10 9 10 13 12 11 13 9 4 \"\n  \"7 14 8 4 10 11 7 7 13 8 8 13 10 9 \"\n  \"12 9 11 10 12 7 8 5 10 7 13 12 13 11 \"\n  \"7 12 10 11 10 8 6 9 11 8 5 11 10 8\"\n).split()\nrelief = [float(s) for s in datalines]\ndiscomfort = [max(relief) - s for s in relief]\narmanaleg = np.array(discomfort[:28])\nbowl = np.array(discomfort[28:])\n</code></pre>"},{"location":"examples/noninferiority/#analysis","title":"Analysis","text":"<p>Run D-SOS treating <code>armanaleg</code> as control and <code>bowl</code> as the new treatment.</p> <pre><code>from samesame.bayes import as_pvalue\nfrom samesame.nit import DSOS\n\ndsos = DSOS.from_samples(armanaleg, bowl)\nfrequentist = dsos.pvalue\nbayesian = as_pvalue(dsos.bayes_factor)\nprint(f\"Frequentist p-value: {frequentist:.4f}\")\nprint(f\"Bayesian p-value: {bayesian:.4f}\")\n</code></pre> <p>Typical output (reproduced from the example):</p> <pre><code>Frequentist p-value: 0.1215\nBayesian p-value: 0.1159\n</code></pre> <p>We fail to reject the null of no adverse shift \u2014 the new treatment (Bowl) is not shown to be meaningfully worse than the reference under the D-SOS criterion.</p>"},{"location":"examples/noninferiority/#interpreting-results","title":"Interpreting results","text":"<ul> <li>Small p-value \u2192 evidence of adverse shift (new sample has more extreme outliers)</li> <li>Large p-value \u2192 insufficient evidence of being worse (noninferior)</li> </ul>"},{"location":"examples/noninferiority/#practical-tips","title":"Practical tips","text":"<ul> <li>Pick outlier scores aligned to \u201cworse outcomes\u201d (e.g., high error, high risk, discomfort)</li> <li>Pair with CTST: CTST says \u201cdifferent\u201d, D-SOS says \u201cmeaningfully worse?\u201d</li> <li>Use when labels are scarce: outlier scores can be model-based or proxy metrics</li> </ul>"}]}